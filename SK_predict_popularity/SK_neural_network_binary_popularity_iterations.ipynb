{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark_dist_explore import hist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from ann_visualizer.visualize import ann_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize findspark\n",
    "findspark.init()\n",
    "\n",
    "# Initialize the spark session\n",
    "spark = SparkSession.builder.appName(\"SK_binary_nn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|   primary_artist|          track_name|popularity|explicit|danceability|energy|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|      duration_min|time_signature_0|time_signature_1|time_signature_3|time_signature_4|time_signature_5|key_0|key_1|key_2|key_3|key_4|key_5|key_6|key_7|key_8|key_9|key_10|key_11|num_artists_binned_1|num_artists_binned_2|num_artists_binned_3|num_artists_binned_4|num_artists_binned_5|num_artists_binned_6|track_genre_0|track_genre_1|track_genre_2|track_genre_3|track_genre_4|track_genre_5|track_genre_6|\n",
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|      Gen Hoshino|              Comedy|        73|       0|       0.676| 0.461|  -6.746|   0|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|3.8444333333333334|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Ben Woodward|    Ghost - Acoustic|        55|       0|        0.42| 0.166| -17.235|   1|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|            2.4935|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|Ingrid Michaelson|      To Begin Again|        57|       0|       0.438| 0.359|  -9.734|   1|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|3.5137666666666667|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Kina Grannis|Can't Help Fallin...|        71|       0|       0.266|0.0596| -18.515|   1|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|           3.36555|               0|               0|               1|               0|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "| Chord Overstreet|             Hold On|        82|       0|       0.618| 0.443|  -9.681|   1|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949| 3.314216666666667|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Tyrone Wells|Days I Will Remember|        58|       0|       0.688| 0.481|  -8.807|   1|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017| 3.570666666666667|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|A Great Big World|       Say Something|        74|       0|       0.407| 0.147|  -8.822|   1|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|3.8233333333333333|               0|               0|               1|               0|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|           I'm Yours|        80|       0|       0.703| 0.444|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|            4.0491|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     1|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|               Lucky|        74|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|   Ross Copperman|              Hunger|        56|       0|       0.442| 0.632|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|3.4265666666666665|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Zack Tabudlo|Give Me Your Forever|        74|       0|       0.627| 0.363|  -8.127|   1|     0.0291|       0.279|             0.0|  0.0928|  0.301| 99.905|              4.08|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|     I Won't Give Up|        69|       0|       0.483| 0.303| -10.058|   1|     0.0429|       0.694|             0.0|   0.115|  0.139|133.406|           4.00275|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|         Dan Berk|                Solo|        52|       0|       0.489| 0.314|  -9.245|   0|     0.0331|       0.749|             0.0|   0.113|  0.607|124.234|3.3118666666666665|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|    Anna Hamilton|            Bad Liar|        62|       0|       0.691| 0.234|  -6.441|   1|     0.0285|       0.777|             0.0|    0.12|  0.209| 87.103|            4.1408|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "| Chord Overstreet|     Hold On - Remix|        56|       0|       0.755|  0.78|  -6.084|   1|     0.0327|       0.124|         2.83E-5|   0.121|  0.387|120.004|           3.13555|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|      Landon Pigg|Falling in Love a...|        58|       0|       0.489| 0.561|  -7.933|   1|     0.0274|         0.2|         4.56E-5|   0.179|  0.238| 83.457|            4.0831|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Andrew Foy|ily (i love you b...|        56|       0|       0.706| 0.112| -18.098|   1|     0.0391|       0.827|         4.03E-6|   0.125|  0.414|110.154|            2.1625|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Andrew Foy|         At My Worst|        54|       0|       0.795|0.0841|  -18.09|   0|     0.0461|       0.742|         1.17E-5|  0.0853|  0.609| 91.803|            2.8288|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     1|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|               Lucky|        68|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Boyce Avenue|          Photograph|        67|       0|       0.717|  0.32|  -8.393|   1|     0.0283|        0.83|             0.0|   0.107|  0.322|107.946| 4.336433333333333|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the pre-processed data\n",
    "s_df = spark.read.csv(\"../Resources/filtered_encoded_dataset.csv\", sep=\",\", header=True, inferSchema=True)\n",
    "s_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|popularity|explicit|danceability|energy|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|      duration_min|time_signature_0|time_signature_1|time_signature_3|time_signature_4|time_signature_5|key_0|key_1|key_2|key_3|key_4|key_5|key_6|key_7|key_8|key_9|key_10|key_11|num_artists_binned_1|num_artists_binned_2|num_artists_binned_3|num_artists_binned_4|num_artists_binned_5|num_artists_binned_6|track_genre_0|track_genre_1|track_genre_2|track_genre_3|track_genre_4|track_genre_5|track_genre_6|\n",
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|        73|       0|       0.676| 0.461|  -6.746|   0|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|3.8444333333333334|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        55|       0|        0.42| 0.166| -17.235|   1|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|            2.4935|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        57|       0|       0.438| 0.359|  -9.734|   1|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|3.5137666666666667|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        71|       0|       0.266|0.0596| -18.515|   1|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|           3.36555|               0|               0|               1|               0|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        82|       0|       0.618| 0.443|  -9.681|   1|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949| 3.314216666666667|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        58|       0|       0.688| 0.481|  -8.807|   1|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017| 3.570666666666667|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.407| 0.147|  -8.822|   1|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|3.8233333333333333|               0|               0|               1|               0|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        80|       0|       0.703| 0.444|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|            4.0491|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     1|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.442| 0.632|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|3.4265666666666665|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.627| 0.363|  -8.127|   1|     0.0291|       0.279|             0.0|  0.0928|  0.301| 99.905|              4.08|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        69|       0|       0.483| 0.303| -10.058|   1|     0.0429|       0.694|             0.0|   0.115|  0.139|133.406|           4.00275|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        52|       0|       0.489| 0.314|  -9.245|   0|     0.0331|       0.749|             0.0|   0.113|  0.607|124.234|3.3118666666666665|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        62|       0|       0.691| 0.234|  -6.441|   1|     0.0285|       0.777|             0.0|    0.12|  0.209| 87.103|            4.1408|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.755|  0.78|  -6.084|   1|     0.0327|       0.124|         2.83E-5|   0.121|  0.387|120.004|           3.13555|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        58|       0|       0.489| 0.561|  -7.933|   1|     0.0274|         0.2|         4.56E-5|   0.179|  0.238| 83.457|            4.0831|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.706| 0.112| -18.098|   1|     0.0391|       0.827|         4.03E-6|   0.125|  0.414|110.154|            2.1625|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        54|       0|       0.795|0.0841|  -18.09|   0|     0.0461|       0.742|         1.17E-5|  0.0853|  0.609| 91.803|            2.8288|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     1|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        68|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        67|       0|       0.717|  0.32|  -8.393|   1|     0.0283|        0.83|             0.0|   0.107|  0.322|107.946| 4.336433333333333|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove primary_artist and track_name from the dataset\n",
    "s_df = s_df.drop(\"primary_artist\", \"track_name\")\n",
    "s_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([22457.,  9279., 18354., 14910., 19205., 14532.,  9344.,  4269.,\n",
       "         1101.,    98.]),\n",
       " array([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check popularity values\n",
    "fig, ax = plt.subplots()\n",
    "hist(ax, s_df.select(\"popularity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>...</th>\n",
       "      <th>num_artists_binned_4</th>\n",
       "      <th>num_artists_binned_5</th>\n",
       "      <th>num_artists_binned_6</th>\n",
       "      <th>track_genre_0</th>\n",
       "      <th>track_genre_1</th>\n",
       "      <th>track_genre_2</th>\n",
       "      <th>track_genre_3</th>\n",
       "      <th>track_genre_4</th>\n",
       "      <th>track_genre_5</th>\n",
       "      <th>track_genre_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   popularity  explicit  danceability  energy  loudness  mode  speechiness   \n",
       "0          73         0         0.676  0.4610    -6.746     0       0.1430  \\\n",
       "1          55         0         0.420  0.1660   -17.235     1       0.0763   \n",
       "2          57         0         0.438  0.3590    -9.734     1       0.0557   \n",
       "3          71         0         0.266  0.0596   -18.515     1       0.0363   \n",
       "4          82         0         0.618  0.4430    -9.681     1       0.0526   \n",
       "\n",
       "   acousticness  instrumentalness  liveness  ...  num_artists_binned_4   \n",
       "0        0.0322          0.000001    0.3580  ...                     0  \\\n",
       "1        0.9240          0.000006    0.1010  ...                     0   \n",
       "2        0.2100          0.000000    0.1170  ...                     0   \n",
       "3        0.9050          0.000071    0.1320  ...                     0   \n",
       "4        0.4690          0.000000    0.0829  ...                     0   \n",
       "\n",
       "   num_artists_binned_5  num_artists_binned_6  track_genre_0  track_genre_1   \n",
       "0                     0                     0              0              0  \\\n",
       "1                     0                     0              0              0   \n",
       "2                     0                     0              0              0   \n",
       "3                     0                     0              0              0   \n",
       "4                     0                     0              0              0   \n",
       "\n",
       "   track_genre_2  track_genre_3  track_genre_4  track_genre_5  track_genre_6  \n",
       "0              0              0              0              0              1  \n",
       "1              0              0              0              0              1  \n",
       "2              0              0              0              0              1  \n",
       "3              0              0              0              0              1  \n",
       "4              0              0              0              0              1  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data to a pandas Dataframe\n",
    "df = s_df.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popularity\n",
       "0      15843\n",
       "1       2116\n",
       "2       1025\n",
       "3        570\n",
       "4        377\n",
       "       ...  \n",
       "96         7\n",
       "97         8\n",
       "98         7\n",
       "99         1\n",
       "100        2\n",
       "Name: count, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show value counts of popularity to help determine proper bins\n",
    "df['popularity'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popularity_binned\n",
       "0    99988\n",
       "1    13561\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Bin the popularity into: low=0 (pop<=60) or high=1 (60<pop)\n",
    "# Set up a list of bins\n",
    "pop_bins = [0, 1]\n",
    "# Set up list of conditions\n",
    "pop_conditions = [\n",
    "    (df[\"popularity\"] <= 60),\n",
    "    (df[\"popularity\"] > 60)\n",
    "]\n",
    "# Set up the column with bins\n",
    "df[\"popularity_binned\"] = np.select(pop_conditions, pop_bins)\n",
    "\n",
    "# Confirm binning\n",
    "df['popularity_binned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>...</th>\n",
       "      <th>num_artists_binned_5</th>\n",
       "      <th>num_artists_binned_6</th>\n",
       "      <th>track_genre_0</th>\n",
       "      <th>track_genre_1</th>\n",
       "      <th>track_genre_2</th>\n",
       "      <th>track_genre_3</th>\n",
       "      <th>track_genre_4</th>\n",
       "      <th>track_genre_5</th>\n",
       "      <th>track_genre_6</th>\n",
       "      <th>popularity_binned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   explicit  danceability  energy  loudness  mode  speechiness  acousticness   \n",
       "0         0         0.676  0.4610    -6.746     0       0.1430        0.0322  \\\n",
       "1         0         0.420  0.1660   -17.235     1       0.0763        0.9240   \n",
       "2         0         0.438  0.3590    -9.734     1       0.0557        0.2100   \n",
       "3         0         0.266  0.0596   -18.515     1       0.0363        0.9050   \n",
       "4         0         0.618  0.4430    -9.681     1       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence  ...  num_artists_binned_5   \n",
       "0          0.000001    0.3580    0.715  ...                     0  \\\n",
       "1          0.000006    0.1010    0.267  ...                     0   \n",
       "2          0.000000    0.1170    0.120  ...                     0   \n",
       "3          0.000071    0.1320    0.143  ...                     0   \n",
       "4          0.000000    0.0829    0.167  ...                     0   \n",
       "\n",
       "   num_artists_binned_6  track_genre_0  track_genre_1  track_genre_2   \n",
       "0                     0              0              0              0  \\\n",
       "1                     0              0              0              0   \n",
       "2                     0              0              0              0   \n",
       "3                     0              0              0              0   \n",
       "4                     0              0              0              0   \n",
       "\n",
       "   track_genre_3  track_genre_4  track_genre_5  track_genre_6   \n",
       "0              0              0              0              1  \\\n",
       "1              0              0              0              1   \n",
       "2              0              0              0              1   \n",
       "3              0              0              0              1   \n",
       "4              0              0              0              1   \n",
       "\n",
       "   popularity_binned  \n",
       "0                  1  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  1  \n",
       "4                  1  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the original popularity column\n",
    "df.drop(columns=\"popularity\", inplace=True)\n",
    "# Check removal\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the prediction (song popularity) from the rest of the features\n",
    "y = df.popularity_binned.values\n",
    "X = df.drop(columns=\"popularity_binned\")\n",
    "\n",
    "# Split the training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for dictionaries to hold the summary and accuracy for each iteration\n",
    "iterations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,541\n",
      "Trainable params: 7,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2662/2662 [==============================] - 2s 602us/step - loss: 0.3353 - accuracy: 0.8801\n",
      "Epoch 2/100\n",
      "2662/2662 [==============================] - 2s 601us/step - loss: 0.3081 - accuracy: 0.8848\n",
      "Epoch 3/100\n",
      "2662/2662 [==============================] - 2s 600us/step - loss: 0.2899 - accuracy: 0.8863\n",
      "Epoch 4/100\n",
      "2662/2662 [==============================] - 2s 601us/step - loss: 0.2806 - accuracy: 0.8877\n",
      "Epoch 5/100\n",
      "2662/2662 [==============================] - 2s 574us/step - loss: 0.2739 - accuracy: 0.8890\n",
      "Epoch 6/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.2700 - accuracy: 0.8895\n",
      "Epoch 7/100\n",
      "2662/2662 [==============================] - 2s 619us/step - loss: 0.2660 - accuracy: 0.8907\n",
      "Epoch 8/100\n",
      "2662/2662 [==============================] - 2s 616us/step - loss: 0.2633 - accuracy: 0.8918\n",
      "Epoch 9/100\n",
      "2662/2662 [==============================] - 2s 598us/step - loss: 0.2606 - accuracy: 0.8915\n",
      "Epoch 10/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.2580 - accuracy: 0.8941\n",
      "Epoch 11/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.2558 - accuracy: 0.8942\n",
      "Epoch 12/100\n",
      "2662/2662 [==============================] - 2s 578us/step - loss: 0.2544 - accuracy: 0.8952\n",
      "Epoch 13/100\n",
      "2662/2662 [==============================] - 2s 577us/step - loss: 0.2521 - accuracy: 0.8947\n",
      "Epoch 14/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.2499 - accuracy: 0.8962\n",
      "Epoch 15/100\n",
      "2662/2662 [==============================] - 2s 608us/step - loss: 0.2485 - accuracy: 0.8965\n",
      "Epoch 16/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.2468 - accuracy: 0.8977\n",
      "Epoch 17/100\n",
      "2662/2662 [==============================] - 2s 599us/step - loss: 0.2450 - accuracy: 0.8974\n",
      "Epoch 18/100\n",
      "2662/2662 [==============================] - 2s 598us/step - loss: 0.2432 - accuracy: 0.8983\n",
      "Epoch 19/100\n",
      "2662/2662 [==============================] - 2s 600us/step - loss: 0.2425 - accuracy: 0.8980\n",
      "Epoch 20/100\n",
      "2662/2662 [==============================] - 2s 637us/step - loss: 0.2408 - accuracy: 0.8994\n",
      "Epoch 21/100\n",
      "2662/2662 [==============================] - 2s 688us/step - loss: 0.2392 - accuracy: 0.9004\n",
      "Epoch 22/100\n",
      "2662/2662 [==============================] - 2s 679us/step - loss: 0.2378 - accuracy: 0.9003\n",
      "Epoch 23/100\n",
      "2662/2662 [==============================] - 3s 991us/step - loss: 0.2365 - accuracy: 0.9008\n",
      "Epoch 24/100\n",
      "2662/2662 [==============================] - 3s 1ms/step - loss: 0.2355 - accuracy: 0.9013\n",
      "Epoch 25/100\n",
      "2662/2662 [==============================] - 2s 856us/step - loss: 0.2342 - accuracy: 0.9021\n",
      "Epoch 26/100\n",
      "2662/2662 [==============================] - 2s 887us/step - loss: 0.2327 - accuracy: 0.9019\n",
      "Epoch 27/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2315 - accuracy: 0.9023\n",
      "Epoch 28/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.2306 - accuracy: 0.9031\n",
      "Epoch 29/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.2293 - accuracy: 0.9041\n",
      "Epoch 30/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.2286 - accuracy: 0.9043\n",
      "Epoch 31/100\n",
      "2662/2662 [==============================] - 2s 577us/step - loss: 0.2281 - accuracy: 0.9040\n",
      "Epoch 32/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.2266 - accuracy: 0.9050\n",
      "Epoch 33/100\n",
      "2662/2662 [==============================] - 2s 579us/step - loss: 0.2256 - accuracy: 0.9050\n",
      "Epoch 34/100\n",
      "2662/2662 [==============================] - 2s 590us/step - loss: 0.2255 - accuracy: 0.9058\n",
      "Epoch 35/100\n",
      "2662/2662 [==============================] - 2s 580us/step - loss: 0.2240 - accuracy: 0.9065\n",
      "Epoch 36/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.2230 - accuracy: 0.9062\n",
      "Epoch 37/100\n",
      "2662/2662 [==============================] - 2s 583us/step - loss: 0.2224 - accuracy: 0.9065\n",
      "Epoch 38/100\n",
      "2662/2662 [==============================] - 2s 595us/step - loss: 0.2215 - accuracy: 0.9074\n",
      "Epoch 39/100\n",
      "2662/2662 [==============================] - 2s 610us/step - loss: 0.2207 - accuracy: 0.9074\n",
      "Epoch 40/100\n",
      "2662/2662 [==============================] - 2s 606us/step - loss: 0.2197 - accuracy: 0.9076\n",
      "Epoch 41/100\n",
      "2662/2662 [==============================] - 2s 578us/step - loss: 0.2192 - accuracy: 0.9079\n",
      "Epoch 42/100\n",
      "2662/2662 [==============================] - 2s 581us/step - loss: 0.2193 - accuracy: 0.9076\n",
      "Epoch 43/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.2178 - accuracy: 0.9084\n",
      "Epoch 44/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.2174 - accuracy: 0.9091\n",
      "Epoch 45/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.2167 - accuracy: 0.9087\n",
      "Epoch 46/100\n",
      "2662/2662 [==============================] - 2s 577us/step - loss: 0.2159 - accuracy: 0.9096\n",
      "Epoch 47/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.2157 - accuracy: 0.9095\n",
      "Epoch 48/100\n",
      "2662/2662 [==============================] - 2s 578us/step - loss: 0.2147 - accuracy: 0.9098\n",
      "Epoch 49/100\n",
      "2662/2662 [==============================] - 2s 586us/step - loss: 0.2139 - accuracy: 0.9109\n",
      "Epoch 50/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.2139 - accuracy: 0.9101\n",
      "Epoch 51/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.2125 - accuracy: 0.9109\n",
      "Epoch 52/100\n",
      "2662/2662 [==============================] - 2s 578us/step - loss: 0.2120 - accuracy: 0.9117\n",
      "Epoch 53/100\n",
      "2662/2662 [==============================] - 2s 575us/step - loss: 0.2116 - accuracy: 0.9118\n",
      "Epoch 54/100\n",
      "2662/2662 [==============================] - 2s 583us/step - loss: 0.2110 - accuracy: 0.9113\n",
      "Epoch 55/100\n",
      "2662/2662 [==============================] - 2s 581us/step - loss: 0.2104 - accuracy: 0.9117\n",
      "Epoch 56/100\n",
      "2662/2662 [==============================] - 2s 571us/step - loss: 0.2102 - accuracy: 0.9113\n",
      "Epoch 57/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.2098 - accuracy: 0.9114\n",
      "Epoch 58/100\n",
      "2662/2662 [==============================] - 2s 581us/step - loss: 0.2096 - accuracy: 0.9119\n",
      "Epoch 59/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.2087 - accuracy: 0.9119\n",
      "Epoch 60/100\n",
      "2662/2662 [==============================] - 2s 575us/step - loss: 0.2088 - accuracy: 0.9126\n",
      "Epoch 61/100\n",
      "2662/2662 [==============================] - 2s 578us/step - loss: 0.2069 - accuracy: 0.9140\n",
      "Epoch 62/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.2073 - accuracy: 0.9132\n",
      "Epoch 63/100\n",
      "2662/2662 [==============================] - 2s 585us/step - loss: 0.2069 - accuracy: 0.9130\n",
      "Epoch 64/100\n",
      "2662/2662 [==============================] - 2s 581us/step - loss: 0.2061 - accuracy: 0.9134\n",
      "Epoch 65/100\n",
      "2662/2662 [==============================] - 2s 575us/step - loss: 0.2062 - accuracy: 0.9135\n",
      "Epoch 66/100\n",
      "2662/2662 [==============================] - 2s 577us/step - loss: 0.2057 - accuracy: 0.9137\n",
      "Epoch 67/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.2054 - accuracy: 0.9130\n",
      "Epoch 68/100\n",
      "2662/2662 [==============================] - 2s 574us/step - loss: 0.2045 - accuracy: 0.9148\n",
      "Epoch 69/100\n",
      "2662/2662 [==============================] - 2s 586us/step - loss: 0.2046 - accuracy: 0.9138\n",
      "Epoch 70/100\n",
      "2662/2662 [==============================] - 2s 574us/step - loss: 0.2038 - accuracy: 0.9146\n",
      "Epoch 71/100\n",
      "2662/2662 [==============================] - 2s 580us/step - loss: 0.2032 - accuracy: 0.9144\n",
      "Epoch 72/100\n",
      "2662/2662 [==============================] - 2s 569us/step - loss: 0.2031 - accuracy: 0.9148\n",
      "Epoch 73/100\n",
      "2662/2662 [==============================] - 2s 610us/step - loss: 0.2031 - accuracy: 0.9150\n",
      "Epoch 74/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.2024 - accuracy: 0.9150\n",
      "Epoch 75/100\n",
      "2662/2662 [==============================] - 2s 583us/step - loss: 0.2023 - accuracy: 0.9147\n",
      "Epoch 76/100\n",
      "2662/2662 [==============================] - 2s 574us/step - loss: 0.2021 - accuracy: 0.9154\n",
      "Epoch 77/100\n",
      "2662/2662 [==============================] - 2s 574us/step - loss: 0.2018 - accuracy: 0.9153\n",
      "Epoch 78/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.2011 - accuracy: 0.9158\n",
      "Epoch 79/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.2006 - accuracy: 0.9148\n",
      "Epoch 80/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.2002 - accuracy: 0.9153\n",
      "Epoch 81/100\n",
      "2662/2662 [==============================] - 2s 576us/step - loss: 0.2000 - accuracy: 0.9162\n",
      "Epoch 82/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.1997 - accuracy: 0.9164\n",
      "Epoch 83/100\n",
      "2662/2662 [==============================] - 2s 575us/step - loss: 0.1998 - accuracy: 0.9163\n",
      "Epoch 84/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.1995 - accuracy: 0.9161\n",
      "Epoch 85/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.1989 - accuracy: 0.9167\n",
      "Epoch 86/100\n",
      "2662/2662 [==============================] - 2s 606us/step - loss: 0.1986 - accuracy: 0.9172\n",
      "Epoch 87/100\n",
      "2662/2662 [==============================] - 2s 580us/step - loss: 0.1986 - accuracy: 0.9164\n",
      "Epoch 88/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.1981 - accuracy: 0.9164\n",
      "Epoch 89/100\n",
      "2662/2662 [==============================] - 2s 575us/step - loss: 0.1976 - accuracy: 0.9166\n",
      "Epoch 90/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.1980 - accuracy: 0.9167\n",
      "Epoch 91/100\n",
      "2662/2662 [==============================] - 2s 599us/step - loss: 0.1972 - accuracy: 0.9171\n",
      "Epoch 92/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.1970 - accuracy: 0.9171\n",
      "Epoch 93/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.1969 - accuracy: 0.9176\n",
      "Epoch 94/100\n",
      "2662/2662 [==============================] - 2s 569us/step - loss: 0.1961 - accuracy: 0.9175\n",
      "Epoch 95/100\n",
      "2662/2662 [==============================] - 2s 577us/step - loss: 0.1960 - accuracy: 0.9172\n",
      "Epoch 96/100\n",
      "2662/2662 [==============================] - 2s 604us/step - loss: 0.1959 - accuracy: 0.9175\n",
      "Epoch 97/100\n",
      "2662/2662 [==============================] - 2s 576us/step - loss: 0.1959 - accuracy: 0.9173\n",
      "Epoch 98/100\n",
      "2662/2662 [==============================] - 2s 571us/step - loss: 0.1953 - accuracy: 0.9186\n",
      "Epoch 99/100\n",
      "2662/2662 [==============================] - 2s 585us/step - loss: 0.1954 - accuracy: 0.9181\n",
      "Epoch 100/100\n",
      "2662/2662 [==============================] - 2s 581us/step - loss: 0.1944 - accuracy: 0.9187\n",
      "888/888 - 0s - loss: 0.3442 - accuracy: 0.8684 - 441ms/epoch - 497us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 1: binary population but no other changes to the data processing, with Standard Scaler normalization\n",
    "# Create a dictionary for iteration 1\n",
    "iteration_1 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_1[\"Iteration\"] = 1\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_1 = scaler1.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_1 = X_scaler_1.transform(X_train)\n",
    "X_test_scaled_1 = X_scaler_1.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "nn_1 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_1.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_1 = nn_1.fit(X_train_scaled_1, y_train, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_1, model_accuracy_1 = nn_1.evaluate(X_test_scaled_1, y_test, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_1 dictionary\n",
    "iteration_1[\"Loss\"] = model_loss_1\n",
    "iteration_1[\"Accuracy\"] = model_accuracy_1\n",
    "\n",
    "# Add the iteration_1 dictionary to the iterations list\n",
    "iterations.append(iteration_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,541\n",
      "Trainable params: 7,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2662/2662 [==============================] - 2s 607us/step - loss: 0.3344 - accuracy: 0.8808\n",
      "Epoch 2/100\n",
      "2662/2662 [==============================] - 2s 624us/step - loss: 0.3043 - accuracy: 0.8843\n",
      "Epoch 3/100\n",
      "2662/2662 [==============================] - 2s 613us/step - loss: 0.2905 - accuracy: 0.8860\n",
      "Epoch 4/100\n",
      "2662/2662 [==============================] - 2s 621us/step - loss: 0.2833 - accuracy: 0.8861\n",
      "Epoch 5/100\n",
      "2662/2662 [==============================] - 2s 621us/step - loss: 0.2792 - accuracy: 0.8872\n",
      "Epoch 6/100\n",
      "2662/2662 [==============================] - 2s 614us/step - loss: 0.2760 - accuracy: 0.8870\n",
      "Epoch 7/100\n",
      "2662/2662 [==============================] - 2s 619us/step - loss: 0.2735 - accuracy: 0.8872\n",
      "Epoch 8/100\n",
      "2662/2662 [==============================] - 2s 611us/step - loss: 0.2717 - accuracy: 0.8891\n",
      "Epoch 9/100\n",
      "2662/2662 [==============================] - 2s 625us/step - loss: 0.2688 - accuracy: 0.8894\n",
      "Epoch 10/100\n",
      "2662/2662 [==============================] - 2s 612us/step - loss: 0.2679 - accuracy: 0.8900\n",
      "Epoch 11/100\n",
      "2662/2662 [==============================] - 2s 621us/step - loss: 0.2662 - accuracy: 0.8903\n",
      "Epoch 12/100\n",
      "2662/2662 [==============================] - 2s 606us/step - loss: 0.2648 - accuracy: 0.8904\n",
      "Epoch 13/100\n",
      "2662/2662 [==============================] - 2s 622us/step - loss: 0.2634 - accuracy: 0.8906\n",
      "Epoch 14/100\n",
      "2662/2662 [==============================] - 2s 610us/step - loss: 0.2621 - accuracy: 0.8917\n",
      "Epoch 15/100\n",
      "2662/2662 [==============================] - 2s 647us/step - loss: 0.2610 - accuracy: 0.8919\n",
      "Epoch 16/100\n",
      "2662/2662 [==============================] - 2s 607us/step - loss: 0.2596 - accuracy: 0.8916\n",
      "Epoch 17/100\n",
      "2662/2662 [==============================] - 2s 625us/step - loss: 0.2583 - accuracy: 0.8925\n",
      "Epoch 18/100\n",
      "2662/2662 [==============================] - 2s 610us/step - loss: 0.2568 - accuracy: 0.8925\n",
      "Epoch 19/100\n",
      "2662/2662 [==============================] - 2s 624us/step - loss: 0.2560 - accuracy: 0.8930\n",
      "Epoch 20/100\n",
      "2662/2662 [==============================] - 2s 616us/step - loss: 0.2550 - accuracy: 0.8937\n",
      "Epoch 21/100\n",
      "2662/2662 [==============================] - 2s 625us/step - loss: 0.2535 - accuracy: 0.8937\n",
      "Epoch 22/100\n",
      "2662/2662 [==============================] - 2s 620us/step - loss: 0.2524 - accuracy: 0.8946\n",
      "Epoch 23/100\n",
      "2662/2662 [==============================] - 2s 628us/step - loss: 0.2516 - accuracy: 0.8945\n",
      "Epoch 24/100\n",
      "2662/2662 [==============================] - 2s 630us/step - loss: 0.2510 - accuracy: 0.8946\n",
      "Epoch 25/100\n",
      "2662/2662 [==============================] - 2s 617us/step - loss: 0.2499 - accuracy: 0.8956\n",
      "Epoch 26/100\n",
      "2662/2662 [==============================] - 2s 629us/step - loss: 0.2489 - accuracy: 0.8957\n",
      "Epoch 27/100\n",
      "2662/2662 [==============================] - 2s 615us/step - loss: 0.2481 - accuracy: 0.8960\n",
      "Epoch 28/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2474 - accuracy: 0.8956\n",
      "Epoch 29/100\n",
      "2662/2662 [==============================] - 2s 617us/step - loss: 0.2463 - accuracy: 0.8968\n",
      "Epoch 30/100\n",
      "2662/2662 [==============================] - 2s 616us/step - loss: 0.2454 - accuracy: 0.8973\n",
      "Epoch 31/100\n",
      "2662/2662 [==============================] - 2s 628us/step - loss: 0.2453 - accuracy: 0.8976\n",
      "Epoch 32/100\n",
      "2662/2662 [==============================] - 2s 615us/step - loss: 0.2440 - accuracy: 0.8968\n",
      "Epoch 33/100\n",
      "2662/2662 [==============================] - 2s 626us/step - loss: 0.2431 - accuracy: 0.8983\n",
      "Epoch 34/100\n",
      "2662/2662 [==============================] - 2s 617us/step - loss: 0.2429 - accuracy: 0.8976\n",
      "Epoch 35/100\n",
      "2662/2662 [==============================] - 2s 637us/step - loss: 0.2413 - accuracy: 0.8983\n",
      "Epoch 36/100\n",
      "2662/2662 [==============================] - 2s 625us/step - loss: 0.2412 - accuracy: 0.8988\n",
      "Epoch 37/100\n",
      "2662/2662 [==============================] - 2s 624us/step - loss: 0.2401 - accuracy: 0.8988\n",
      "Epoch 38/100\n",
      "2662/2662 [==============================] - 2s 620us/step - loss: 0.2399 - accuracy: 0.8994\n",
      "Epoch 39/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2387 - accuracy: 0.8993\n",
      "Epoch 40/100\n",
      "2662/2662 [==============================] - 2s 624us/step - loss: 0.2381 - accuracy: 0.9002\n",
      "Epoch 41/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2374 - accuracy: 0.9001\n",
      "Epoch 42/100\n",
      "2662/2662 [==============================] - 2s 625us/step - loss: 0.2368 - accuracy: 0.9002\n",
      "Epoch 43/100\n",
      "2662/2662 [==============================] - 2s 619us/step - loss: 0.2370 - accuracy: 0.9001\n",
      "Epoch 44/100\n",
      "2662/2662 [==============================] - 2s 619us/step - loss: 0.2355 - accuracy: 0.9014\n",
      "Epoch 45/100\n",
      "2662/2662 [==============================] - 2s 624us/step - loss: 0.2353 - accuracy: 0.9010\n",
      "Epoch 46/100\n",
      "2662/2662 [==============================] - 2s 626us/step - loss: 0.2347 - accuracy: 0.9018\n",
      "Epoch 47/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2336 - accuracy: 0.9012\n",
      "Epoch 48/100\n",
      "2662/2662 [==============================] - 2s 625us/step - loss: 0.2334 - accuracy: 0.9016\n",
      "Epoch 49/100\n",
      "2662/2662 [==============================] - 2s 634us/step - loss: 0.2329 - accuracy: 0.9025\n",
      "Epoch 50/100\n",
      "2662/2662 [==============================] - 2s 617us/step - loss: 0.2324 - accuracy: 0.9016\n",
      "Epoch 51/100\n",
      "2662/2662 [==============================] - 2s 615us/step - loss: 0.2319 - accuracy: 0.9025\n",
      "Epoch 52/100\n",
      "2662/2662 [==============================] - 2s 618us/step - loss: 0.2312 - accuracy: 0.9032\n",
      "Epoch 53/100\n",
      "2662/2662 [==============================] - 2s 630us/step - loss: 0.2313 - accuracy: 0.9031\n",
      "Epoch 54/100\n",
      "2662/2662 [==============================] - 2s 633us/step - loss: 0.2299 - accuracy: 0.9038\n",
      "Epoch 55/100\n",
      "2662/2662 [==============================] - 2s 614us/step - loss: 0.2298 - accuracy: 0.9026\n",
      "Epoch 56/100\n",
      "2662/2662 [==============================] - 2s 624us/step - loss: 0.2292 - accuracy: 0.9038\n",
      "Epoch 57/100\n",
      "2662/2662 [==============================] - 2s 624us/step - loss: 0.2281 - accuracy: 0.9043\n",
      "Epoch 58/100\n",
      "2662/2662 [==============================] - 2s 618us/step - loss: 0.2283 - accuracy: 0.9040\n",
      "Epoch 59/100\n",
      "2662/2662 [==============================] - 2s 612us/step - loss: 0.2277 - accuracy: 0.9041\n",
      "Epoch 60/100\n",
      "2662/2662 [==============================] - 2s 624us/step - loss: 0.2275 - accuracy: 0.9045\n",
      "Epoch 61/100\n",
      "2662/2662 [==============================] - 2s 618us/step - loss: 0.2273 - accuracy: 0.9047\n",
      "Epoch 62/100\n",
      "2662/2662 [==============================] - 2s 611us/step - loss: 0.2264 - accuracy: 0.9046\n",
      "Epoch 63/100\n",
      "2662/2662 [==============================] - 2s 616us/step - loss: 0.2261 - accuracy: 0.9046\n",
      "Epoch 64/100\n",
      "2662/2662 [==============================] - 2s 607us/step - loss: 0.2258 - accuracy: 0.9044\n",
      "Epoch 65/100\n",
      "2662/2662 [==============================] - 2s 622us/step - loss: 0.2256 - accuracy: 0.9046\n",
      "Epoch 66/100\n",
      "2662/2662 [==============================] - 2s 610us/step - loss: 0.2248 - accuracy: 0.9050\n",
      "Epoch 67/100\n",
      "2662/2662 [==============================] - 2s 621us/step - loss: 0.2248 - accuracy: 0.9053\n",
      "Epoch 68/100\n",
      "2662/2662 [==============================] - 2s 618us/step - loss: 0.2241 - accuracy: 0.9053\n",
      "Epoch 69/100\n",
      "2662/2662 [==============================] - 2s 605us/step - loss: 0.2233 - accuracy: 0.9057\n",
      "Epoch 70/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2235 - accuracy: 0.9058\n",
      "Epoch 71/100\n",
      "2662/2662 [==============================] - 2s 610us/step - loss: 0.2234 - accuracy: 0.9054\n",
      "Epoch 72/100\n",
      "2662/2662 [==============================] - 2s 626us/step - loss: 0.2229 - accuracy: 0.9063\n",
      "Epoch 73/100\n",
      "2662/2662 [==============================] - 2s 620us/step - loss: 0.2225 - accuracy: 0.9062\n",
      "Epoch 74/100\n",
      "2662/2662 [==============================] - 2s 622us/step - loss: 0.2223 - accuracy: 0.9062\n",
      "Epoch 75/100\n",
      "2662/2662 [==============================] - 2s 619us/step - loss: 0.2216 - accuracy: 0.9064\n",
      "Epoch 76/100\n",
      "2662/2662 [==============================] - 2s 611us/step - loss: 0.2209 - accuracy: 0.9069\n",
      "Epoch 77/100\n",
      "2662/2662 [==============================] - 2s 618us/step - loss: 0.2208 - accuracy: 0.9064\n",
      "Epoch 78/100\n",
      "2662/2662 [==============================] - 2s 619us/step - loss: 0.2208 - accuracy: 0.9072\n",
      "Epoch 79/100\n",
      "2662/2662 [==============================] - 2s 614us/step - loss: 0.2202 - accuracy: 0.9068\n",
      "Epoch 80/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2199 - accuracy: 0.9068\n",
      "Epoch 81/100\n",
      "2662/2662 [==============================] - 2s 622us/step - loss: 0.2192 - accuracy: 0.9068\n",
      "Epoch 82/100\n",
      "2662/2662 [==============================] - 2s 617us/step - loss: 0.2190 - accuracy: 0.9067\n",
      "Epoch 83/100\n",
      "2662/2662 [==============================] - 2s 636us/step - loss: 0.2188 - accuracy: 0.9079\n",
      "Epoch 84/100\n",
      "2662/2662 [==============================] - 2s 620us/step - loss: 0.2186 - accuracy: 0.9068\n",
      "Epoch 85/100\n",
      "2662/2662 [==============================] - 2s 621us/step - loss: 0.2182 - accuracy: 0.9080\n",
      "Epoch 86/100\n",
      "2662/2662 [==============================] - 2s 613us/step - loss: 0.2177 - accuracy: 0.9081\n",
      "Epoch 87/100\n",
      "2662/2662 [==============================] - 2s 627us/step - loss: 0.2174 - accuracy: 0.9086\n",
      "Epoch 88/100\n",
      "2662/2662 [==============================] - 2s 613us/step - loss: 0.2173 - accuracy: 0.9075\n",
      "Epoch 89/100\n",
      "2662/2662 [==============================] - 2s 630us/step - loss: 0.2169 - accuracy: 0.9090\n",
      "Epoch 90/100\n",
      "2662/2662 [==============================] - 2s 617us/step - loss: 0.2169 - accuracy: 0.9081\n",
      "Epoch 91/100\n",
      "2662/2662 [==============================] - 2s 615us/step - loss: 0.2169 - accuracy: 0.9082\n",
      "Epoch 92/100\n",
      "2662/2662 [==============================] - 2s 625us/step - loss: 0.2161 - accuracy: 0.9085\n",
      "Epoch 93/100\n",
      "2662/2662 [==============================] - 2s 629us/step - loss: 0.2161 - accuracy: 0.9087\n",
      "Epoch 94/100\n",
      "2662/2662 [==============================] - 2s 617us/step - loss: 0.2160 - accuracy: 0.9086\n",
      "Epoch 95/100\n",
      "2662/2662 [==============================] - 2s 629us/step - loss: 0.2157 - accuracy: 0.9090\n",
      "Epoch 96/100\n",
      "2662/2662 [==============================] - 2s 654us/step - loss: 0.2155 - accuracy: 0.9085\n",
      "Epoch 97/100\n",
      "2662/2662 [==============================] - 2s 613us/step - loss: 0.2148 - accuracy: 0.9089\n",
      "Epoch 98/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2143 - accuracy: 0.9090\n",
      "Epoch 99/100\n",
      "2662/2662 [==============================] - 2s 620us/step - loss: 0.2145 - accuracy: 0.9092\n",
      "Epoch 100/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2139 - accuracy: 0.9096\n",
      "888/888 - 0s - loss: 0.3235 - accuracy: 0.8793 - 440ms/epoch - 495us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 2: Same as above, but Min Max normalization\n",
    "# Create a dictionary for iteration 2\n",
    "iteration_2 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_2[\"Iteration\"] = 2\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler2 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_2 = scaler2.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_2 = X_scaler_2.transform(X_train)\n",
    "X_test_scaled_2 = X_scaler_2.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "nn_2 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_2.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_2 = nn_2.fit(X_train_scaled_2, y_train, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_2, model_accuracy_2 = nn_2.evaluate(X_test_scaled_2, y_test, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_2 dictionary\n",
    "iteration_2[\"Loss\"] = model_loss_2\n",
    "iteration_2[\"Accuracy\"] = model_accuracy_2\n",
    "\n",
    "# Add the iteration_2 dictionary to the iterations list\n",
    "iterations.append(iteration_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,541\n",
      "Trainable params: 7,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4687/4687 [==============================] - 3s 612us/step - loss: 0.5465 - accuracy: 0.7160\n",
      "Epoch 2/100\n",
      "4687/4687 [==============================] - 3s 605us/step - loss: 0.4748 - accuracy: 0.7688\n",
      "Epoch 3/100\n",
      "4687/4687 [==============================] - 3s 608us/step - loss: 0.4559 - accuracy: 0.7792\n",
      "Epoch 4/100\n",
      "4687/4687 [==============================] - 3s 598us/step - loss: 0.4428 - accuracy: 0.7856\n",
      "Epoch 5/100\n",
      "4687/4687 [==============================] - 3s 624us/step - loss: 0.4323 - accuracy: 0.7926\n",
      "Epoch 6/100\n",
      "4687/4687 [==============================] - 3s 596us/step - loss: 0.4231 - accuracy: 0.7977\n",
      "Epoch 7/100\n",
      "4687/4687 [==============================] - 3s 585us/step - loss: 0.4162 - accuracy: 0.8020\n",
      "Epoch 8/100\n",
      "4687/4687 [==============================] - 3s 579us/step - loss: 0.4085 - accuracy: 0.8070\n",
      "Epoch 9/100\n",
      "4687/4687 [==============================] - 3s 592us/step - loss: 0.4027 - accuracy: 0.8111\n",
      "Epoch 10/100\n",
      "4687/4687 [==============================] - 3s 720us/step - loss: 0.3965 - accuracy: 0.8143\n",
      "Epoch 11/100\n",
      "4687/4687 [==============================] - 4s 803us/step - loss: 0.3913 - accuracy: 0.8186\n",
      "Epoch 12/100\n",
      "4687/4687 [==============================] - 4s 754us/step - loss: 0.3858 - accuracy: 0.8215\n",
      "Epoch 13/100\n",
      "4687/4687 [==============================] - 3s 681us/step - loss: 0.3821 - accuracy: 0.8226\n",
      "Epoch 14/100\n",
      "4687/4687 [==============================] - 3s 687us/step - loss: 0.3772 - accuracy: 0.8273\n",
      "Epoch 15/100\n",
      "4687/4687 [==============================] - 3s 653us/step - loss: 0.3739 - accuracy: 0.8282\n",
      "Epoch 16/100\n",
      "4687/4687 [==============================] - 3s 638us/step - loss: 0.3704 - accuracy: 0.8311\n",
      "Epoch 17/100\n",
      "4687/4687 [==============================] - 3s 616us/step - loss: 0.3668 - accuracy: 0.8328\n",
      "Epoch 18/100\n",
      "4687/4687 [==============================] - 3s 617us/step - loss: 0.3634 - accuracy: 0.8340\n",
      "Epoch 19/100\n",
      "4687/4687 [==============================] - 3s 647us/step - loss: 0.3610 - accuracy: 0.8359\n",
      "Epoch 20/100\n",
      "4687/4687 [==============================] - 3s 654us/step - loss: 0.3579 - accuracy: 0.8375\n",
      "Epoch 21/100\n",
      "4687/4687 [==============================] - 3s 607us/step - loss: 0.3551 - accuracy: 0.8400\n",
      "Epoch 22/100\n",
      "4687/4687 [==============================] - 3s 609us/step - loss: 0.3521 - accuracy: 0.8408\n",
      "Epoch 23/100\n",
      "4687/4687 [==============================] - 3s 602us/step - loss: 0.3494 - accuracy: 0.8432\n",
      "Epoch 24/100\n",
      "4687/4687 [==============================] - 3s 615us/step - loss: 0.3478 - accuracy: 0.8451\n",
      "Epoch 25/100\n",
      "4687/4687 [==============================] - 3s 611us/step - loss: 0.3463 - accuracy: 0.8459\n",
      "Epoch 26/100\n",
      "4687/4687 [==============================] - 3s 612us/step - loss: 0.3426 - accuracy: 0.8475\n",
      "Epoch 27/100\n",
      "4687/4687 [==============================] - 3s 623us/step - loss: 0.3407 - accuracy: 0.8491\n",
      "Epoch 28/100\n",
      "4687/4687 [==============================] - 3s 620us/step - loss: 0.3397 - accuracy: 0.8493\n",
      "Epoch 29/100\n",
      "4687/4687 [==============================] - 3s 623us/step - loss: 0.3371 - accuracy: 0.8501\n",
      "Epoch 30/100\n",
      "4687/4687 [==============================] - 3s 626us/step - loss: 0.3359 - accuracy: 0.8514\n",
      "Epoch 31/100\n",
      "4687/4687 [==============================] - 3s 619us/step - loss: 0.3331 - accuracy: 0.8533\n",
      "Epoch 32/100\n",
      "4687/4687 [==============================] - 3s 633us/step - loss: 0.3319 - accuracy: 0.8545\n",
      "Epoch 33/100\n",
      "4687/4687 [==============================] - 3s 634us/step - loss: 0.3302 - accuracy: 0.8555\n",
      "Epoch 34/100\n",
      "4687/4687 [==============================] - 3s 626us/step - loss: 0.3287 - accuracy: 0.8564\n",
      "Epoch 35/100\n",
      "4687/4687 [==============================] - 3s 650us/step - loss: 0.3270 - accuracy: 0.8566\n",
      "Epoch 36/100\n",
      "4687/4687 [==============================] - 3s 608us/step - loss: 0.3259 - accuracy: 0.8575\n",
      "Epoch 37/100\n",
      "4687/4687 [==============================] - 3s 618us/step - loss: 0.3247 - accuracy: 0.8585\n",
      "Epoch 38/100\n",
      "4687/4687 [==============================] - 3s 628us/step - loss: 0.3226 - accuracy: 0.8602\n",
      "Epoch 39/100\n",
      "4687/4687 [==============================] - 3s 604us/step - loss: 0.3214 - accuracy: 0.8591\n",
      "Epoch 40/100\n",
      "4687/4687 [==============================] - 3s 605us/step - loss: 0.3205 - accuracy: 0.8603\n",
      "Epoch 41/100\n",
      "4687/4687 [==============================] - 3s 634us/step - loss: 0.3189 - accuracy: 0.8619\n",
      "Epoch 42/100\n",
      "4687/4687 [==============================] - 3s 607us/step - loss: 0.3178 - accuracy: 0.8630\n",
      "Epoch 43/100\n",
      "4687/4687 [==============================] - 3s 616us/step - loss: 0.3161 - accuracy: 0.8626\n",
      "Epoch 44/100\n",
      "4687/4687 [==============================] - 3s 632us/step - loss: 0.3154 - accuracy: 0.8640\n",
      "Epoch 45/100\n",
      "4687/4687 [==============================] - 3s 661us/step - loss: 0.3142 - accuracy: 0.8647\n",
      "Epoch 46/100\n",
      "4687/4687 [==============================] - 3s 646us/step - loss: 0.3130 - accuracy: 0.8648\n",
      "Epoch 47/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.3117 - accuracy: 0.8657\n",
      "Epoch 48/100\n",
      "4687/4687 [==============================] - 3s 601us/step - loss: 0.3114 - accuracy: 0.8659\n",
      "Epoch 49/100\n",
      "4687/4687 [==============================] - 3s 602us/step - loss: 0.3098 - accuracy: 0.8673\n",
      "Epoch 50/100\n",
      "4687/4687 [==============================] - 3s 594us/step - loss: 0.3088 - accuracy: 0.8675\n",
      "Epoch 51/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.3082 - accuracy: 0.8680\n",
      "Epoch 52/100\n",
      "4687/4687 [==============================] - 3s 583us/step - loss: 0.3072 - accuracy: 0.8681\n",
      "Epoch 53/100\n",
      "4687/4687 [==============================] - 3s 582us/step - loss: 0.3066 - accuracy: 0.8687\n",
      "Epoch 54/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.3049 - accuracy: 0.8694\n",
      "Epoch 55/100\n",
      "4687/4687 [==============================] - 3s 624us/step - loss: 0.3047 - accuracy: 0.8697\n",
      "Epoch 56/100\n",
      "4687/4687 [==============================] - 3s 595us/step - loss: 0.3040 - accuracy: 0.8699\n",
      "Epoch 57/100\n",
      "4687/4687 [==============================] - 3s 595us/step - loss: 0.3030 - accuracy: 0.8703\n",
      "Epoch 58/100\n",
      "4687/4687 [==============================] - 3s 601us/step - loss: 0.3023 - accuracy: 0.8715\n",
      "Epoch 59/100\n",
      "4687/4687 [==============================] - 3s 617us/step - loss: 0.3019 - accuracy: 0.8719\n",
      "Epoch 60/100\n",
      "4687/4687 [==============================] - 3s 639us/step - loss: 0.3011 - accuracy: 0.8714\n",
      "Epoch 61/100\n",
      "4687/4687 [==============================] - 3s 647us/step - loss: 0.3008 - accuracy: 0.8709\n",
      "Epoch 62/100\n",
      "4687/4687 [==============================] - 3s 649us/step - loss: 0.2990 - accuracy: 0.8726\n",
      "Epoch 63/100\n",
      "4687/4687 [==============================] - 3s 643us/step - loss: 0.2988 - accuracy: 0.8723\n",
      "Epoch 64/100\n",
      "4687/4687 [==============================] - 3s 650us/step - loss: 0.2976 - accuracy: 0.8734\n",
      "Epoch 65/100\n",
      "4687/4687 [==============================] - 3s 649us/step - loss: 0.2982 - accuracy: 0.8731\n",
      "Epoch 66/100\n",
      "4687/4687 [==============================] - 3s 630us/step - loss: 0.2972 - accuracy: 0.8736\n",
      "Epoch 67/100\n",
      "4687/4687 [==============================] - 3s 630us/step - loss: 0.2966 - accuracy: 0.8743\n",
      "Epoch 68/100\n",
      "4687/4687 [==============================] - 3s 650us/step - loss: 0.2961 - accuracy: 0.8741\n",
      "Epoch 69/100\n",
      "4687/4687 [==============================] - 3s 665us/step - loss: 0.2953 - accuracy: 0.8745\n",
      "Epoch 70/100\n",
      "4687/4687 [==============================] - 3s 683us/step - loss: 0.2946 - accuracy: 0.8764\n",
      "Epoch 71/100\n",
      "4687/4687 [==============================] - 3s 683us/step - loss: 0.2947 - accuracy: 0.8749\n",
      "Epoch 72/100\n",
      "4687/4687 [==============================] - 3s 655us/step - loss: 0.2939 - accuracy: 0.8755\n",
      "Epoch 73/100\n",
      "4687/4687 [==============================] - 3s 670us/step - loss: 0.2928 - accuracy: 0.8764\n",
      "Epoch 74/100\n",
      "4687/4687 [==============================] - 3s 681us/step - loss: 0.2926 - accuracy: 0.8763\n",
      "Epoch 75/100\n",
      "4687/4687 [==============================] - 3s 671us/step - loss: 0.2924 - accuracy: 0.8768\n",
      "Epoch 76/100\n",
      "4687/4687 [==============================] - 3s 663us/step - loss: 0.2913 - accuracy: 0.8772\n",
      "Epoch 77/100\n",
      "4687/4687 [==============================] - 3s 672us/step - loss: 0.2908 - accuracy: 0.8778\n",
      "Epoch 78/100\n",
      "4687/4687 [==============================] - 3s 687us/step - loss: 0.2900 - accuracy: 0.8784\n",
      "Epoch 79/100\n",
      "4687/4687 [==============================] - 3s 646us/step - loss: 0.2899 - accuracy: 0.8788\n",
      "Epoch 80/100\n",
      "4687/4687 [==============================] - 3s 625us/step - loss: 0.2895 - accuracy: 0.8788\n",
      "Epoch 81/100\n",
      "4687/4687 [==============================] - 3s 599us/step - loss: 0.2889 - accuracy: 0.8795\n",
      "Epoch 82/100\n",
      "4687/4687 [==============================] - 3s 623us/step - loss: 0.2879 - accuracy: 0.8796\n",
      "Epoch 83/100\n",
      "4687/4687 [==============================] - 3s 616us/step - loss: 0.2876 - accuracy: 0.8797\n",
      "Epoch 84/100\n",
      "4687/4687 [==============================] - 3s 619us/step - loss: 0.2873 - accuracy: 0.8797\n",
      "Epoch 85/100\n",
      "4687/4687 [==============================] - 3s 635us/step - loss: 0.2862 - accuracy: 0.8807\n",
      "Epoch 86/100\n",
      "4687/4687 [==============================] - 3s 630us/step - loss: 0.2861 - accuracy: 0.8811\n",
      "Epoch 87/100\n",
      "4687/4687 [==============================] - 3s 627us/step - loss: 0.2860 - accuracy: 0.8807\n",
      "Epoch 88/100\n",
      "4687/4687 [==============================] - 3s 593us/step - loss: 0.2852 - accuracy: 0.8814\n",
      "Epoch 89/100\n",
      "4687/4687 [==============================] - 3s 632us/step - loss: 0.2847 - accuracy: 0.8816\n",
      "Epoch 90/100\n",
      "4687/4687 [==============================] - 3s 617us/step - loss: 0.2838 - accuracy: 0.8819\n",
      "Epoch 91/100\n",
      "4687/4687 [==============================] - 3s 608us/step - loss: 0.2839 - accuracy: 0.8818\n",
      "Epoch 92/100\n",
      "4687/4687 [==============================] - 3s 618us/step - loss: 0.2835 - accuracy: 0.8823\n",
      "Epoch 93/100\n",
      "4687/4687 [==============================] - 3s 632us/step - loss: 0.2832 - accuracy: 0.8827\n",
      "Epoch 94/100\n",
      "4687/4687 [==============================] - 3s 665us/step - loss: 0.2824 - accuracy: 0.8835\n",
      "Epoch 95/100\n",
      "4687/4687 [==============================] - 3s 649us/step - loss: 0.2820 - accuracy: 0.8834\n",
      "Epoch 96/100\n",
      "4687/4687 [==============================] - 3s 668us/step - loss: 0.2816 - accuracy: 0.8830\n",
      "Epoch 97/100\n",
      "4687/4687 [==============================] - 3s 674us/step - loss: 0.2811 - accuracy: 0.8837\n",
      "Epoch 98/100\n",
      "4687/4687 [==============================] - 3s 681us/step - loss: 0.2808 - accuracy: 0.8836\n",
      "Epoch 99/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.2808 - accuracy: 0.8831\n",
      "Epoch 100/100\n",
      "4687/4687 [==============================] - 3s 681us/step - loss: 0.2802 - accuracy: 0.8840\n",
      "888/888 - 0s - loss: 0.5453 - accuracy: 0.7952 - 455ms/epoch - 512us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 3: Same as above, but adds random oversampling to adjust for skewed data\n",
    "# Create a dictionary for iteration 3\n",
    "iteration_3 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_3[\"Iteration\"] = 3\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler3 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_3 = scaler3.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_3 = X_scaler_3.transform(X_train)\n",
    "X_test_scaled_3 = X_scaler_3.transform(X_test)\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_3 = RandomOverSampler(random_state=1)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_3, y_ros_3 = ros_3.fit_resample(X_train_scaled_3, y_train)\n",
    "\n",
    "# Define the model\n",
    "nn_3 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_3.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_3 = nn_3.fit(X_ros_3, y_ros_3, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_3, model_accuracy_3 = nn_3.evaluate(X_test_scaled_3, y_test, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_3 dictionary\n",
    "iteration_3[\"Loss\"] = model_loss_3\n",
    "iteration_3[\"Accuracy\"] = model_accuracy_3\n",
    "\n",
    "# Add the iteration_3 dictionary to the iterations list\n",
    "iterations.append(iteration_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,541\n",
      "Trainable params: 7,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4687/4687 [==============================] - 3s 655us/step - loss: 0.5594 - accuracy: 0.7040\n",
      "Epoch 2/100\n",
      "4687/4687 [==============================] - 3s 672us/step - loss: 0.4812 - accuracy: 0.7665\n",
      "Epoch 3/100\n",
      "4687/4687 [==============================] - 3s 689us/step - loss: 0.4601 - accuracy: 0.7787\n",
      "Epoch 4/100\n",
      "4687/4687 [==============================] - 3s 689us/step - loss: 0.4466 - accuracy: 0.7865\n",
      "Epoch 5/100\n",
      "4687/4687 [==============================] - 3s 684us/step - loss: 0.4362 - accuracy: 0.7917\n",
      "Epoch 6/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.4271 - accuracy: 0.7965\n",
      "Epoch 7/100\n",
      "4687/4687 [==============================] - 3s 676us/step - loss: 0.4191 - accuracy: 0.8020\n",
      "Epoch 8/100\n",
      "4687/4687 [==============================] - 3s 683us/step - loss: 0.4121 - accuracy: 0.8055\n",
      "Epoch 9/100\n",
      "4687/4687 [==============================] - 3s 693us/step - loss: 0.4056 - accuracy: 0.8097\n",
      "Epoch 10/100\n",
      "4687/4687 [==============================] - 3s 685us/step - loss: 0.3999 - accuracy: 0.8133\n",
      "Epoch 11/100\n",
      "4687/4687 [==============================] - 3s 671us/step - loss: 0.3943 - accuracy: 0.8168\n",
      "Epoch 12/100\n",
      "4687/4687 [==============================] - 3s 677us/step - loss: 0.3900 - accuracy: 0.8197\n",
      "Epoch 13/100\n",
      "4687/4687 [==============================] - 3s 675us/step - loss: 0.3853 - accuracy: 0.8220\n",
      "Epoch 14/100\n",
      "4687/4687 [==============================] - 3s 668us/step - loss: 0.3817 - accuracy: 0.8240\n",
      "Epoch 15/100\n",
      "4687/4687 [==============================] - 3s 666us/step - loss: 0.3782 - accuracy: 0.8265\n",
      "Epoch 16/100\n",
      "4687/4687 [==============================] - 3s 651us/step - loss: 0.3747 - accuracy: 0.8289\n",
      "Epoch 17/100\n",
      "4687/4687 [==============================] - 3s 635us/step - loss: 0.3710 - accuracy: 0.8307\n",
      "Epoch 18/100\n",
      "4687/4687 [==============================] - 3s 663us/step - loss: 0.3672 - accuracy: 0.8339\n",
      "Epoch 19/100\n",
      "4687/4687 [==============================] - 3s 665us/step - loss: 0.3656 - accuracy: 0.8341\n",
      "Epoch 20/100\n",
      "4687/4687 [==============================] - 3s 641us/step - loss: 0.3624 - accuracy: 0.8359\n",
      "Epoch 21/100\n",
      "4687/4687 [==============================] - 3s 669us/step - loss: 0.3599 - accuracy: 0.8380\n",
      "Epoch 22/100\n",
      "4687/4687 [==============================] - 3s 675us/step - loss: 0.3575 - accuracy: 0.8403\n",
      "Epoch 23/100\n",
      "4687/4687 [==============================] - 3s 658us/step - loss: 0.3547 - accuracy: 0.8414\n",
      "Epoch 24/100\n",
      "4687/4687 [==============================] - 3s 650us/step - loss: 0.3525 - accuracy: 0.8422\n",
      "Epoch 25/100\n",
      "4687/4687 [==============================] - 3s 636us/step - loss: 0.3509 - accuracy: 0.8433\n",
      "Epoch 26/100\n",
      "4687/4687 [==============================] - 3s 596us/step - loss: 0.3491 - accuracy: 0.8442\n",
      "Epoch 27/100\n",
      "4687/4687 [==============================] - 3s 601us/step - loss: 0.3463 - accuracy: 0.8470\n",
      "Epoch 28/100\n",
      "4687/4687 [==============================] - 3s 623us/step - loss: 0.3453 - accuracy: 0.8474\n",
      "Epoch 29/100\n",
      "4687/4687 [==============================] - 3s 596us/step - loss: 0.3429 - accuracy: 0.8480\n",
      "Epoch 30/100\n",
      "4687/4687 [==============================] - 3s 600us/step - loss: 0.3411 - accuracy: 0.8507\n",
      "Epoch 31/100\n",
      "4687/4687 [==============================] - 3s 600us/step - loss: 0.3389 - accuracy: 0.8511\n",
      "Epoch 32/100\n",
      "4687/4687 [==============================] - 3s 585us/step - loss: 0.3388 - accuracy: 0.8508\n",
      "Epoch 33/100\n",
      "4687/4687 [==============================] - 3s 571us/step - loss: 0.3361 - accuracy: 0.8529\n",
      "Epoch 34/100\n",
      "4687/4687 [==============================] - 3s 584us/step - loss: 0.3342 - accuracy: 0.8540\n",
      "Epoch 35/100\n",
      "4687/4687 [==============================] - 3s 581us/step - loss: 0.3332 - accuracy: 0.8545\n",
      "Epoch 36/100\n",
      "4687/4687 [==============================] - 3s 597us/step - loss: 0.3318 - accuracy: 0.8561\n",
      "Epoch 37/100\n",
      "4687/4687 [==============================] - 3s 592us/step - loss: 0.3300 - accuracy: 0.8571\n",
      "Epoch 38/100\n",
      "4687/4687 [==============================] - 3s 587us/step - loss: 0.3290 - accuracy: 0.8570\n",
      "Epoch 39/100\n",
      "4687/4687 [==============================] - 3s 593us/step - loss: 0.3275 - accuracy: 0.8576\n",
      "Epoch 40/100\n",
      "4687/4687 [==============================] - 3s 601us/step - loss: 0.3260 - accuracy: 0.8586\n",
      "Epoch 41/100\n",
      "4687/4687 [==============================] - 3s 631us/step - loss: 0.3249 - accuracy: 0.8598\n",
      "Epoch 42/100\n",
      "4687/4687 [==============================] - 3s 611us/step - loss: 0.3241 - accuracy: 0.8604\n",
      "Epoch 43/100\n",
      "4687/4687 [==============================] - 3s 581us/step - loss: 0.3223 - accuracy: 0.8605\n",
      "Epoch 44/100\n",
      "4687/4687 [==============================] - 3s 572us/step - loss: 0.3222 - accuracy: 0.8612\n",
      "Epoch 45/100\n",
      "4687/4687 [==============================] - 3s 578us/step - loss: 0.3195 - accuracy: 0.8632\n",
      "Epoch 46/100\n",
      "4687/4687 [==============================] - 3s 579us/step - loss: 0.3197 - accuracy: 0.8620\n",
      "Epoch 47/100\n",
      "4687/4687 [==============================] - 3s 573us/step - loss: 0.3185 - accuracy: 0.8629\n",
      "Epoch 48/100\n",
      "4687/4687 [==============================] - 3s 576us/step - loss: 0.3174 - accuracy: 0.8646\n",
      "Epoch 49/100\n",
      "4687/4687 [==============================] - 3s 579us/step - loss: 0.3169 - accuracy: 0.8641\n",
      "Epoch 50/100\n",
      "4687/4687 [==============================] - 3s 581us/step - loss: 0.3156 - accuracy: 0.8659\n",
      "Epoch 51/100\n",
      "4687/4687 [==============================] - 3s 583us/step - loss: 0.3151 - accuracy: 0.8644\n",
      "Epoch 52/100\n",
      "4687/4687 [==============================] - 3s 591us/step - loss: 0.3141 - accuracy: 0.8658\n",
      "Epoch 53/100\n",
      "4687/4687 [==============================] - 3s 604us/step - loss: 0.3130 - accuracy: 0.8665\n",
      "Epoch 54/100\n",
      "4687/4687 [==============================] - 3s 595us/step - loss: 0.3121 - accuracy: 0.8668\n",
      "Epoch 55/100\n",
      "4687/4687 [==============================] - 3s 591us/step - loss: 0.3113 - accuracy: 0.8677\n",
      "Epoch 56/100\n",
      "4687/4687 [==============================] - 3s 596us/step - loss: 0.3111 - accuracy: 0.8673\n",
      "Epoch 57/100\n",
      "4687/4687 [==============================] - 3s 585us/step - loss: 0.3091 - accuracy: 0.8683\n",
      "Epoch 58/100\n",
      "4687/4687 [==============================] - 3s 582us/step - loss: 0.3091 - accuracy: 0.8690\n",
      "Epoch 59/100\n",
      "4687/4687 [==============================] - 3s 590us/step - loss: 0.3083 - accuracy: 0.8701\n",
      "Epoch 60/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.3072 - accuracy: 0.8692\n",
      "Epoch 61/100\n",
      "4687/4687 [==============================] - 3s 584us/step - loss: 0.3068 - accuracy: 0.8698\n",
      "Epoch 62/100\n",
      "4687/4687 [==============================] - 3s 578us/step - loss: 0.3062 - accuracy: 0.8707\n",
      "Epoch 63/100\n",
      "4687/4687 [==============================] - 3s 591us/step - loss: 0.3055 - accuracy: 0.8703\n",
      "Epoch 64/100\n",
      "4687/4687 [==============================] - 3s 586us/step - loss: 0.3047 - accuracy: 0.8706\n",
      "Epoch 65/100\n",
      "4687/4687 [==============================] - 3s 586us/step - loss: 0.3037 - accuracy: 0.8715\n",
      "Epoch 66/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.3040 - accuracy: 0.8716\n",
      "Epoch 67/100\n",
      "4687/4687 [==============================] - 3s 590us/step - loss: 0.3024 - accuracy: 0.8727\n",
      "Epoch 68/100\n",
      "4687/4687 [==============================] - 3s 585us/step - loss: 0.3022 - accuracy: 0.8721\n",
      "Epoch 69/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.3017 - accuracy: 0.8725\n",
      "Epoch 70/100\n",
      "4687/4687 [==============================] - 3s 596us/step - loss: 0.3011 - accuracy: 0.8734\n",
      "Epoch 71/100\n",
      "4687/4687 [==============================] - 3s 577us/step - loss: 0.3004 - accuracy: 0.8727\n",
      "Epoch 72/100\n",
      "4687/4687 [==============================] - 3s 572us/step - loss: 0.2997 - accuracy: 0.8737\n",
      "Epoch 73/100\n",
      "4687/4687 [==============================] - 3s 577us/step - loss: 0.2993 - accuracy: 0.8743\n",
      "Epoch 74/100\n",
      "4687/4687 [==============================] - 3s 604us/step - loss: 0.2982 - accuracy: 0.8745\n",
      "Epoch 75/100\n",
      "4687/4687 [==============================] - 3s 597us/step - loss: 0.2984 - accuracy: 0.8740\n",
      "Epoch 76/100\n",
      "4687/4687 [==============================] - 3s 602us/step - loss: 0.2973 - accuracy: 0.8750\n",
      "Epoch 77/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.2971 - accuracy: 0.8753\n",
      "Epoch 78/100\n",
      "4687/4687 [==============================] - 3s 592us/step - loss: 0.2968 - accuracy: 0.8754\n",
      "Epoch 79/100\n",
      "4687/4687 [==============================] - 3s 593us/step - loss: 0.2961 - accuracy: 0.8765\n",
      "Epoch 80/100\n",
      "4687/4687 [==============================] - 3s 597us/step - loss: 0.2961 - accuracy: 0.8761\n",
      "Epoch 81/100\n",
      "4687/4687 [==============================] - 3s 595us/step - loss: 0.2947 - accuracy: 0.8764\n",
      "Epoch 82/100\n",
      "4687/4687 [==============================] - 3s 590us/step - loss: 0.2941 - accuracy: 0.8767\n",
      "Epoch 83/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.2939 - accuracy: 0.8769\n",
      "Epoch 84/100\n",
      "4687/4687 [==============================] - 3s 583us/step - loss: 0.2939 - accuracy: 0.8762\n",
      "Epoch 85/100\n",
      "4687/4687 [==============================] - 3s 586us/step - loss: 0.2938 - accuracy: 0.8772\n",
      "Epoch 86/100\n",
      "4687/4687 [==============================] - 3s 585us/step - loss: 0.2924 - accuracy: 0.8782\n",
      "Epoch 87/100\n",
      "4687/4687 [==============================] - 3s 587us/step - loss: 0.2919 - accuracy: 0.8786\n",
      "Epoch 88/100\n",
      "4687/4687 [==============================] - 3s 598us/step - loss: 0.2920 - accuracy: 0.8785\n",
      "Epoch 89/100\n",
      "4687/4687 [==============================] - 3s 590us/step - loss: 0.2913 - accuracy: 0.8789\n",
      "Epoch 90/100\n",
      "4687/4687 [==============================] - 3s 586us/step - loss: 0.2912 - accuracy: 0.8787\n",
      "Epoch 91/100\n",
      "4687/4687 [==============================] - 3s 581us/step - loss: 0.2907 - accuracy: 0.8790\n",
      "Epoch 92/100\n",
      "4687/4687 [==============================] - 3s 584us/step - loss: 0.2902 - accuracy: 0.8791\n",
      "Epoch 93/100\n",
      "4687/4687 [==============================] - 3s 578us/step - loss: 0.2900 - accuracy: 0.8793\n",
      "Epoch 94/100\n",
      "4687/4687 [==============================] - 3s 584us/step - loss: 0.2896 - accuracy: 0.8796\n",
      "Epoch 95/100\n",
      "4687/4687 [==============================] - 3s 580us/step - loss: 0.2887 - accuracy: 0.8799\n",
      "Epoch 96/100\n",
      "4687/4687 [==============================] - 3s 593us/step - loss: 0.2882 - accuracy: 0.8804\n",
      "Epoch 97/100\n",
      "4687/4687 [==============================] - 3s 597us/step - loss: 0.2884 - accuracy: 0.8792\n",
      "Epoch 98/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.2877 - accuracy: 0.8805\n",
      "Epoch 99/100\n",
      "4687/4687 [==============================] - 3s 590us/step - loss: 0.2875 - accuracy: 0.8803\n",
      "Epoch 100/100\n",
      "4687/4687 [==============================] - 3s 599us/step - loss: 0.2873 - accuracy: 0.8814\n",
      "888/888 - 0s - loss: 0.5461 - accuracy: 0.7817 - 435ms/epoch - 489us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 4: Same as above, but scaling only applied to the columns that are NOT either binary or already between 0 and 1\n",
    "# Create a dictionary for iteration 4\n",
    "iteration_4 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_4[\"Iteration\"] = 4\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler4 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_4 = scaler4.fit(X_train[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_4 = X_train\n",
    "X_train_scaled_4[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_4.transform(X_train[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "X_test_scaled_4 = X_test\n",
    "X_test_scaled_4[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_4.transform(X_test[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_4 = RandomOverSampler(random_state=1)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_4, y_ros_4 = ros_4.fit_resample(X_train_scaled_4, y_train)\n",
    "\n",
    "# Define the model\n",
    "nn_4 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_4.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_4.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_4 = nn_4.fit(X_ros_4, y_ros_4, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_4, model_accuracy_4 = nn_4.evaluate(X_test_scaled_4, y_test, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_4 dictionary\n",
    "iteration_4[\"Loss\"] = model_loss_4\n",
    "iteration_4[\"Accuracy\"] = model_accuracy_4\n",
    "\n",
    "# Add the iteration_4 dictionary to the iterations list\n",
    "iterations.append(iteration_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model structure (this is the same for iterations 1-4 so is only done once)\n",
    "ann_viz(nn_4, title=\"Binary Popularity Testing Structure\", filename=\"summary_stats/binary_popularity_testing_nn_structure.gv\", view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 01m 24s]\n",
      "val_accuracy: 0.7296745181083679\n",
      "\n",
      "Best val_accuracy So Far: 0.8023108243942261\n",
      "Total elapsed time: 00h 30m 33s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "### Iteration 5: Same as above, but with hyperparameter tuning to find best parameters\n",
    "# Create a dictionary for iteration 5\n",
    "iteration_5 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_5[\"Iteration\"] = 5\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler5 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_5 = scaler4.fit(X_train[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_5 = X_train\n",
    "X_train_scaled_5[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_5.transform(X_train[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "X_test_scaled_5 = X_test\n",
    "X_test_scaled_5[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_5.transform(X_test[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_5 = RandomOverSampler(random_state=1)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_5, y_ros_5 = ros_5.fit_resample(X_train_scaled_5, y_train)\n",
    "\n",
    "# Create a method that creates a Sequential model with hyperparameter tuning\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Determine the activation functions for each layer\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "\n",
    "    # Determine neurons in the first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units', \n",
    "        min_value=80, \n",
    "        max_value=120, \n",
    "        step=20), \n",
    "        activation=activation, input_dim=42))\n",
    "    \n",
    "    # Determine the number of hidden layers and neurons in them\n",
    "    for i in range(hp.Int('num_layers', 2, 4)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_'+str(i),\n",
    "            min_value=50, \n",
    "            max_value=90,\n",
    "            step=20),\n",
    "            activation=activation))\n",
    "    \n",
    "    # Set up the output layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return nn_model\n",
    "\n",
    "# Set up the kerastuner instance\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\", \n",
    "    max_epochs=20, \n",
    "    hyperband_iterations=2,\n",
    "    overwrite=True)\n",
    "\n",
    "# Run the kerastuner\n",
    "tuner.search(X_ros_5, y_ros_5, epochs=20, validation_data=(X_test_scaled_5, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'first_units': 120, 'num_layers': 2, 'units_0': 70, 'units_1': 90, 'units_2': 50, 'units_3': 90, 'tuner/epochs': 20, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 120)               5160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 70)                8470      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 90)                6390      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 91        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,111\n",
      "Trainable params: 20,111\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4687/4687 [==============================] - 4s 690us/step - loss: 0.5275 - accuracy: 0.7263\n",
      "Epoch 2/100\n",
      "4687/4687 [==============================] - 3s 689us/step - loss: 0.4554 - accuracy: 0.7784\n",
      "Epoch 3/100\n",
      "4687/4687 [==============================] - 3s 683us/step - loss: 0.4310 - accuracy: 0.7917\n",
      "Epoch 4/100\n",
      "4687/4687 [==============================] - 3s 679us/step - loss: 0.4113 - accuracy: 0.8041\n",
      "Epoch 5/100\n",
      "4687/4687 [==============================] - 3s 685us/step - loss: 0.3927 - accuracy: 0.8158\n",
      "Epoch 6/100\n",
      "4687/4687 [==============================] - 3s 677us/step - loss: 0.3775 - accuracy: 0.8249\n",
      "Epoch 7/100\n",
      "4687/4687 [==============================] - 3s 677us/step - loss: 0.3634 - accuracy: 0.8331\n",
      "Epoch 8/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.3509 - accuracy: 0.8404\n",
      "Epoch 9/100\n",
      "4687/4687 [==============================] - 3s 673us/step - loss: 0.3393 - accuracy: 0.8472\n",
      "Epoch 10/100\n",
      "4687/4687 [==============================] - 3s 694us/step - loss: 0.3292 - accuracy: 0.8538\n",
      "Epoch 11/100\n",
      "4687/4687 [==============================] - 3s 691us/step - loss: 0.3195 - accuracy: 0.8586\n",
      "Epoch 12/100\n",
      "4687/4687 [==============================] - 3s 686us/step - loss: 0.3103 - accuracy: 0.8640\n",
      "Epoch 13/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.3027 - accuracy: 0.8688\n",
      "Epoch 14/100\n",
      "4687/4687 [==============================] - 3s 681us/step - loss: 0.2952 - accuracy: 0.8727\n",
      "Epoch 15/100\n",
      "4687/4687 [==============================] - 3s 676us/step - loss: 0.2872 - accuracy: 0.8768\n",
      "Epoch 16/100\n",
      "4687/4687 [==============================] - 3s 690us/step - loss: 0.2808 - accuracy: 0.8799\n",
      "Epoch 17/100\n",
      "4687/4687 [==============================] - 3s 698us/step - loss: 0.2739 - accuracy: 0.8838\n",
      "Epoch 18/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.2684 - accuracy: 0.8877\n",
      "Epoch 19/100\n",
      "4687/4687 [==============================] - 3s 679us/step - loss: 0.2621 - accuracy: 0.8911\n",
      "Epoch 20/100\n",
      "4687/4687 [==============================] - 3s 687us/step - loss: 0.2574 - accuracy: 0.8926\n",
      "Epoch 21/100\n",
      "4687/4687 [==============================] - 3s 692us/step - loss: 0.2525 - accuracy: 0.8956\n",
      "Epoch 22/100\n",
      "4687/4687 [==============================] - 3s 684us/step - loss: 0.2476 - accuracy: 0.8979\n",
      "Epoch 23/100\n",
      "4687/4687 [==============================] - 3s 684us/step - loss: 0.2429 - accuracy: 0.9007\n",
      "Epoch 24/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.2383 - accuracy: 0.9027\n",
      "Epoch 25/100\n",
      "4687/4687 [==============================] - 3s 684us/step - loss: 0.2347 - accuracy: 0.9047\n",
      "Epoch 26/100\n",
      "4687/4687 [==============================] - 3s 681us/step - loss: 0.2304 - accuracy: 0.9059\n",
      "Epoch 27/100\n",
      "4687/4687 [==============================] - 3s 679us/step - loss: 0.2265 - accuracy: 0.9087\n",
      "Epoch 28/100\n",
      "4687/4687 [==============================] - 3s 674us/step - loss: 0.2235 - accuracy: 0.9105\n",
      "Epoch 29/100\n",
      "4687/4687 [==============================] - 3s 670us/step - loss: 0.2198 - accuracy: 0.9123\n",
      "Epoch 30/100\n",
      "4687/4687 [==============================] - 3s 687us/step - loss: 0.2174 - accuracy: 0.9130\n",
      "Epoch 31/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.2139 - accuracy: 0.9147\n",
      "Epoch 32/100\n",
      "4687/4687 [==============================] - 3s 677us/step - loss: 0.2106 - accuracy: 0.9160\n",
      "Epoch 33/100\n",
      "4687/4687 [==============================] - 3s 676us/step - loss: 0.2079 - accuracy: 0.9174\n",
      "Epoch 34/100\n",
      "4687/4687 [==============================] - 3s 669us/step - loss: 0.2046 - accuracy: 0.9194\n",
      "Epoch 35/100\n",
      "4687/4687 [==============================] - 3s 675us/step - loss: 0.2020 - accuracy: 0.9203\n",
      "Epoch 36/100\n",
      "4687/4687 [==============================] - 3s 670us/step - loss: 0.1976 - accuracy: 0.9226\n",
      "Epoch 37/100\n",
      "4687/4687 [==============================] - 3s 672us/step - loss: 0.1960 - accuracy: 0.9228\n",
      "Epoch 38/100\n",
      "4687/4687 [==============================] - 3s 668us/step - loss: 0.1935 - accuracy: 0.9240\n",
      "Epoch 39/100\n",
      "4687/4687 [==============================] - 3s 675us/step - loss: 0.1914 - accuracy: 0.9255\n",
      "Epoch 40/100\n",
      "4687/4687 [==============================] - 3s 674us/step - loss: 0.1877 - accuracy: 0.9275\n",
      "Epoch 41/100\n",
      "4687/4687 [==============================] - 3s 678us/step - loss: 0.1855 - accuracy: 0.9281\n",
      "Epoch 42/100\n",
      "4687/4687 [==============================] - 3s 674us/step - loss: 0.1837 - accuracy: 0.9283\n",
      "Epoch 43/100\n",
      "4687/4687 [==============================] - 3s 668us/step - loss: 0.1813 - accuracy: 0.9298\n",
      "Epoch 44/100\n",
      "4687/4687 [==============================] - 3s 669us/step - loss: 0.1802 - accuracy: 0.9303\n",
      "Epoch 45/100\n",
      "4687/4687 [==============================] - 3s 665us/step - loss: 0.1787 - accuracy: 0.9313\n",
      "Epoch 46/100\n",
      "4687/4687 [==============================] - 3s 689us/step - loss: 0.1758 - accuracy: 0.9326\n",
      "Epoch 47/100\n",
      "4687/4687 [==============================] - 3s 720us/step - loss: 0.1739 - accuracy: 0.9325\n",
      "Epoch 48/100\n",
      "4687/4687 [==============================] - 3s 702us/step - loss: 0.1723 - accuracy: 0.9340\n",
      "Epoch 49/100\n",
      "4687/4687 [==============================] - 3s 701us/step - loss: 0.1705 - accuracy: 0.9348\n",
      "Epoch 50/100\n",
      "4687/4687 [==============================] - 3s 688us/step - loss: 0.1688 - accuracy: 0.9351\n",
      "Epoch 51/100\n",
      "4687/4687 [==============================] - 3s 678us/step - loss: 0.1671 - accuracy: 0.9361\n",
      "Epoch 52/100\n",
      "4687/4687 [==============================] - 3s 672us/step - loss: 0.1658 - accuracy: 0.9371\n",
      "Epoch 53/100\n",
      "4687/4687 [==============================] - 3s 674us/step - loss: 0.1653 - accuracy: 0.9373\n",
      "Epoch 54/100\n",
      "4687/4687 [==============================] - 3s 679us/step - loss: 0.1623 - accuracy: 0.9380\n",
      "Epoch 55/100\n",
      "4687/4687 [==============================] - 3s 677us/step - loss: 0.1621 - accuracy: 0.9382\n",
      "Epoch 56/100\n",
      "4687/4687 [==============================] - 3s 667us/step - loss: 0.1599 - accuracy: 0.9393\n",
      "Epoch 57/100\n",
      "4687/4687 [==============================] - 3s 667us/step - loss: 0.1597 - accuracy: 0.9400\n",
      "Epoch 58/100\n",
      "4687/4687 [==============================] - 3s 682us/step - loss: 0.1572 - accuracy: 0.9404\n",
      "Epoch 59/100\n",
      "4687/4687 [==============================] - 3s 675us/step - loss: 0.1553 - accuracy: 0.9406\n",
      "Epoch 60/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.1554 - accuracy: 0.9415\n",
      "Epoch 61/100\n",
      "4687/4687 [==============================] - 3s 678us/step - loss: 0.1533 - accuracy: 0.9420\n",
      "Epoch 62/100\n",
      "4687/4687 [==============================] - 3s 674us/step - loss: 0.1526 - accuracy: 0.9430\n",
      "Epoch 63/100\n",
      "4687/4687 [==============================] - 3s 673us/step - loss: 0.1521 - accuracy: 0.9426\n",
      "Epoch 64/100\n",
      "4687/4687 [==============================] - 3s 675us/step - loss: 0.1487 - accuracy: 0.9440\n",
      "Epoch 65/100\n",
      "4687/4687 [==============================] - 3s 674us/step - loss: 0.1492 - accuracy: 0.9436\n",
      "Epoch 66/100\n",
      "4687/4687 [==============================] - 3s 677us/step - loss: 0.1477 - accuracy: 0.9443\n",
      "Epoch 67/100\n",
      "4687/4687 [==============================] - 3s 675us/step - loss: 0.1471 - accuracy: 0.9454\n",
      "Epoch 68/100\n",
      "4687/4687 [==============================] - 3s 682us/step - loss: 0.1459 - accuracy: 0.9453\n",
      "Epoch 69/100\n",
      "4687/4687 [==============================] - 3s 679us/step - loss: 0.1439 - accuracy: 0.9463\n",
      "Epoch 70/100\n",
      "4687/4687 [==============================] - 3s 676us/step - loss: 0.1432 - accuracy: 0.9468\n",
      "Epoch 71/100\n",
      "4687/4687 [==============================] - 3s 676us/step - loss: 0.1428 - accuracy: 0.9469\n",
      "Epoch 72/100\n",
      "4687/4687 [==============================] - 3s 674us/step - loss: 0.1422 - accuracy: 0.9470\n",
      "Epoch 73/100\n",
      "4687/4687 [==============================] - 3s 674us/step - loss: 0.1404 - accuracy: 0.9480\n",
      "Epoch 74/100\n",
      "4687/4687 [==============================] - 3s 675us/step - loss: 0.1403 - accuracy: 0.9480\n",
      "Epoch 75/100\n",
      "4687/4687 [==============================] - 3s 677us/step - loss: 0.1393 - accuracy: 0.9480\n",
      "Epoch 76/100\n",
      "4687/4687 [==============================] - 3s 676us/step - loss: 0.1372 - accuracy: 0.9493\n",
      "Epoch 77/100\n",
      "4687/4687 [==============================] - 3s 678us/step - loss: 0.1379 - accuracy: 0.9491\n",
      "Epoch 78/100\n",
      "4687/4687 [==============================] - 3s 679us/step - loss: 0.1368 - accuracy: 0.9492\n",
      "Epoch 79/100\n",
      "4687/4687 [==============================] - 3s 674us/step - loss: 0.1351 - accuracy: 0.9504\n",
      "Epoch 80/100\n",
      "4687/4687 [==============================] - 3s 673us/step - loss: 0.1353 - accuracy: 0.9488\n",
      "Epoch 81/100\n",
      "4687/4687 [==============================] - 3s 677us/step - loss: 0.1332 - accuracy: 0.9509\n",
      "Epoch 82/100\n",
      "4687/4687 [==============================] - 3s 673us/step - loss: 0.1327 - accuracy: 0.9506\n",
      "Epoch 83/100\n",
      "4687/4687 [==============================] - 3s 687us/step - loss: 0.1329 - accuracy: 0.9508\n",
      "Epoch 84/100\n",
      "4687/4687 [==============================] - 3s 712us/step - loss: 0.1322 - accuracy: 0.9513\n",
      "Epoch 85/100\n",
      "4687/4687 [==============================] - 3s 683us/step - loss: 0.1295 - accuracy: 0.9522\n",
      "Epoch 86/100\n",
      "4687/4687 [==============================] - 3s 681us/step - loss: 0.1303 - accuracy: 0.9515\n",
      "Epoch 87/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.1290 - accuracy: 0.9532\n",
      "Epoch 88/100\n",
      "4687/4687 [==============================] - 3s 678us/step - loss: 0.1275 - accuracy: 0.9530\n",
      "Epoch 89/100\n",
      "4687/4687 [==============================] - 3s 676us/step - loss: 0.1287 - accuracy: 0.9524\n",
      "Epoch 90/100\n",
      "4687/4687 [==============================] - 3s 678us/step - loss: 0.1263 - accuracy: 0.9531\n",
      "Epoch 91/100\n",
      "4687/4687 [==============================] - 3s 677us/step - loss: 0.1261 - accuracy: 0.9540\n",
      "Epoch 92/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.1241 - accuracy: 0.9544\n",
      "Epoch 93/100\n",
      "4687/4687 [==============================] - 3s 677us/step - loss: 0.1235 - accuracy: 0.9548\n",
      "Epoch 94/100\n",
      "4687/4687 [==============================] - 3s 685us/step - loss: 0.1230 - accuracy: 0.9548\n",
      "Epoch 95/100\n",
      "4687/4687 [==============================] - 3s 693us/step - loss: 0.1226 - accuracy: 0.9547\n",
      "Epoch 96/100\n",
      "4687/4687 [==============================] - 3s 684us/step - loss: 0.1226 - accuracy: 0.9547\n",
      "Epoch 97/100\n",
      "4687/4687 [==============================] - 3s 688us/step - loss: 0.1219 - accuracy: 0.9548\n",
      "Epoch 98/100\n",
      "4687/4687 [==============================] - 3s 696us/step - loss: 0.1208 - accuracy: 0.9556\n",
      "Epoch 99/100\n",
      "4687/4687 [==============================] - 3s 685us/step - loss: 0.1213 - accuracy: 0.9554\n",
      "Epoch 100/100\n",
      "4687/4687 [==============================] - 3s 680us/step - loss: 0.1208 - accuracy: 0.9554\n",
      "888/888 - 0s - loss: 1.1309 - accuracy: 0.8252 - 443ms/epoch - 499us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 5 continued\n",
    "# Get the best model and show its parameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)\n",
    "for params in best_hyper:\n",
    "    print(params.values)\n",
    "\n",
    "# Get a summary of the best model\n",
    "best_model = tuner.get_best_models()\n",
    "best_model[0].summary()\n",
    "\n",
    "# Build the best model and train it on the data\n",
    "model = tuner.hypermodel.build(best_hyper[0])\n",
    "model.fit(X_ros_5, y_ros_5, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_5, model_accuracy_5 = model.evaluate(X_test_scaled_5, y_test, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_5 dictionary\n",
    "iteration_5[\"Loss\"] = model_loss_5\n",
    "iteration_5[\"Accuracy\"] = model_accuracy_5\n",
    "\n",
    "# Add the iteration_5 dictionary to the iterations list\n",
    "iterations.append(iteration_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the final model structure\n",
    "ann_viz(model, title=\"Binary Popularity Final Structure\", filename=\"summary_stats/binary_popularity_final_nn_structure.gv\", view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss: 0.34424471855163574, Accuracy: 0.8684303164482117\n",
      "Iteration 2: Loss: 0.32352831959724426, Accuracy: 0.879315197467804\n",
      "Iteration 3: Loss: 0.5452646017074585, Accuracy: 0.7951951622962952\n",
      "Iteration 4: Loss: 0.5461353063583374, Accuracy: 0.7817387580871582\n",
      "Iteration 5: Loss: 1.1308646202087402, Accuracy: 0.8252430558204651\n"
     ]
    }
   ],
   "source": [
    "# Print Final Statistics:\n",
    "for i in range(len(iterations)):\n",
    "    print(f\"Iteration {i+1}: Loss: {iterations[i]['Loss']}, Accuracy: {iterations[i]['Accuracy']}\")\n",
    "\n",
    "# Add final statistics to a dataframe and output to a csv\n",
    "stats_df = pd.DataFrame(iterations)\n",
    "stats_df.to_csv(\"summary_stats/binary_popularity_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
