{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark_dist_explore import hist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize findspark\n",
    "findspark.init()\n",
    "\n",
    "# Initialize the spark session\n",
    "spark = SparkSession.builder.appName(\"SK_model\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|   primary_artist|          track_name|popularity|explicit|danceability|energy|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|      duration_min|time_signature_0|time_signature_1|time_signature_3|time_signature_4|time_signature_5|key_0|key_1|key_2|key_3|key_4|key_5|key_6|key_7|key_8|key_9|key_10|key_11|num_artists_binned_1|num_artists_binned_2|num_artists_binned_3|num_artists_binned_4|num_artists_binned_5|num_artists_binned_6|track_genre_0|track_genre_1|track_genre_2|track_genre_3|track_genre_4|track_genre_5|track_genre_6|\n",
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|      Gen Hoshino|              Comedy|        73|       0|       0.676| 0.461|  -6.746|   0|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|3.8444333333333334|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Ben Woodward|    Ghost - Acoustic|        55|       0|        0.42| 0.166| -17.235|   1|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|            2.4935|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|Ingrid Michaelson|      To Begin Again|        57|       0|       0.438| 0.359|  -9.734|   1|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|3.5137666666666667|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Kina Grannis|Can't Help Fallin...|        71|       0|       0.266|0.0596| -18.515|   1|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|           3.36555|               0|               0|               1|               0|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "| Chord Overstreet|             Hold On|        82|       0|       0.618| 0.443|  -9.681|   1|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949| 3.314216666666667|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Tyrone Wells|Days I Will Remember|        58|       0|       0.688| 0.481|  -8.807|   1|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017| 3.570666666666667|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|A Great Big World|       Say Something|        74|       0|       0.407| 0.147|  -8.822|   1|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|3.8233333333333333|               0|               0|               1|               0|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|           I'm Yours|        80|       0|       0.703| 0.444|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|            4.0491|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     1|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|               Lucky|        74|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|   Ross Copperman|              Hunger|        56|       0|       0.442| 0.632|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|3.4265666666666665|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Zack Tabudlo|Give Me Your Forever|        74|       0|       0.627| 0.363|  -8.127|   1|     0.0291|       0.279|             0.0|  0.0928|  0.301| 99.905|              4.08|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|     I Won't Give Up|        69|       0|       0.483| 0.303| -10.058|   1|     0.0429|       0.694|             0.0|   0.115|  0.139|133.406|           4.00275|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|         Dan Berk|                Solo|        52|       0|       0.489| 0.314|  -9.245|   0|     0.0331|       0.749|             0.0|   0.113|  0.607|124.234|3.3118666666666665|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|    Anna Hamilton|            Bad Liar|        62|       0|       0.691| 0.234|  -6.441|   1|     0.0285|       0.777|             0.0|    0.12|  0.209| 87.103|            4.1408|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "| Chord Overstreet|     Hold On - Remix|        56|       0|       0.755|  0.78|  -6.084|   1|     0.0327|       0.124|         2.83E-5|   0.121|  0.387|120.004|           3.13555|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|      Landon Pigg|Falling in Love a...|        58|       0|       0.489| 0.561|  -7.933|   1|     0.0274|         0.2|         4.56E-5|   0.179|  0.238| 83.457|            4.0831|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Andrew Foy|ily (i love you b...|        56|       0|       0.706| 0.112| -18.098|   1|     0.0391|       0.827|         4.03E-6|   0.125|  0.414|110.154|            2.1625|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Andrew Foy|         At My Worst|        54|       0|       0.795|0.0841|  -18.09|   0|     0.0461|       0.742|         1.17E-5|  0.0853|  0.609| 91.803|            2.8288|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     1|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|               Lucky|        68|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Boyce Avenue|          Photograph|        67|       0|       0.717|  0.32|  -8.393|   1|     0.0283|        0.83|             0.0|   0.107|  0.322|107.946| 4.336433333333333|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the pre-processed data\n",
    "s_df = spark.read.csv(\"../Resources/filtered_encoded_dataset.csv\", sep=\",\", header=True, inferSchema=True)\n",
    "s_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|popularity|explicit|danceability|energy|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|      duration_min|time_signature_0|time_signature_1|time_signature_3|time_signature_4|time_signature_5|key_0|key_1|key_2|key_3|key_4|key_5|key_6|key_7|key_8|key_9|key_10|key_11|num_artists_binned_1|num_artists_binned_2|num_artists_binned_3|num_artists_binned_4|num_artists_binned_5|num_artists_binned_6|track_genre_0|track_genre_1|track_genre_2|track_genre_3|track_genre_4|track_genre_5|track_genre_6|\n",
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|        73|       0|       0.676| 0.461|  -6.746|   0|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|3.8444333333333334|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        55|       0|        0.42| 0.166| -17.235|   1|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|            2.4935|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        57|       0|       0.438| 0.359|  -9.734|   1|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|3.5137666666666667|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        71|       0|       0.266|0.0596| -18.515|   1|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|           3.36555|               0|               0|               1|               0|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        82|       0|       0.618| 0.443|  -9.681|   1|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949| 3.314216666666667|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        58|       0|       0.688| 0.481|  -8.807|   1|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017| 3.570666666666667|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.407| 0.147|  -8.822|   1|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|3.8233333333333333|               0|               0|               1|               0|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        80|       0|       0.703| 0.444|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|            4.0491|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     1|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.442| 0.632|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|3.4265666666666665|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.627| 0.363|  -8.127|   1|     0.0291|       0.279|             0.0|  0.0928|  0.301| 99.905|              4.08|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        69|       0|       0.483| 0.303| -10.058|   1|     0.0429|       0.694|             0.0|   0.115|  0.139|133.406|           4.00275|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        52|       0|       0.489| 0.314|  -9.245|   0|     0.0331|       0.749|             0.0|   0.113|  0.607|124.234|3.3118666666666665|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        62|       0|       0.691| 0.234|  -6.441|   1|     0.0285|       0.777|             0.0|    0.12|  0.209| 87.103|            4.1408|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.755|  0.78|  -6.084|   1|     0.0327|       0.124|         2.83E-5|   0.121|  0.387|120.004|           3.13555|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        58|       0|       0.489| 0.561|  -7.933|   1|     0.0274|         0.2|         4.56E-5|   0.179|  0.238| 83.457|            4.0831|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.706| 0.112| -18.098|   1|     0.0391|       0.827|         4.03E-6|   0.125|  0.414|110.154|            2.1625|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        54|       0|       0.795|0.0841|  -18.09|   0|     0.0461|       0.742|         1.17E-5|  0.0853|  0.609| 91.803|            2.8288|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     1|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        68|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        67|       0|       0.717|  0.32|  -8.393|   1|     0.0283|        0.83|             0.0|   0.107|  0.322|107.946| 4.336433333333333|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove primary_artist and track_name from the dataset\n",
    "s_df = s_df.drop(\"primary_artist\", \"track_name\")\n",
    "s_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([22457.,  9279., 18354., 14910., 19205., 14532.,  9344.,  4269.,\n",
       "         1101.,    98.]),\n",
       " array([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check popularity values\n",
    "fig, ax = plt.subplots()\n",
    "hist(ax, s_df.select(\"popularity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>...</th>\n",
       "      <th>num_artists_binned_4</th>\n",
       "      <th>num_artists_binned_5</th>\n",
       "      <th>num_artists_binned_6</th>\n",
       "      <th>track_genre_0</th>\n",
       "      <th>track_genre_1</th>\n",
       "      <th>track_genre_2</th>\n",
       "      <th>track_genre_3</th>\n",
       "      <th>track_genre_4</th>\n",
       "      <th>track_genre_5</th>\n",
       "      <th>track_genre_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   popularity  explicit  danceability  energy  loudness  mode  speechiness   \n",
       "0          73         0         0.676  0.4610    -6.746     0       0.1430  \\\n",
       "1          55         0         0.420  0.1660   -17.235     1       0.0763   \n",
       "2          57         0         0.438  0.3590    -9.734     1       0.0557   \n",
       "3          71         0         0.266  0.0596   -18.515     1       0.0363   \n",
       "4          82         0         0.618  0.4430    -9.681     1       0.0526   \n",
       "\n",
       "   acousticness  instrumentalness  liveness  ...  num_artists_binned_4   \n",
       "0        0.0322          0.000001    0.3580  ...                     0  \\\n",
       "1        0.9240          0.000006    0.1010  ...                     0   \n",
       "2        0.2100          0.000000    0.1170  ...                     0   \n",
       "3        0.9050          0.000071    0.1320  ...                     0   \n",
       "4        0.4690          0.000000    0.0829  ...                     0   \n",
       "\n",
       "   num_artists_binned_5  num_artists_binned_6  track_genre_0  track_genre_1   \n",
       "0                     0                     0              0              0  \\\n",
       "1                     0                     0              0              0   \n",
       "2                     0                     0              0              0   \n",
       "3                     0                     0              0              0   \n",
       "4                     0                     0              0              0   \n",
       "\n",
       "   track_genre_2  track_genre_3  track_genre_4  track_genre_5  track_genre_6  \n",
       "0              0              0              0              0              1  \n",
       "1              0              0              0              0              1  \n",
       "2              0              0              0              0              1  \n",
       "3              0              0              0              0              1  \n",
       "4              0              0              0              0              1  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data to a pandas Dataframe\n",
    "df = s_df.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popularity\n",
       "0      15843\n",
       "1       2116\n",
       "2       1025\n",
       "3        570\n",
       "4        377\n",
       "       ...  \n",
       "96         7\n",
       "97         8\n",
       "98         7\n",
       "99         1\n",
       "100        2\n",
       "Name: count, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show value counts of popularity to help determine proper bins\n",
    "df['popularity'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popularity_binned\n",
       "0    99988\n",
       "1    13561\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Bin the popularity into: low=0 (pop<=60) or high=1 (60<pop)\n",
    "# Set up a list of bins\n",
    "pop_bins = [0, 1]\n",
    "# Set up list of conditions\n",
    "pop_conditions = [\n",
    "    (df[\"popularity\"] <= 60),\n",
    "    (df[\"popularity\"] > 60)\n",
    "]\n",
    "# Set up the column with bins\n",
    "df[\"popularity_binned\"] = np.select(pop_conditions, pop_bins)\n",
    "\n",
    "# Confirm binning\n",
    "df['popularity_binned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>...</th>\n",
       "      <th>num_artists_binned_5</th>\n",
       "      <th>num_artists_binned_6</th>\n",
       "      <th>track_genre_0</th>\n",
       "      <th>track_genre_1</th>\n",
       "      <th>track_genre_2</th>\n",
       "      <th>track_genre_3</th>\n",
       "      <th>track_genre_4</th>\n",
       "      <th>track_genre_5</th>\n",
       "      <th>track_genre_6</th>\n",
       "      <th>popularity_binned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   explicit  danceability  energy  loudness  mode  speechiness  acousticness   \n",
       "0         0         0.676  0.4610    -6.746     0       0.1430        0.0322  \\\n",
       "1         0         0.420  0.1660   -17.235     1       0.0763        0.9240   \n",
       "2         0         0.438  0.3590    -9.734     1       0.0557        0.2100   \n",
       "3         0         0.266  0.0596   -18.515     1       0.0363        0.9050   \n",
       "4         0         0.618  0.4430    -9.681     1       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence  ...  num_artists_binned_5   \n",
       "0          0.000001    0.3580    0.715  ...                     0  \\\n",
       "1          0.000006    0.1010    0.267  ...                     0   \n",
       "2          0.000000    0.1170    0.120  ...                     0   \n",
       "3          0.000071    0.1320    0.143  ...                     0   \n",
       "4          0.000000    0.0829    0.167  ...                     0   \n",
       "\n",
       "   num_artists_binned_6  track_genre_0  track_genre_1  track_genre_2   \n",
       "0                     0              0              0              0  \\\n",
       "1                     0              0              0              0   \n",
       "2                     0              0              0              0   \n",
       "3                     0              0              0              0   \n",
       "4                     0              0              0              0   \n",
       "\n",
       "   track_genre_3  track_genre_4  track_genre_5  track_genre_6   \n",
       "0              0              0              0              1  \\\n",
       "1              0              0              0              1   \n",
       "2              0              0              0              1   \n",
       "3              0              0              0              1   \n",
       "4              0              0              0              1   \n",
       "\n",
       "   popularity_binned  \n",
       "0                  1  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  1  \n",
       "4                  1  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the original popularity column\n",
    "df.drop(columns=\"popularity\", inplace=True)\n",
    "# Check removal\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the prediction (song popularity) from the rest of the features\n",
    "y = df.popularity_binned.values\n",
    "X = df.drop(columns=\"popularity_binned\")\n",
    "\n",
    "# Split the training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for dictionaries to hold the summary and accuracy for each iteration\n",
    "iterations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,541\n",
      "Trainable params: 7,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2662/2662 [==============================] - 2s 599us/step - loss: 0.3373 - accuracy: 0.8795\n",
      "Epoch 2/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.3092 - accuracy: 0.8835\n",
      "Epoch 3/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.2918 - accuracy: 0.8866\n",
      "Epoch 4/100\n",
      "2662/2662 [==============================] - 2s 599us/step - loss: 0.2817 - accuracy: 0.8877\n",
      "Epoch 5/100\n",
      "2662/2662 [==============================] - 2s 601us/step - loss: 0.2756 - accuracy: 0.8894\n",
      "Epoch 6/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.2707 - accuracy: 0.8900\n",
      "Epoch 7/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.2665 - accuracy: 0.8916\n",
      "Epoch 8/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.2633 - accuracy: 0.8922\n",
      "Epoch 9/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.2609 - accuracy: 0.8928\n",
      "Epoch 10/100\n",
      "2662/2662 [==============================] - 2s 630us/step - loss: 0.2583 - accuracy: 0.8933\n",
      "Epoch 11/100\n",
      "2662/2662 [==============================] - 2s 661us/step - loss: 0.2558 - accuracy: 0.8944\n",
      "Epoch 12/100\n",
      "2662/2662 [==============================] - 2s 619us/step - loss: 0.2542 - accuracy: 0.8957\n",
      "Epoch 13/100\n",
      "2662/2662 [==============================] - 2s 610us/step - loss: 0.2523 - accuracy: 0.8958\n",
      "Epoch 14/100\n",
      "2662/2662 [==============================] - 2s 626us/step - loss: 0.2499 - accuracy: 0.8959\n",
      "Epoch 15/100\n",
      "2662/2662 [==============================] - 2s 644us/step - loss: 0.2480 - accuracy: 0.8970\n",
      "Epoch 16/100\n",
      "2662/2662 [==============================] - 2s 592us/step - loss: 0.2467 - accuracy: 0.8979\n",
      "Epoch 17/100\n",
      "2662/2662 [==============================] - 2s 784us/step - loss: 0.2444 - accuracy: 0.8986\n",
      "Epoch 18/100\n",
      "2662/2662 [==============================] - 2s 678us/step - loss: 0.2431 - accuracy: 0.8999\n",
      "Epoch 19/100\n",
      "2662/2662 [==============================] - 2s 690us/step - loss: 0.2420 - accuracy: 0.9000\n",
      "Epoch 20/100\n",
      "2662/2662 [==============================] - 2s 836us/step - loss: 0.2402 - accuracy: 0.8998\n",
      "Epoch 21/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2394 - accuracy: 0.9004\n",
      "Epoch 22/100\n",
      "2662/2662 [==============================] - 2s 669us/step - loss: 0.2379 - accuracy: 0.9009\n",
      "Epoch 23/100\n",
      "2662/2662 [==============================] - 2s 630us/step - loss: 0.2366 - accuracy: 0.9014\n",
      "Epoch 24/100\n",
      "2662/2662 [==============================] - 2s 694us/step - loss: 0.2351 - accuracy: 0.9017\n",
      "Epoch 25/100\n",
      "2662/2662 [==============================] - 2s 705us/step - loss: 0.2340 - accuracy: 0.9029\n",
      "Epoch 26/100\n",
      "2662/2662 [==============================] - 2s 657us/step - loss: 0.2334 - accuracy: 0.9029\n",
      "Epoch 27/100\n",
      "2662/2662 [==============================] - 2s 639us/step - loss: 0.2317 - accuracy: 0.9035\n",
      "Epoch 28/100\n",
      "2662/2662 [==============================] - 2s 688us/step - loss: 0.2311 - accuracy: 0.9036\n",
      "Epoch 29/100\n",
      "2662/2662 [==============================] - 2s 607us/step - loss: 0.2295 - accuracy: 0.9044\n",
      "Epoch 30/100\n",
      "2662/2662 [==============================] - 2s 632us/step - loss: 0.2293 - accuracy: 0.9049\n",
      "Epoch 31/100\n",
      "2662/2662 [==============================] - 2s 633us/step - loss: 0.2278 - accuracy: 0.9049\n",
      "Epoch 32/100\n",
      "2662/2662 [==============================] - 2s 624us/step - loss: 0.2270 - accuracy: 0.9053\n",
      "Epoch 33/100\n",
      "2662/2662 [==============================] - 2s 636us/step - loss: 0.2262 - accuracy: 0.9059\n",
      "Epoch 34/100\n",
      "2662/2662 [==============================] - 2s 614us/step - loss: 0.2248 - accuracy: 0.9066\n",
      "Epoch 35/100\n",
      "2662/2662 [==============================] - 2s 626us/step - loss: 0.2237 - accuracy: 0.9069\n",
      "Epoch 36/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.2235 - accuracy: 0.9079\n",
      "Epoch 37/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.2223 - accuracy: 0.9076\n",
      "Epoch 38/100\n",
      "2662/2662 [==============================] - 2s 602us/step - loss: 0.2221 - accuracy: 0.9070\n",
      "Epoch 39/100\n",
      "2662/2662 [==============================] - 2s 621us/step - loss: 0.2208 - accuracy: 0.9080\n",
      "Epoch 40/100\n",
      "2662/2662 [==============================] - 2s 620us/step - loss: 0.2198 - accuracy: 0.9084\n",
      "Epoch 41/100\n",
      "2662/2662 [==============================] - 2s 609us/step - loss: 0.2198 - accuracy: 0.9089\n",
      "Epoch 42/100\n",
      "2662/2662 [==============================] - 2s 602us/step - loss: 0.2191 - accuracy: 0.9083\n",
      "Epoch 43/100\n",
      "2662/2662 [==============================] - 2s 604us/step - loss: 0.2175 - accuracy: 0.9097\n",
      "Epoch 44/100\n",
      "2662/2662 [==============================] - 2s 604us/step - loss: 0.2175 - accuracy: 0.9091\n",
      "Epoch 45/100\n",
      "2662/2662 [==============================] - 2s 612us/step - loss: 0.2166 - accuracy: 0.9089\n",
      "Epoch 46/100\n",
      "2662/2662 [==============================] - 2s 600us/step - loss: 0.2154 - accuracy: 0.9104\n",
      "Epoch 47/100\n",
      "2662/2662 [==============================] - 2s 598us/step - loss: 0.2156 - accuracy: 0.9102\n",
      "Epoch 48/100\n",
      "2662/2662 [==============================] - 2s 623us/step - loss: 0.2143 - accuracy: 0.9106\n",
      "Epoch 49/100\n",
      "2662/2662 [==============================] - 2s 597us/step - loss: 0.2144 - accuracy: 0.9117\n",
      "Epoch 50/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.2138 - accuracy: 0.9113\n",
      "Epoch 51/100\n",
      "2662/2662 [==============================] - 2s 612us/step - loss: 0.2128 - accuracy: 0.9113\n",
      "Epoch 52/100\n",
      "2662/2662 [==============================] - 2s 618us/step - loss: 0.2125 - accuracy: 0.9114\n",
      "Epoch 53/100\n",
      "2662/2662 [==============================] - 2s 597us/step - loss: 0.2123 - accuracy: 0.9116\n",
      "Epoch 54/100\n",
      "2662/2662 [==============================] - 2s 600us/step - loss: 0.2117 - accuracy: 0.9119\n",
      "Epoch 55/100\n",
      "2662/2662 [==============================] - 2s 635us/step - loss: 0.2112 - accuracy: 0.9118\n",
      "Epoch 56/100\n",
      "2662/2662 [==============================] - 2s 711us/step - loss: 0.2106 - accuracy: 0.9128\n",
      "Epoch 57/100\n",
      "2662/2662 [==============================] - 2s 660us/step - loss: 0.2100 - accuracy: 0.9127\n",
      "Epoch 58/100\n",
      "2662/2662 [==============================] - 2s 670us/step - loss: 0.2093 - accuracy: 0.9124\n",
      "Epoch 59/100\n",
      "2662/2662 [==============================] - 2s 686us/step - loss: 0.2092 - accuracy: 0.9126\n",
      "Epoch 60/100\n",
      "2662/2662 [==============================] - 2s 659us/step - loss: 0.2085 - accuracy: 0.9141\n",
      "Epoch 61/100\n",
      "2662/2662 [==============================] - 2s 649us/step - loss: 0.2077 - accuracy: 0.9128\n",
      "Epoch 62/100\n",
      "2662/2662 [==============================] - 2s 658us/step - loss: 0.2074 - accuracy: 0.9137\n",
      "Epoch 63/100\n",
      "2662/2662 [==============================] - 2s 665us/step - loss: 0.2071 - accuracy: 0.9134\n",
      "Epoch 64/100\n",
      "2662/2662 [==============================] - 2s 653us/step - loss: 0.2064 - accuracy: 0.9140\n",
      "Epoch 65/100\n",
      "2662/2662 [==============================] - 2s 657us/step - loss: 0.2059 - accuracy: 0.9144\n",
      "Epoch 66/100\n",
      "2662/2662 [==============================] - 2s 660us/step - loss: 0.2058 - accuracy: 0.9138\n",
      "Epoch 67/100\n",
      "2662/2662 [==============================] - 2s 643us/step - loss: 0.2054 - accuracy: 0.9142\n",
      "Epoch 68/100\n",
      "2662/2662 [==============================] - 2s 681us/step - loss: 0.2051 - accuracy: 0.9141\n",
      "Epoch 69/100\n",
      "2662/2662 [==============================] - 2s 659us/step - loss: 0.2053 - accuracy: 0.9148\n",
      "Epoch 70/100\n",
      "2662/2662 [==============================] - 2s 652us/step - loss: 0.2042 - accuracy: 0.9152\n",
      "Epoch 71/100\n",
      "2662/2662 [==============================] - 2s 661us/step - loss: 0.2040 - accuracy: 0.9152\n",
      "Epoch 72/100\n",
      "2662/2662 [==============================] - 2s 675us/step - loss: 0.2033 - accuracy: 0.9158\n",
      "Epoch 73/100\n",
      "2662/2662 [==============================] - 2s 676us/step - loss: 0.2031 - accuracy: 0.9156\n",
      "Epoch 74/100\n",
      "2662/2662 [==============================] - 2s 688us/step - loss: 0.2026 - accuracy: 0.9153\n",
      "Epoch 75/100\n",
      "2662/2662 [==============================] - 2s 685us/step - loss: 0.2032 - accuracy: 0.9151\n",
      "Epoch 76/100\n",
      "2662/2662 [==============================] - 2s 696us/step - loss: 0.2020 - accuracy: 0.9160\n",
      "Epoch 77/100\n",
      "2662/2662 [==============================] - 2s 671us/step - loss: 0.2021 - accuracy: 0.9155\n",
      "Epoch 78/100\n",
      "2662/2662 [==============================] - 2s 653us/step - loss: 0.2018 - accuracy: 0.9156\n",
      "Epoch 79/100\n",
      "2662/2662 [==============================] - 2s 714us/step - loss: 0.2013 - accuracy: 0.9166\n",
      "Epoch 80/100\n",
      "2662/2662 [==============================] - 2s 691us/step - loss: 0.2012 - accuracy: 0.9160\n",
      "Epoch 81/100\n",
      "2662/2662 [==============================] - 2s 701us/step - loss: 0.2012 - accuracy: 0.9155\n",
      "Epoch 82/100\n",
      "2662/2662 [==============================] - 2s 668us/step - loss: 0.2006 - accuracy: 0.9162\n",
      "Epoch 83/100\n",
      "2662/2662 [==============================] - 2s 653us/step - loss: 0.2002 - accuracy: 0.9170\n",
      "Epoch 84/100\n",
      "2662/2662 [==============================] - 2s 671us/step - loss: 0.1999 - accuracy: 0.9160\n",
      "Epoch 85/100\n",
      "2662/2662 [==============================] - 2s 676us/step - loss: 0.1995 - accuracy: 0.9171\n",
      "Epoch 86/100\n",
      "2662/2662 [==============================] - 2s 701us/step - loss: 0.1997 - accuracy: 0.9166\n",
      "Epoch 87/100\n",
      "2662/2662 [==============================] - 2s 695us/step - loss: 0.1992 - accuracy: 0.9170\n",
      "Epoch 88/100\n",
      "2662/2662 [==============================] - 2s 704us/step - loss: 0.1988 - accuracy: 0.9171\n",
      "Epoch 89/100\n",
      "2662/2662 [==============================] - 2s 687us/step - loss: 0.1983 - accuracy: 0.9173\n",
      "Epoch 90/100\n",
      "2662/2662 [==============================] - 2s 753us/step - loss: 0.1987 - accuracy: 0.9177\n",
      "Epoch 91/100\n",
      "2662/2662 [==============================] - 2s 678us/step - loss: 0.1983 - accuracy: 0.9175\n",
      "Epoch 92/100\n",
      "2662/2662 [==============================] - 2s 681us/step - loss: 0.1976 - accuracy: 0.9175\n",
      "Epoch 93/100\n",
      "2662/2662 [==============================] - 2s 682us/step - loss: 0.1977 - accuracy: 0.9178\n",
      "Epoch 94/100\n",
      "2662/2662 [==============================] - 2s 679us/step - loss: 0.1971 - accuracy: 0.9179\n",
      "Epoch 95/100\n",
      "2662/2662 [==============================] - 2s 717us/step - loss: 0.1974 - accuracy: 0.9182\n",
      "Epoch 96/100\n",
      "2662/2662 [==============================] - 2s 700us/step - loss: 0.1970 - accuracy: 0.9184\n",
      "Epoch 97/100\n",
      "2662/2662 [==============================] - 2s 660us/step - loss: 0.1971 - accuracy: 0.9175\n",
      "Epoch 98/100\n",
      "2662/2662 [==============================] - 2s 670us/step - loss: 0.1965 - accuracy: 0.9180\n",
      "Epoch 99/100\n",
      "2662/2662 [==============================] - 2s 666us/step - loss: 0.1962 - accuracy: 0.9183\n",
      "Epoch 100/100\n",
      "2662/2662 [==============================] - 2s 670us/step - loss: 0.1960 - accuracy: 0.9183\n",
      "888/888 - 0s - loss: 0.3503 - accuracy: 0.8705 - 471ms/epoch - 530us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 1: binary population but no other changes to the data processing, with Standard Scaler normalization\n",
    "# Create a dictionary for iteration 1\n",
    "iteration_1 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_1[\"Iteration\"] = 1\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_1 = scaler1.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_1 = X_scaler_1.transform(X_train)\n",
    "X_test_scaled_1 = X_scaler_1.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "nn_1 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_1.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_1 = nn_1.fit(X_train_scaled_1, y_train, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_1, model_accuracy_1 = nn_1.evaluate(X_test_scaled_1, y_test, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_1 dictionary\n",
    "iteration_1[\"Loss\"] = model_loss_1\n",
    "iteration_1[\"Accuracy\"] = model_accuracy_1\n",
    "\n",
    "# Add the iteration_1 dictionary to the iterations list\n",
    "iterations.append(iteration_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,541\n",
      "Trainable params: 7,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2662/2662 [==============================] - 2s 669us/step - loss: 0.3370 - accuracy: 0.8808\n",
      "Epoch 2/100\n",
      "2662/2662 [==============================] - 2s 667us/step - loss: 0.3078 - accuracy: 0.8839\n",
      "Epoch 3/100\n",
      "2662/2662 [==============================] - 2s 654us/step - loss: 0.2921 - accuracy: 0.8860\n",
      "Epoch 4/100\n",
      "2662/2662 [==============================] - 2s 649us/step - loss: 0.2849 - accuracy: 0.8865\n",
      "Epoch 5/100\n",
      "2662/2662 [==============================] - 2s 613us/step - loss: 0.2803 - accuracy: 0.8868\n",
      "Epoch 6/100\n",
      "2662/2662 [==============================] - 2s 596us/step - loss: 0.2770 - accuracy: 0.8882\n",
      "Epoch 7/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.2744 - accuracy: 0.8890\n",
      "Epoch 8/100\n",
      "2662/2662 [==============================] - 2s 604us/step - loss: 0.2723 - accuracy: 0.8884\n",
      "Epoch 9/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.2704 - accuracy: 0.8891\n",
      "Epoch 10/100\n",
      "2662/2662 [==============================] - 2s 606us/step - loss: 0.2685 - accuracy: 0.8900\n",
      "Epoch 11/100\n",
      "2662/2662 [==============================] - 2s 630us/step - loss: 0.2667 - accuracy: 0.8907\n",
      "Epoch 12/100\n",
      "2662/2662 [==============================] - 2s 637us/step - loss: 0.2653 - accuracy: 0.8902\n",
      "Epoch 13/100\n",
      "2662/2662 [==============================] - 2s 629us/step - loss: 0.2638 - accuracy: 0.8917\n",
      "Epoch 14/100\n",
      "2662/2662 [==============================] - 2s 610us/step - loss: 0.2629 - accuracy: 0.8913\n",
      "Epoch 15/100\n",
      "2662/2662 [==============================] - 2s 604us/step - loss: 0.2617 - accuracy: 0.8917\n",
      "Epoch 16/100\n",
      "2662/2662 [==============================] - 2s 638us/step - loss: 0.2602 - accuracy: 0.8922\n",
      "Epoch 17/100\n",
      "2662/2662 [==============================] - 2s 614us/step - loss: 0.2594 - accuracy: 0.8924\n",
      "Epoch 18/100\n",
      "2662/2662 [==============================] - 2s 615us/step - loss: 0.2578 - accuracy: 0.8931\n",
      "Epoch 19/100\n",
      "2662/2662 [==============================] - 2s 604us/step - loss: 0.2574 - accuracy: 0.8931\n",
      "Epoch 20/100\n",
      "2662/2662 [==============================] - 2s 618us/step - loss: 0.2563 - accuracy: 0.8928\n",
      "Epoch 21/100\n",
      "2662/2662 [==============================] - 2s 618us/step - loss: 0.2549 - accuracy: 0.8946\n",
      "Epoch 22/100\n",
      "2662/2662 [==============================] - 2s 608us/step - loss: 0.2546 - accuracy: 0.8943\n",
      "Epoch 23/100\n",
      "2662/2662 [==============================] - 2s 611us/step - loss: 0.2533 - accuracy: 0.8943\n",
      "Epoch 24/100\n",
      "2662/2662 [==============================] - 2s 620us/step - loss: 0.2526 - accuracy: 0.8951\n",
      "Epoch 25/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.2517 - accuracy: 0.8952\n",
      "Epoch 26/100\n",
      "2662/2662 [==============================] - 2s 602us/step - loss: 0.2510 - accuracy: 0.8951\n",
      "Epoch 27/100\n",
      "2662/2662 [==============================] - 2s 602us/step - loss: 0.2498 - accuracy: 0.8951\n",
      "Epoch 28/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.2491 - accuracy: 0.8958\n",
      "Epoch 29/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.2484 - accuracy: 0.8963\n",
      "Epoch 30/100\n",
      "2662/2662 [==============================] - 2s 589us/step - loss: 0.2480 - accuracy: 0.8955\n",
      "Epoch 31/100\n",
      "2662/2662 [==============================] - 2s 592us/step - loss: 0.2471 - accuracy: 0.8962\n",
      "Epoch 32/100\n",
      "2662/2662 [==============================] - 2s 574us/step - loss: 0.2466 - accuracy: 0.8969\n",
      "Epoch 33/100\n",
      "2662/2662 [==============================] - 2s 575us/step - loss: 0.2455 - accuracy: 0.8970\n",
      "Epoch 34/100\n",
      "2662/2662 [==============================] - 2s 579us/step - loss: 0.2450 - accuracy: 0.8978\n",
      "Epoch 35/100\n",
      "2662/2662 [==============================] - 2s 578us/step - loss: 0.2448 - accuracy: 0.8972\n",
      "Epoch 36/100\n",
      "2662/2662 [==============================] - 2s 595us/step - loss: 0.2438 - accuracy: 0.8976\n",
      "Epoch 37/100\n",
      "2662/2662 [==============================] - 2s 607us/step - loss: 0.2425 - accuracy: 0.8981\n",
      "Epoch 38/100\n",
      "2662/2662 [==============================] - 2s 613us/step - loss: 0.2424 - accuracy: 0.8986\n",
      "Epoch 39/100\n",
      "2662/2662 [==============================] - 2s 599us/step - loss: 0.2417 - accuracy: 0.8982\n",
      "Epoch 40/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.2411 - accuracy: 0.8991\n",
      "Epoch 41/100\n",
      "2662/2662 [==============================] - 2s 606us/step - loss: 0.2406 - accuracy: 0.8991\n",
      "Epoch 42/100\n",
      "2662/2662 [==============================] - 2s 597us/step - loss: 0.2399 - accuracy: 0.8994\n",
      "Epoch 43/100\n",
      "2662/2662 [==============================] - 2s 600us/step - loss: 0.2397 - accuracy: 0.8998\n",
      "Epoch 44/100\n",
      "2662/2662 [==============================] - 2s 601us/step - loss: 0.2391 - accuracy: 0.8996\n",
      "Epoch 45/100\n",
      "2662/2662 [==============================] - 2s 580us/step - loss: 0.2383 - accuracy: 0.8998\n",
      "Epoch 46/100\n",
      "2662/2662 [==============================] - 2s 590us/step - loss: 0.2378 - accuracy: 0.9002\n",
      "Epoch 47/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.2376 - accuracy: 0.9003\n",
      "Epoch 48/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.2368 - accuracy: 0.9008\n",
      "Epoch 49/100\n",
      "2662/2662 [==============================] - 2s 578us/step - loss: 0.2363 - accuracy: 0.8998\n",
      "Epoch 50/100\n",
      "2662/2662 [==============================] - 2s 602us/step - loss: 0.2360 - accuracy: 0.9006\n",
      "Epoch 51/100\n",
      "2662/2662 [==============================] - 2s 585us/step - loss: 0.2357 - accuracy: 0.9014\n",
      "Epoch 52/100\n",
      "2662/2662 [==============================] - 2s 649us/step - loss: 0.2347 - accuracy: 0.9015\n",
      "Epoch 53/100\n",
      "2662/2662 [==============================] - 2s 673us/step - loss: 0.2343 - accuracy: 0.9018\n",
      "Epoch 54/100\n",
      "2662/2662 [==============================] - 2s 695us/step - loss: 0.2336 - accuracy: 0.9022\n",
      "Epoch 55/100\n",
      "2662/2662 [==============================] - 2s 694us/step - loss: 0.2333 - accuracy: 0.9017\n",
      "Epoch 56/100\n",
      "2662/2662 [==============================] - 2s 697us/step - loss: 0.2327 - accuracy: 0.9018\n",
      "Epoch 57/100\n",
      "2662/2662 [==============================] - 2s 682us/step - loss: 0.2325 - accuracy: 0.9034\n",
      "Epoch 58/100\n",
      "2662/2662 [==============================] - 2s 695us/step - loss: 0.2319 - accuracy: 0.9029\n",
      "Epoch 59/100\n",
      "2662/2662 [==============================] - 2s 673us/step - loss: 0.2312 - accuracy: 0.9024\n",
      "Epoch 60/100\n",
      "2662/2662 [==============================] - 2s 671us/step - loss: 0.2309 - accuracy: 0.9025\n",
      "Epoch 61/100\n",
      "2662/2662 [==============================] - 2s 675us/step - loss: 0.2309 - accuracy: 0.9029\n",
      "Epoch 62/100\n",
      "2662/2662 [==============================] - 2s 678us/step - loss: 0.2302 - accuracy: 0.9035\n",
      "Epoch 63/100\n",
      "2662/2662 [==============================] - 2s 681us/step - loss: 0.2292 - accuracy: 0.9032\n",
      "Epoch 64/100\n",
      "2662/2662 [==============================] - 2s 655us/step - loss: 0.2294 - accuracy: 0.9038\n",
      "Epoch 65/100\n",
      "2662/2662 [==============================] - 2s 654us/step - loss: 0.2287 - accuracy: 0.9039\n",
      "Epoch 66/100\n",
      "2662/2662 [==============================] - 2s 662us/step - loss: 0.2287 - accuracy: 0.9037\n",
      "Epoch 67/100\n",
      "2662/2662 [==============================] - 2s 667us/step - loss: 0.2279 - accuracy: 0.9040\n",
      "Epoch 68/100\n",
      "2662/2662 [==============================] - 2s 671us/step - loss: 0.2276 - accuracy: 0.9038\n",
      "Epoch 69/100\n",
      "2662/2662 [==============================] - 2s 673us/step - loss: 0.2269 - accuracy: 0.9045\n",
      "Epoch 70/100\n",
      "2662/2662 [==============================] - 2s 691us/step - loss: 0.2266 - accuracy: 0.9036\n",
      "Epoch 71/100\n",
      "2662/2662 [==============================] - 2s 692us/step - loss: 0.2265 - accuracy: 0.9047\n",
      "Epoch 72/100\n",
      "2662/2662 [==============================] - 2s 698us/step - loss: 0.2260 - accuracy: 0.9046\n",
      "Epoch 73/100\n",
      "2662/2662 [==============================] - 2s 687us/step - loss: 0.2262 - accuracy: 0.9047\n",
      "Epoch 74/100\n",
      "2662/2662 [==============================] - 2s 688us/step - loss: 0.2258 - accuracy: 0.9047\n",
      "Epoch 75/100\n",
      "2662/2662 [==============================] - 2s 668us/step - loss: 0.2250 - accuracy: 0.9053\n",
      "Epoch 76/100\n",
      "2662/2662 [==============================] - 2s 682us/step - loss: 0.2252 - accuracy: 0.9045\n",
      "Epoch 77/100\n",
      "2662/2662 [==============================] - 2s 675us/step - loss: 0.2247 - accuracy: 0.9049\n",
      "Epoch 78/100\n",
      "2662/2662 [==============================] - 2s 712us/step - loss: 0.2242 - accuracy: 0.9056\n",
      "Epoch 79/100\n",
      "2662/2662 [==============================] - 2s 678us/step - loss: 0.2241 - accuracy: 0.9052\n",
      "Epoch 80/100\n",
      "2662/2662 [==============================] - 2s 684us/step - loss: 0.2237 - accuracy: 0.9055\n",
      "Epoch 81/100\n",
      "2662/2662 [==============================] - 2s 700us/step - loss: 0.2235 - accuracy: 0.9060\n",
      "Epoch 82/100\n",
      "2662/2662 [==============================] - 2s 679us/step - loss: 0.2226 - accuracy: 0.9064\n",
      "Epoch 83/100\n",
      "2662/2662 [==============================] - 2s 688us/step - loss: 0.2226 - accuracy: 0.9062\n",
      "Epoch 84/100\n",
      "2662/2662 [==============================] - 2s 677us/step - loss: 0.2223 - accuracy: 0.9064\n",
      "Epoch 85/100\n",
      "2662/2662 [==============================] - 2s 689us/step - loss: 0.2220 - accuracy: 0.9061\n",
      "Epoch 86/100\n",
      "2662/2662 [==============================] - 2s 680us/step - loss: 0.2216 - accuracy: 0.9067\n",
      "Epoch 87/100\n",
      "2662/2662 [==============================] - 2s 671us/step - loss: 0.2220 - accuracy: 0.9061\n",
      "Epoch 88/100\n",
      "2662/2662 [==============================] - 2s 679us/step - loss: 0.2211 - accuracy: 0.9062\n",
      "Epoch 89/100\n",
      "2662/2662 [==============================] - 2s 666us/step - loss: 0.2210 - accuracy: 0.9065\n",
      "Epoch 90/100\n",
      "2662/2662 [==============================] - 2s 678us/step - loss: 0.2207 - accuracy: 0.9073\n",
      "Epoch 91/100\n",
      "2662/2662 [==============================] - 2s 648us/step - loss: 0.2207 - accuracy: 0.9079\n",
      "Epoch 92/100\n",
      "2662/2662 [==============================] - 2s 676us/step - loss: 0.2199 - accuracy: 0.9071\n",
      "Epoch 93/100\n",
      "2662/2662 [==============================] - 2s 712us/step - loss: 0.2201 - accuracy: 0.9069\n",
      "Epoch 94/100\n",
      "2662/2662 [==============================] - 2s 769us/step - loss: 0.2194 - accuracy: 0.9075\n",
      "Epoch 95/100\n",
      "2662/2662 [==============================] - 2s 769us/step - loss: 0.2195 - accuracy: 0.9076\n",
      "Epoch 96/100\n",
      "2662/2662 [==============================] - 2s 756us/step - loss: 0.2188 - accuracy: 0.9071\n",
      "Epoch 97/100\n",
      "2662/2662 [==============================] - 2s 721us/step - loss: 0.2185 - accuracy: 0.9073\n",
      "Epoch 98/100\n",
      "2662/2662 [==============================] - 2s 742us/step - loss: 0.2187 - accuracy: 0.9078\n",
      "Epoch 99/100\n",
      "2662/2662 [==============================] - 2s 704us/step - loss: 0.2182 - accuracy: 0.9074\n",
      "Epoch 100/100\n",
      "2662/2662 [==============================] - 2s 647us/step - loss: 0.2183 - accuracy: 0.9079\n",
      "888/888 - 0s - loss: 0.3183 - accuracy: 0.8752 - 463ms/epoch - 521us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 2: Same as above, but Min Max normalization\n",
    "# Create a dictionary for iteration 2\n",
    "iteration_2 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_2[\"Iteration\"] = 2\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler2 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_2 = scaler2.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_2 = X_scaler_2.transform(X_train)\n",
    "X_test_scaled_2 = X_scaler_2.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "nn_2 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_2.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_2 = nn_2.fit(X_train_scaled_2, y_train, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_2, model_accuracy_2 = nn_2.evaluate(X_test_scaled_2, y_test, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_2 dictionary\n",
    "iteration_2[\"Loss\"] = model_loss_2\n",
    "iteration_2[\"Accuracy\"] = model_accuracy_2\n",
    "\n",
    "# Add the iteration_2 dictionary to the iterations list\n",
    "iterations.append(iteration_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,541\n",
      "Trainable params: 7,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4687/4687 [==============================] - 3s 606us/step - loss: 0.5564 - accuracy: 0.7060\n",
      "Epoch 2/100\n",
      "4687/4687 [==============================] - 3s 610us/step - loss: 0.4790 - accuracy: 0.7660\n",
      "Epoch 3/100\n",
      "4687/4687 [==============================] - 3s 601us/step - loss: 0.4596 - accuracy: 0.7782\n",
      "Epoch 4/100\n",
      "4687/4687 [==============================] - 3s 597us/step - loss: 0.4474 - accuracy: 0.7845\n",
      "Epoch 5/100\n",
      "4687/4687 [==============================] - 3s 606us/step - loss: 0.4376 - accuracy: 0.7900\n",
      "Epoch 6/100\n",
      "4687/4687 [==============================] - 3s 591us/step - loss: 0.4294 - accuracy: 0.7947\n",
      "Epoch 7/100\n",
      "4687/4687 [==============================] - 3s 611us/step - loss: 0.4220 - accuracy: 0.7990\n",
      "Epoch 8/100\n",
      "4687/4687 [==============================] - 3s 624us/step - loss: 0.4155 - accuracy: 0.8036\n",
      "Epoch 9/100\n",
      "4687/4687 [==============================] - 3s 600us/step - loss: 0.4100 - accuracy: 0.8063\n",
      "Epoch 10/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.4042 - accuracy: 0.8113\n",
      "Epoch 11/100\n",
      "4687/4687 [==============================] - 3s 578us/step - loss: 0.3988 - accuracy: 0.8145\n",
      "Epoch 12/100\n",
      "4687/4687 [==============================] - 3s 580us/step - loss: 0.3947 - accuracy: 0.8161\n",
      "Epoch 13/100\n",
      "4687/4687 [==============================] - 3s 590us/step - loss: 0.3903 - accuracy: 0.8199\n",
      "Epoch 14/100\n",
      "4687/4687 [==============================] - 3s 601us/step - loss: 0.3859 - accuracy: 0.8216\n",
      "Epoch 15/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.3817 - accuracy: 0.8250\n",
      "Epoch 16/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.3779 - accuracy: 0.8266\n",
      "Epoch 17/100\n",
      "4687/4687 [==============================] - 3s 599us/step - loss: 0.3746 - accuracy: 0.8287\n",
      "Epoch 18/100\n",
      "4687/4687 [==============================] - 3s 583us/step - loss: 0.3708 - accuracy: 0.8316\n",
      "Epoch 19/100\n",
      "4687/4687 [==============================] - 3s 569us/step - loss: 0.3676 - accuracy: 0.8332\n",
      "Epoch 20/100\n",
      "4687/4687 [==============================] - 3s 567us/step - loss: 0.3652 - accuracy: 0.8351\n",
      "Epoch 21/100\n",
      "4687/4687 [==============================] - 3s 574us/step - loss: 0.3624 - accuracy: 0.8371\n",
      "Epoch 22/100\n",
      "4687/4687 [==============================] - 3s 580us/step - loss: 0.3592 - accuracy: 0.8386\n",
      "Epoch 23/100\n",
      "4687/4687 [==============================] - 3s 575us/step - loss: 0.3569 - accuracy: 0.8406\n",
      "Epoch 24/100\n",
      "4687/4687 [==============================] - 3s 569us/step - loss: 0.3541 - accuracy: 0.8420\n",
      "Epoch 25/100\n",
      "4687/4687 [==============================] - 3s 574us/step - loss: 0.3524 - accuracy: 0.8429\n",
      "Epoch 26/100\n",
      "4687/4687 [==============================] - 3s 576us/step - loss: 0.3495 - accuracy: 0.8449\n",
      "Epoch 27/100\n",
      "4687/4687 [==============================] - 3s 576us/step - loss: 0.3473 - accuracy: 0.8465\n",
      "Epoch 28/100\n",
      "4687/4687 [==============================] - 3s 570us/step - loss: 0.3458 - accuracy: 0.8471\n",
      "Epoch 29/100\n",
      "4687/4687 [==============================] - 3s 563us/step - loss: 0.3443 - accuracy: 0.8481\n",
      "Epoch 30/100\n",
      "4687/4687 [==============================] - 3s 572us/step - loss: 0.3416 - accuracy: 0.8494\n",
      "Epoch 31/100\n",
      "4687/4687 [==============================] - 3s 571us/step - loss: 0.3400 - accuracy: 0.8506\n",
      "Epoch 32/100\n",
      "4687/4687 [==============================] - 3s 581us/step - loss: 0.3387 - accuracy: 0.8513\n",
      "Epoch 33/100\n",
      "4687/4687 [==============================] - 3s 568us/step - loss: 0.3368 - accuracy: 0.8524\n",
      "Epoch 34/100\n",
      "4687/4687 [==============================] - 3s 577us/step - loss: 0.3351 - accuracy: 0.8538\n",
      "Epoch 35/100\n",
      "4687/4687 [==============================] - 3s 566us/step - loss: 0.3333 - accuracy: 0.8546\n",
      "Epoch 36/100\n",
      "4687/4687 [==============================] - 3s 572us/step - loss: 0.3320 - accuracy: 0.8552\n",
      "Epoch 37/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.3302 - accuracy: 0.8564\n",
      "Epoch 38/100\n",
      "4687/4687 [==============================] - 3s 582us/step - loss: 0.3295 - accuracy: 0.8563\n",
      "Epoch 39/100\n",
      "4687/4687 [==============================] - 3s 591us/step - loss: 0.3276 - accuracy: 0.8580\n",
      "Epoch 40/100\n",
      "4687/4687 [==============================] - 3s 569us/step - loss: 0.3259 - accuracy: 0.8590\n",
      "Epoch 41/100\n",
      "4687/4687 [==============================] - 3s 567us/step - loss: 0.3254 - accuracy: 0.8592\n",
      "Epoch 42/100\n",
      "4687/4687 [==============================] - 3s 564us/step - loss: 0.3236 - accuracy: 0.8593\n",
      "Epoch 43/100\n",
      "4687/4687 [==============================] - 3s 565us/step - loss: 0.3227 - accuracy: 0.8611\n",
      "Epoch 44/100\n",
      "4687/4687 [==============================] - 3s 562us/step - loss: 0.3218 - accuracy: 0.8614\n",
      "Epoch 45/100\n",
      "4687/4687 [==============================] - 3s 561us/step - loss: 0.3203 - accuracy: 0.8616\n",
      "Epoch 46/100\n",
      "4687/4687 [==============================] - 3s 570us/step - loss: 0.3199 - accuracy: 0.8629\n",
      "Epoch 47/100\n",
      "4687/4687 [==============================] - 3s 581us/step - loss: 0.3184 - accuracy: 0.8634\n",
      "Epoch 48/100\n",
      "4687/4687 [==============================] - 3s 598us/step - loss: 0.3177 - accuracy: 0.8641\n",
      "Epoch 49/100\n",
      "4687/4687 [==============================] - 3s 596us/step - loss: 0.3158 - accuracy: 0.8636\n",
      "Epoch 50/100\n",
      "4687/4687 [==============================] - 3s 568us/step - loss: 0.3150 - accuracy: 0.8650\n",
      "Epoch 51/100\n",
      "4687/4687 [==============================] - 3s 568us/step - loss: 0.3135 - accuracy: 0.8657\n",
      "Epoch 52/100\n",
      "4687/4687 [==============================] - 3s 568us/step - loss: 0.3133 - accuracy: 0.8669\n",
      "Epoch 53/100\n",
      "4687/4687 [==============================] - 3s 575us/step - loss: 0.3121 - accuracy: 0.8658\n",
      "Epoch 54/100\n",
      "4687/4687 [==============================] - 3s 578us/step - loss: 0.3110 - accuracy: 0.8675\n",
      "Epoch 55/100\n",
      "4687/4687 [==============================] - 3s 576us/step - loss: 0.3110 - accuracy: 0.8672\n",
      "Epoch 56/100\n",
      "4687/4687 [==============================] - 3s 571us/step - loss: 0.3104 - accuracy: 0.8675\n",
      "Epoch 57/100\n",
      "4687/4687 [==============================] - 3s 568us/step - loss: 0.3086 - accuracy: 0.8680\n",
      "Epoch 58/100\n",
      "4687/4687 [==============================] - 3s 564us/step - loss: 0.3084 - accuracy: 0.8687\n",
      "Epoch 59/100\n",
      "4687/4687 [==============================] - 3s 569us/step - loss: 0.3071 - accuracy: 0.8689\n",
      "Epoch 60/100\n",
      "4687/4687 [==============================] - 3s 569us/step - loss: 0.3066 - accuracy: 0.8696\n",
      "Epoch 61/100\n",
      "4687/4687 [==============================] - 3s 573us/step - loss: 0.3060 - accuracy: 0.8698\n",
      "Epoch 62/100\n",
      "4687/4687 [==============================] - 3s 575us/step - loss: 0.3052 - accuracy: 0.8711\n",
      "Epoch 63/100\n",
      "4687/4687 [==============================] - 3s 570us/step - loss: 0.3045 - accuracy: 0.8710\n",
      "Epoch 64/100\n",
      "4687/4687 [==============================] - 3s 569us/step - loss: 0.3036 - accuracy: 0.8718\n",
      "Epoch 65/100\n",
      "4687/4687 [==============================] - 3s 562us/step - loss: 0.3030 - accuracy: 0.8715\n",
      "Epoch 66/100\n",
      "4687/4687 [==============================] - 3s 564us/step - loss: 0.3028 - accuracy: 0.8726\n",
      "Epoch 67/100\n",
      "4687/4687 [==============================] - 3s 567us/step - loss: 0.3014 - accuracy: 0.8728\n",
      "Epoch 68/100\n",
      "4687/4687 [==============================] - 3s 562us/step - loss: 0.3010 - accuracy: 0.8724\n",
      "Epoch 69/100\n",
      "4687/4687 [==============================] - 3s 571us/step - loss: 0.3002 - accuracy: 0.8735\n",
      "Epoch 70/100\n",
      "4687/4687 [==============================] - 3s 567us/step - loss: 0.2997 - accuracy: 0.8741\n",
      "Epoch 71/100\n",
      "4687/4687 [==============================] - 3s 578us/step - loss: 0.2998 - accuracy: 0.8743\n",
      "Epoch 72/100\n",
      "4687/4687 [==============================] - 3s 596us/step - loss: 0.2981 - accuracy: 0.8752\n",
      "Epoch 73/100\n",
      "4687/4687 [==============================] - 3s 583us/step - loss: 0.2978 - accuracy: 0.8745\n",
      "Epoch 74/100\n",
      "4687/4687 [==============================] - 3s 571us/step - loss: 0.2973 - accuracy: 0.8757\n",
      "Epoch 75/100\n",
      "4687/4687 [==============================] - 3s 577us/step - loss: 0.2967 - accuracy: 0.8753\n",
      "Epoch 76/100\n",
      "4687/4687 [==============================] - 3s 619us/step - loss: 0.2956 - accuracy: 0.8763\n",
      "Epoch 77/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.2955 - accuracy: 0.8768\n",
      "Epoch 78/100\n",
      "4687/4687 [==============================] - 3s 580us/step - loss: 0.2951 - accuracy: 0.8768\n",
      "Epoch 79/100\n",
      "4687/4687 [==============================] - 3s 593us/step - loss: 0.2942 - accuracy: 0.8770\n",
      "Epoch 80/100\n",
      "4687/4687 [==============================] - 3s 587us/step - loss: 0.2944 - accuracy: 0.8763\n",
      "Epoch 81/100\n",
      "4687/4687 [==============================] - 3s 600us/step - loss: 0.2942 - accuracy: 0.8765\n",
      "Epoch 82/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.2926 - accuracy: 0.8778\n",
      "Epoch 83/100\n",
      "4687/4687 [==============================] - 3s 596us/step - loss: 0.2929 - accuracy: 0.8779\n",
      "Epoch 84/100\n",
      "4687/4687 [==============================] - 3s 613us/step - loss: 0.2915 - accuracy: 0.8783\n",
      "Epoch 85/100\n",
      "4687/4687 [==============================] - 3s 576us/step - loss: 0.2920 - accuracy: 0.8786\n",
      "Epoch 86/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.2914 - accuracy: 0.8780\n",
      "Epoch 87/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.2907 - accuracy: 0.8791\n",
      "Epoch 88/100\n",
      "4687/4687 [==============================] - 3s 570us/step - loss: 0.2901 - accuracy: 0.8795\n",
      "Epoch 89/100\n",
      "4687/4687 [==============================] - 3s 577us/step - loss: 0.2895 - accuracy: 0.8796\n",
      "Epoch 90/100\n",
      "4687/4687 [==============================] - 3s 578us/step - loss: 0.2893 - accuracy: 0.8794\n",
      "Epoch 91/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.2885 - accuracy: 0.8796\n",
      "Epoch 92/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.2879 - accuracy: 0.8805\n",
      "Epoch 93/100\n",
      "4687/4687 [==============================] - 3s 583us/step - loss: 0.2877 - accuracy: 0.8805\n",
      "Epoch 94/100\n",
      "4687/4687 [==============================] - 3s 563us/step - loss: 0.2880 - accuracy: 0.8804\n",
      "Epoch 95/100\n",
      "4687/4687 [==============================] - 3s 572us/step - loss: 0.2869 - accuracy: 0.8811\n",
      "Epoch 96/100\n",
      "4687/4687 [==============================] - 3s 631us/step - loss: 0.2868 - accuracy: 0.8813\n",
      "Epoch 97/100\n",
      "4687/4687 [==============================] - 3s 660us/step - loss: 0.2863 - accuracy: 0.8816\n",
      "Epoch 98/100\n",
      "4687/4687 [==============================] - 3s 663us/step - loss: 0.2856 - accuracy: 0.8828\n",
      "Epoch 99/100\n",
      "4687/4687 [==============================] - 3s 664us/step - loss: 0.2860 - accuracy: 0.8821\n",
      "Epoch 100/100\n",
      "4687/4687 [==============================] - 3s 609us/step - loss: 0.2845 - accuracy: 0.8821\n",
      "888/888 - 0s - loss: 0.5668 - accuracy: 0.7746 - 431ms/epoch - 485us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 3: Same as above, but adds random oversampling to adjust for skewed data\n",
    "# Create a dictionary for iteration 3\n",
    "iteration_3 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_3[\"Iteration\"] = 3\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler3 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_3 = scaler3.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_3 = X_scaler_3.transform(X_train)\n",
    "X_test_scaled_3 = X_scaler_3.transform(X_test)\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_3 = RandomOverSampler(random_state=1)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_3, y_ros_3 = ros_3.fit_resample(X_train_scaled_3, y_train)\n",
    "\n",
    "# Count the distinct values of the resampled labels data\n",
    "Counter(y_ros_3)\n",
    "\n",
    "# Define the model\n",
    "nn_3 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_3.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_3 = nn_3.fit(X_ros_3, y_ros_3, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_3, model_accuracy_3 = nn_3.evaluate(X_test_scaled_3, y_test, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_3 dictionary\n",
    "iteration_3[\"Loss\"] = model_loss_3\n",
    "iteration_3[\"Accuracy\"] = model_accuracy_3\n",
    "\n",
    "# Add the iteration_3 dictionary to the iterations list\n",
    "iterations.append(iteration_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,541\n",
      "Trainable params: 7,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4687/4687 [==============================] - 4s 703us/step - loss: 0.5567 - accuracy: 0.7052\n",
      "Epoch 2/100\n",
      "4687/4687 [==============================] - 3s 621us/step - loss: 0.4787 - accuracy: 0.7652\n",
      "Epoch 3/100\n",
      "4687/4687 [==============================] - 3s 640us/step - loss: 0.4582 - accuracy: 0.7790\n",
      "Epoch 4/100\n",
      "4687/4687 [==============================] - 3s 678us/step - loss: 0.4451 - accuracy: 0.7863\n",
      "Epoch 5/100\n",
      "4687/4687 [==============================] - 4s 772us/step - loss: 0.4346 - accuracy: 0.7920\n",
      "Epoch 6/100\n",
      "4687/4687 [==============================] - 4s 817us/step - loss: 0.4256 - accuracy: 0.7973\n",
      "Epoch 7/100\n",
      "4687/4687 [==============================] - 3s 665us/step - loss: 0.4176 - accuracy: 0.8022\n",
      "Epoch 8/100\n",
      "4687/4687 [==============================] - 3s 656us/step - loss: 0.4101 - accuracy: 0.8058\n",
      "Epoch 9/100\n",
      "4687/4687 [==============================] - 3s 580us/step - loss: 0.4037 - accuracy: 0.8099\n",
      "Epoch 10/100\n",
      "4687/4687 [==============================] - 3s 614us/step - loss: 0.3980 - accuracy: 0.8135\n",
      "Epoch 11/100\n",
      "4687/4687 [==============================] - 3s 670us/step - loss: 0.3928 - accuracy: 0.8176\n",
      "Epoch 12/100\n",
      "4687/4687 [==============================] - 3s 662us/step - loss: 0.3885 - accuracy: 0.8204\n",
      "Epoch 13/100\n",
      "4687/4687 [==============================] - 3s 635us/step - loss: 0.3836 - accuracy: 0.8228\n",
      "Epoch 14/100\n",
      "4687/4687 [==============================] - 3s 637us/step - loss: 0.3803 - accuracy: 0.8241\n",
      "Epoch 15/100\n",
      "4687/4687 [==============================] - 3s 631us/step - loss: 0.3758 - accuracy: 0.8270\n",
      "Epoch 16/100\n",
      "4687/4687 [==============================] - 3s 622us/step - loss: 0.3718 - accuracy: 0.8291\n",
      "Epoch 17/100\n",
      "4687/4687 [==============================] - 3s 613us/step - loss: 0.3685 - accuracy: 0.8312\n",
      "Epoch 18/100\n",
      "4687/4687 [==============================] - 3s 618us/step - loss: 0.3652 - accuracy: 0.8328\n",
      "Epoch 19/100\n",
      "4687/4687 [==============================] - 3s 611us/step - loss: 0.3622 - accuracy: 0.8356\n",
      "Epoch 20/100\n",
      "4687/4687 [==============================] - 3s 622us/step - loss: 0.3587 - accuracy: 0.8378\n",
      "Epoch 21/100\n",
      "4687/4687 [==============================] - 3s 619us/step - loss: 0.3566 - accuracy: 0.8388\n",
      "Epoch 22/100\n",
      "4687/4687 [==============================] - 3s 617us/step - loss: 0.3533 - accuracy: 0.8396\n",
      "Epoch 23/100\n",
      "4687/4687 [==============================] - 3s 621us/step - loss: 0.3505 - accuracy: 0.8433\n",
      "Epoch 24/100\n",
      "4687/4687 [==============================] - 3s 618us/step - loss: 0.3484 - accuracy: 0.8442\n",
      "Epoch 25/100\n",
      "4687/4687 [==============================] - 3s 622us/step - loss: 0.3463 - accuracy: 0.8455\n",
      "Epoch 26/100\n",
      "4687/4687 [==============================] - 3s 617us/step - loss: 0.3440 - accuracy: 0.8460\n",
      "Epoch 27/100\n",
      "4687/4687 [==============================] - 3s 619us/step - loss: 0.3415 - accuracy: 0.8482\n",
      "Epoch 28/100\n",
      "4687/4687 [==============================] - 3s 619us/step - loss: 0.3400 - accuracy: 0.8500\n",
      "Epoch 29/100\n",
      "4687/4687 [==============================] - 3s 611us/step - loss: 0.3383 - accuracy: 0.8496\n",
      "Epoch 30/100\n",
      "4687/4687 [==============================] - 3s 615us/step - loss: 0.3368 - accuracy: 0.8513\n",
      "Epoch 31/100\n",
      "4687/4687 [==============================] - 3s 610us/step - loss: 0.3351 - accuracy: 0.8516\n",
      "Epoch 32/100\n",
      "4687/4687 [==============================] - 3s 612us/step - loss: 0.3325 - accuracy: 0.8533\n",
      "Epoch 33/100\n",
      "4687/4687 [==============================] - 3s 615us/step - loss: 0.3313 - accuracy: 0.8546\n",
      "Epoch 34/100\n",
      "4687/4687 [==============================] - 3s 610us/step - loss: 0.3290 - accuracy: 0.8557\n",
      "Epoch 35/100\n",
      "4687/4687 [==============================] - 3s 615us/step - loss: 0.3281 - accuracy: 0.8565\n",
      "Epoch 36/100\n",
      "4687/4687 [==============================] - 3s 611us/step - loss: 0.3267 - accuracy: 0.8574\n",
      "Epoch 37/100\n",
      "4687/4687 [==============================] - 3s 621us/step - loss: 0.3254 - accuracy: 0.8585\n",
      "Epoch 38/100\n",
      "4687/4687 [==============================] - 3s 617us/step - loss: 0.3230 - accuracy: 0.8589\n",
      "Epoch 39/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.3223 - accuracy: 0.8608\n",
      "Epoch 40/100\n",
      "4687/4687 [==============================] - 3s 574us/step - loss: 0.3208 - accuracy: 0.8602\n",
      "Epoch 41/100\n",
      "4687/4687 [==============================] - 3s 579us/step - loss: 0.3192 - accuracy: 0.8617\n",
      "Epoch 42/100\n",
      "4687/4687 [==============================] - 3s 578us/step - loss: 0.3179 - accuracy: 0.8625\n",
      "Epoch 43/100\n",
      "4687/4687 [==============================] - 3s 580us/step - loss: 0.3165 - accuracy: 0.8633\n",
      "Epoch 44/100\n",
      "4687/4687 [==============================] - 3s 579us/step - loss: 0.3159 - accuracy: 0.8633\n",
      "Epoch 45/100\n",
      "4687/4687 [==============================] - 3s 591us/step - loss: 0.3152 - accuracy: 0.8638\n",
      "Epoch 46/100\n",
      "4687/4687 [==============================] - 3s 602us/step - loss: 0.3140 - accuracy: 0.8643\n",
      "Epoch 47/100\n",
      "4687/4687 [==============================] - 3s 590us/step - loss: 0.3120 - accuracy: 0.8659\n",
      "Epoch 48/100\n",
      "4687/4687 [==============================] - 3s 580us/step - loss: 0.3113 - accuracy: 0.8659\n",
      "Epoch 49/100\n",
      "4687/4687 [==============================] - 3s 586us/step - loss: 0.3107 - accuracy: 0.8666\n",
      "Epoch 50/100\n",
      "4687/4687 [==============================] - 3s 607us/step - loss: 0.3092 - accuracy: 0.8665\n",
      "Epoch 51/100\n",
      "4687/4687 [==============================] - 3s 573us/step - loss: 0.3086 - accuracy: 0.8670\n",
      "Epoch 52/100\n",
      "4687/4687 [==============================] - 3s 609us/step - loss: 0.3073 - accuracy: 0.8687\n",
      "Epoch 53/100\n",
      "4687/4687 [==============================] - 3s 570us/step - loss: 0.3060 - accuracy: 0.8692\n",
      "Epoch 54/100\n",
      "4687/4687 [==============================] - 3s 576us/step - loss: 0.3054 - accuracy: 0.8702\n",
      "Epoch 55/100\n",
      "4687/4687 [==============================] - 3s 580us/step - loss: 0.3043 - accuracy: 0.8695\n",
      "Epoch 56/100\n",
      "4687/4687 [==============================] - 3s 586us/step - loss: 0.3032 - accuracy: 0.8703\n",
      "Epoch 57/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.3027 - accuracy: 0.8713\n",
      "Epoch 58/100\n",
      "4687/4687 [==============================] - 3s 599us/step - loss: 0.3014 - accuracy: 0.8717\n",
      "Epoch 59/100\n",
      "4687/4687 [==============================] - 3s 586us/step - loss: 0.3005 - accuracy: 0.8727\n",
      "Epoch 60/100\n",
      "4687/4687 [==============================] - 3s 579us/step - loss: 0.3002 - accuracy: 0.8722\n",
      "Epoch 61/100\n",
      "4687/4687 [==============================] - 3s 585us/step - loss: 0.2991 - accuracy: 0.8738\n",
      "Epoch 62/100\n",
      "4687/4687 [==============================] - 3s 647us/step - loss: 0.2985 - accuracy: 0.8731\n",
      "Epoch 63/100\n",
      "4687/4687 [==============================] - 3s 653us/step - loss: 0.2978 - accuracy: 0.8738\n",
      "Epoch 64/100\n",
      "4687/4687 [==============================] - 3s 652us/step - loss: 0.2967 - accuracy: 0.8745\n",
      "Epoch 65/100\n",
      "4687/4687 [==============================] - 3s 670us/step - loss: 0.2961 - accuracy: 0.8753\n",
      "Epoch 66/100\n",
      "4687/4687 [==============================] - 3s 616us/step - loss: 0.2953 - accuracy: 0.8751\n",
      "Epoch 67/100\n",
      "4687/4687 [==============================] - 3s 587us/step - loss: 0.2952 - accuracy: 0.8758\n",
      "Epoch 68/100\n",
      "4687/4687 [==============================] - 3s 587us/step - loss: 0.2940 - accuracy: 0.8764\n",
      "Epoch 69/100\n",
      "4687/4687 [==============================] - 3s 596us/step - loss: 0.2938 - accuracy: 0.8765\n",
      "Epoch 70/100\n",
      "4687/4687 [==============================] - 3s 597us/step - loss: 0.2928 - accuracy: 0.8773\n",
      "Epoch 71/100\n",
      "4687/4687 [==============================] - 3s 583us/step - loss: 0.2917 - accuracy: 0.8781\n",
      "Epoch 72/100\n",
      "4687/4687 [==============================] - 3s 609us/step - loss: 0.2913 - accuracy: 0.8779\n",
      "Epoch 73/100\n",
      "4687/4687 [==============================] - 3s 612us/step - loss: 0.2909 - accuracy: 0.8788\n",
      "Epoch 74/100\n",
      "4687/4687 [==============================] - 3s 592us/step - loss: 0.2898 - accuracy: 0.8788\n",
      "Epoch 75/100\n",
      "4687/4687 [==============================] - 3s 600us/step - loss: 0.2904 - accuracy: 0.8786\n",
      "Epoch 76/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.2889 - accuracy: 0.8791\n",
      "Epoch 77/100\n",
      "4687/4687 [==============================] - 3s 585us/step - loss: 0.2887 - accuracy: 0.8792\n",
      "Epoch 78/100\n",
      "4687/4687 [==============================] - 3s 589us/step - loss: 0.2880 - accuracy: 0.8792\n",
      "Epoch 79/100\n",
      "4687/4687 [==============================] - 3s 592us/step - loss: 0.2876 - accuracy: 0.8797\n",
      "Epoch 80/100\n",
      "4687/4687 [==============================] - 3s 591us/step - loss: 0.2873 - accuracy: 0.8803\n",
      "Epoch 81/100\n",
      "4687/4687 [==============================] - 3s 639us/step - loss: 0.2861 - accuracy: 0.8807\n",
      "Epoch 82/100\n",
      "4687/4687 [==============================] - 3s 588us/step - loss: 0.2860 - accuracy: 0.8808\n",
      "Epoch 83/100\n",
      "4687/4687 [==============================] - 3s 587us/step - loss: 0.2854 - accuracy: 0.8805\n",
      "Epoch 84/100\n",
      "4687/4687 [==============================] - 3s 580us/step - loss: 0.2851 - accuracy: 0.8814\n",
      "Epoch 85/100\n",
      "4687/4687 [==============================] - 3s 577us/step - loss: 0.2841 - accuracy: 0.8811\n",
      "Epoch 86/100\n",
      "4687/4687 [==============================] - 3s 579us/step - loss: 0.2844 - accuracy: 0.8818\n",
      "Epoch 87/100\n",
      "4687/4687 [==============================] - 3s 577us/step - loss: 0.2835 - accuracy: 0.8818\n",
      "Epoch 88/100\n",
      "4687/4687 [==============================] - 3s 582us/step - loss: 0.2827 - accuracy: 0.8825\n",
      "Epoch 89/100\n",
      "4687/4687 [==============================] - 3s 575us/step - loss: 0.2817 - accuracy: 0.8830\n",
      "Epoch 90/100\n",
      "4687/4687 [==============================] - 3s 575us/step - loss: 0.2823 - accuracy: 0.8831\n",
      "Epoch 91/100\n",
      "4687/4687 [==============================] - 3s 581us/step - loss: 0.2808 - accuracy: 0.8838\n",
      "Epoch 92/100\n",
      "4687/4687 [==============================] - 3s 578us/step - loss: 0.2807 - accuracy: 0.8834\n",
      "Epoch 93/100\n",
      "4687/4687 [==============================] - 3s 562us/step - loss: 0.2807 - accuracy: 0.8834\n",
      "Epoch 94/100\n",
      "4687/4687 [==============================] - 3s 575us/step - loss: 0.2801 - accuracy: 0.8842\n",
      "Epoch 95/100\n",
      "4687/4687 [==============================] - 3s 579us/step - loss: 0.2798 - accuracy: 0.8839\n",
      "Epoch 96/100\n",
      "4687/4687 [==============================] - 3s 590us/step - loss: 0.2797 - accuracy: 0.8845\n",
      "Epoch 97/100\n",
      "4687/4687 [==============================] - 3s 595us/step - loss: 0.2786 - accuracy: 0.8846\n",
      "Epoch 98/100\n",
      "4687/4687 [==============================] - 3s 592us/step - loss: 0.2785 - accuracy: 0.8851\n",
      "Epoch 99/100\n",
      "4687/4687 [==============================] - 3s 601us/step - loss: 0.2786 - accuracy: 0.8851\n",
      "Epoch 100/100\n",
      "4687/4687 [==============================] - 3s 645us/step - loss: 0.2771 - accuracy: 0.8857\n",
      "888/888 - 0s - loss: 0.5580 - accuracy: 0.7821 - 460ms/epoch - 518us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 4: Same as above, but scaling only applied to the columns that are NOT either binary or already between 0 and 1\n",
    "# Create a dictionary for iteration 4\n",
    "iteration_4 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_4[\"Iteration\"] = 4\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler4 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_4 = scaler4.fit(X_train[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_4 = X_train\n",
    "X_train_scaled_4[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_4.transform(X_train[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "X_test_scaled_4 = X_test\n",
    "X_test_scaled_4[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_4.transform(X_test[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_4 = RandomOverSampler(random_state=1)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_4, y_ros_4 = ros_4.fit_resample(X_train_scaled_4, y_train)\n",
    "\n",
    "# Count the distinct values of the resampled labels data\n",
    "Counter(y_ros_4)\n",
    "\n",
    "# Define the model\n",
    "nn_4 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_4.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_4.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_4 = nn_4.fit(X_ros_4, y_ros_4, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_4, model_accuracy_4 = nn_4.evaluate(X_test_scaled_4, y_test, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_4 dictionary\n",
    "iteration_4[\"Loss\"] = model_loss_4\n",
    "iteration_4[\"Accuracy\"] = model_accuracy_4\n",
    "\n",
    "# Add the iteration_4 dictionary to the iterations list\n",
    "iterations.append(iteration_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 01m 05s]\n",
      "val_accuracy: 0.774587869644165\n",
      "\n",
      "Best val_accuracy So Far: 0.811082124710083\n",
      "Total elapsed time: 00h 27m 17s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "### Iteration 5: Same as above, but with hyperparameter tuning to find best parameters\n",
    "# Create a dictionary for iteration 5\n",
    "iteration_5 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_5[\"Iteration\"] = 5\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler5 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_5 = scaler4.fit(X_train[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_5 = X_train\n",
    "X_train_scaled_5[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_5.transform(X_train[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "X_test_scaled_5 = X_test\n",
    "X_test_scaled_5[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_5.transform(X_test[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_5 = RandomOverSampler(random_state=1)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_5, y_ros_5 = ros_5.fit_resample(X_train_scaled_5, y_train)\n",
    "\n",
    "# Count the distinct values of the resampled labels data\n",
    "Counter(y_ros_5)\n",
    "\n",
    "# Create a method that creates a Sequential model with hyperparameter tuning\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Determine the activation functions for each layer\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "\n",
    "    # Determine neurons in the first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units', \n",
    "        min_value=80, \n",
    "        max_value=120, \n",
    "        step=20), \n",
    "        activation=activation, input_dim=42))\n",
    "    \n",
    "    # Determine the number of hidden layers and neurons in them\n",
    "    for i in range(hp.Int('num_layers', 1, 4)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_'+str(i),\n",
    "            min_value=50, \n",
    "            max_value=90,\n",
    "            step=20),\n",
    "            activation=activation))\n",
    "    \n",
    "    # Set up the output layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return nn_model\n",
    "\n",
    "# Set up the kerastuner instance\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\", \n",
    "    max_epochs=20, \n",
    "    hyperband_iterations=2,\n",
    "    overwrite=True)\n",
    "\n",
    "# Run the kerastuner\n",
    "tuner.search(X_ros_5, y_ros_5, epochs=20, validation_data=(X_test_scaled_5, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'first_units': 100, 'num_layers': 2, 'units_0': 90, 'units_1': 90, 'units_2': 90, 'units_3': 90, 'tuner/epochs': 20, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               4300      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 90)                9090      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 90)                8190      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 91        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,671\n",
      "Trainable params: 21,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4687/4687 [==============================] - 3s 639us/step - loss: 0.5194 - accuracy: 0.7333\n",
      "Epoch 2/100\n",
      "4687/4687 [==============================] - 3s 633us/step - loss: 0.4537 - accuracy: 0.7793\n",
      "Epoch 3/100\n",
      "4687/4687 [==============================] - 3s 640us/step - loss: 0.4308 - accuracy: 0.7920\n",
      "Epoch 4/100\n",
      "4687/4687 [==============================] - 3s 637us/step - loss: 0.4113 - accuracy: 0.8052\n",
      "Epoch 5/100\n",
      "4687/4687 [==============================] - 3s 644us/step - loss: 0.3950 - accuracy: 0.8139\n",
      "Epoch 6/100\n",
      "4687/4687 [==============================] - 3s 644us/step - loss: 0.3790 - accuracy: 0.8239\n",
      "Epoch 7/100\n",
      "4687/4687 [==============================] - 3s 640us/step - loss: 0.3641 - accuracy: 0.8326\n",
      "Epoch 8/100\n",
      "4687/4687 [==============================] - 3s 643us/step - loss: 0.3508 - accuracy: 0.8406\n",
      "Epoch 9/100\n",
      "4687/4687 [==============================] - 3s 650us/step - loss: 0.3384 - accuracy: 0.8476\n",
      "Epoch 10/100\n",
      "4687/4687 [==============================] - 3s 643us/step - loss: 0.3263 - accuracy: 0.8548\n",
      "Epoch 11/100\n",
      "4687/4687 [==============================] - 3s 635us/step - loss: 0.3168 - accuracy: 0.8611\n",
      "Epoch 12/100\n",
      "4687/4687 [==============================] - 3s 631us/step - loss: 0.3069 - accuracy: 0.8654\n",
      "Epoch 13/100\n",
      "4687/4687 [==============================] - 3s 631us/step - loss: 0.2967 - accuracy: 0.8717\n",
      "Epoch 14/100\n",
      "4687/4687 [==============================] - 3s 630us/step - loss: 0.2882 - accuracy: 0.8767\n",
      "Epoch 15/100\n",
      "4687/4687 [==============================] - 3s 639us/step - loss: 0.2806 - accuracy: 0.8807\n",
      "Epoch 16/100\n",
      "4687/4687 [==============================] - 3s 646us/step - loss: 0.2732 - accuracy: 0.8838\n",
      "Epoch 17/100\n",
      "4687/4687 [==============================] - 3s 636us/step - loss: 0.2668 - accuracy: 0.8881\n",
      "Epoch 18/100\n",
      "4687/4687 [==============================] - 3s 641us/step - loss: 0.2603 - accuracy: 0.8913\n",
      "Epoch 19/100\n",
      "4687/4687 [==============================] - 3s 632us/step - loss: 0.2536 - accuracy: 0.8950\n",
      "Epoch 20/100\n",
      "4687/4687 [==============================] - 3s 635us/step - loss: 0.2482 - accuracy: 0.8972\n",
      "Epoch 21/100\n",
      "4687/4687 [==============================] - 3s 635us/step - loss: 0.2422 - accuracy: 0.9010\n",
      "Epoch 22/100\n",
      "4687/4687 [==============================] - 3s 630us/step - loss: 0.2362 - accuracy: 0.9030\n",
      "Epoch 23/100\n",
      "4687/4687 [==============================] - 3s 638us/step - loss: 0.2321 - accuracy: 0.9058\n",
      "Epoch 24/100\n",
      "4687/4687 [==============================] - 3s 652us/step - loss: 0.2280 - accuracy: 0.9069\n",
      "Epoch 25/100\n",
      "4687/4687 [==============================] - 3s 645us/step - loss: 0.2237 - accuracy: 0.9092\n",
      "Epoch 26/100\n",
      "4687/4687 [==============================] - 3s 647us/step - loss: 0.2201 - accuracy: 0.9112\n",
      "Epoch 27/100\n",
      "4687/4687 [==============================] - 3s 644us/step - loss: 0.2161 - accuracy: 0.9134\n",
      "Epoch 28/100\n",
      "4687/4687 [==============================] - 3s 643us/step - loss: 0.2127 - accuracy: 0.9154\n",
      "Epoch 29/100\n",
      "4687/4687 [==============================] - 3s 642us/step - loss: 0.2072 - accuracy: 0.9175\n",
      "Epoch 30/100\n",
      "4687/4687 [==============================] - 3s 635us/step - loss: 0.2044 - accuracy: 0.9189\n",
      "Epoch 31/100\n",
      "4687/4687 [==============================] - 3s 632us/step - loss: 0.2015 - accuracy: 0.9193\n",
      "Epoch 32/100\n",
      "4687/4687 [==============================] - 3s 637us/step - loss: 0.1978 - accuracy: 0.9215\n",
      "Epoch 33/100\n",
      "4687/4687 [==============================] - 3s 636us/step - loss: 0.1942 - accuracy: 0.9239\n",
      "Epoch 34/100\n",
      "4687/4687 [==============================] - 3s 648us/step - loss: 0.1929 - accuracy: 0.9240\n",
      "Epoch 35/100\n",
      "4687/4687 [==============================] - 3s 639us/step - loss: 0.1900 - accuracy: 0.9252\n",
      "Epoch 36/100\n",
      "4687/4687 [==============================] - 3s 631us/step - loss: 0.1866 - accuracy: 0.9267\n",
      "Epoch 37/100\n",
      "4687/4687 [==============================] - 3s 647us/step - loss: 0.1845 - accuracy: 0.9280\n",
      "Epoch 38/100\n",
      "4687/4687 [==============================] - 3s 647us/step - loss: 0.1817 - accuracy: 0.9285\n",
      "Epoch 39/100\n",
      "4687/4687 [==============================] - 3s 645us/step - loss: 0.1793 - accuracy: 0.9303\n",
      "Epoch 40/100\n",
      "4687/4687 [==============================] - 3s 637us/step - loss: 0.1765 - accuracy: 0.9315\n",
      "Epoch 41/100\n",
      "4687/4687 [==============================] - 3s 643us/step - loss: 0.1749 - accuracy: 0.9319\n",
      "Epoch 42/100\n",
      "4687/4687 [==============================] - 3s 630us/step - loss: 0.1713 - accuracy: 0.9338\n",
      "Epoch 43/100\n",
      "4687/4687 [==============================] - 3s 631us/step - loss: 0.1705 - accuracy: 0.9346\n",
      "Epoch 44/100\n",
      "4687/4687 [==============================] - 3s 653us/step - loss: 0.1678 - accuracy: 0.9353\n",
      "Epoch 45/100\n",
      "4687/4687 [==============================] - 3s 650us/step - loss: 0.1667 - accuracy: 0.9359\n",
      "Epoch 46/100\n",
      "4687/4687 [==============================] - 3s 649us/step - loss: 0.1653 - accuracy: 0.9368\n",
      "Epoch 47/100\n",
      "4687/4687 [==============================] - 3s 646us/step - loss: 0.1633 - accuracy: 0.9378\n",
      "Epoch 48/100\n",
      "4687/4687 [==============================] - 3s 646us/step - loss: 0.1611 - accuracy: 0.9382\n",
      "Epoch 49/100\n",
      "4687/4687 [==============================] - 3s 642us/step - loss: 0.1591 - accuracy: 0.9395\n",
      "Epoch 50/100\n",
      "4687/4687 [==============================] - 3s 645us/step - loss: 0.1576 - accuracy: 0.9396\n",
      "Epoch 51/100\n",
      "4687/4687 [==============================] - 3s 641us/step - loss: 0.1558 - accuracy: 0.9403\n",
      "Epoch 52/100\n",
      "4687/4687 [==============================] - 3s 643us/step - loss: 0.1553 - accuracy: 0.9404\n",
      "Epoch 53/100\n",
      "4687/4687 [==============================] - 3s 634us/step - loss: 0.1531 - accuracy: 0.9419\n",
      "Epoch 54/100\n",
      "4687/4687 [==============================] - 3s 630us/step - loss: 0.1514 - accuracy: 0.9424\n",
      "Epoch 55/100\n",
      "4687/4687 [==============================] - 3s 638us/step - loss: 0.1521 - accuracy: 0.9429\n",
      "Epoch 56/100\n",
      "4687/4687 [==============================] - 3s 633us/step - loss: 0.1510 - accuracy: 0.9430\n",
      "Epoch 57/100\n",
      "4687/4687 [==============================] - 3s 642us/step - loss: 0.1490 - accuracy: 0.9438\n",
      "Epoch 58/100\n",
      "4687/4687 [==============================] - 3s 637us/step - loss: 0.1473 - accuracy: 0.9446\n",
      "Epoch 59/100\n",
      "4687/4687 [==============================] - 3s 647us/step - loss: 0.1452 - accuracy: 0.9456\n",
      "Epoch 60/100\n",
      "4687/4687 [==============================] - 3s 635us/step - loss: 0.1455 - accuracy: 0.9447\n",
      "Epoch 61/100\n",
      "4687/4687 [==============================] - 3s 632us/step - loss: 0.1434 - accuracy: 0.9462\n",
      "Epoch 62/100\n",
      "4687/4687 [==============================] - 3s 635us/step - loss: 0.1428 - accuracy: 0.9466\n",
      "Epoch 63/100\n",
      "4687/4687 [==============================] - 3s 638us/step - loss: 0.1401 - accuracy: 0.9475\n",
      "Epoch 64/100\n",
      "4687/4687 [==============================] - 3s 638us/step - loss: 0.1400 - accuracy: 0.9473\n",
      "Epoch 65/100\n",
      "4687/4687 [==============================] - 3s 643us/step - loss: 0.1390 - accuracy: 0.9482\n",
      "Epoch 66/100\n",
      "4687/4687 [==============================] - 3s 638us/step - loss: 0.1370 - accuracy: 0.9484\n",
      "Epoch 67/100\n",
      "4687/4687 [==============================] - 3s 632us/step - loss: 0.1370 - accuracy: 0.9490\n",
      "Epoch 68/100\n",
      "4687/4687 [==============================] - 3s 636us/step - loss: 0.1360 - accuracy: 0.9493\n",
      "Epoch 69/100\n",
      "4687/4687 [==============================] - 3s 628us/step - loss: 0.1353 - accuracy: 0.9496\n",
      "Epoch 70/100\n",
      "4687/4687 [==============================] - 3s 637us/step - loss: 0.1342 - accuracy: 0.9499\n",
      "Epoch 71/100\n",
      "4687/4687 [==============================] - 3s 646us/step - loss: 0.1329 - accuracy: 0.9505\n",
      "Epoch 72/100\n",
      "4687/4687 [==============================] - 3s 638us/step - loss: 0.1325 - accuracy: 0.9505\n",
      "Epoch 73/100\n",
      "4687/4687 [==============================] - 3s 637us/step - loss: 0.1305 - accuracy: 0.9519\n",
      "Epoch 74/100\n",
      "4687/4687 [==============================] - 3s 633us/step - loss: 0.1312 - accuracy: 0.9515\n",
      "Epoch 75/100\n",
      "4687/4687 [==============================] - 3s 636us/step - loss: 0.1303 - accuracy: 0.9518\n",
      "Epoch 76/100\n",
      "4687/4687 [==============================] - 3s 638us/step - loss: 0.1276 - accuracy: 0.9529\n",
      "Epoch 77/100\n",
      "4687/4687 [==============================] - 3s 631us/step - loss: 0.1295 - accuracy: 0.9519\n",
      "Epoch 78/100\n",
      "4687/4687 [==============================] - 3s 643us/step - loss: 0.1274 - accuracy: 0.9528\n",
      "Epoch 79/100\n",
      "4687/4687 [==============================] - 3s 644us/step - loss: 0.1258 - accuracy: 0.9538\n",
      "Epoch 80/100\n",
      "4687/4687 [==============================] - 3s 639us/step - loss: 0.1260 - accuracy: 0.9536\n",
      "Epoch 81/100\n",
      "4687/4687 [==============================] - 3s 643us/step - loss: 0.1254 - accuracy: 0.9536\n",
      "Epoch 82/100\n",
      "4687/4687 [==============================] - 3s 634us/step - loss: 0.1239 - accuracy: 0.9549\n",
      "Epoch 83/100\n",
      "4687/4687 [==============================] - 3s 636us/step - loss: 0.1236 - accuracy: 0.9548\n",
      "Epoch 84/100\n",
      "4687/4687 [==============================] - 3s 630us/step - loss: 0.1225 - accuracy: 0.9547\n",
      "Epoch 85/100\n",
      "4687/4687 [==============================] - 3s 627us/step - loss: 0.1212 - accuracy: 0.9552\n",
      "Epoch 86/100\n",
      "4687/4687 [==============================] - 3s 640us/step - loss: 0.1207 - accuracy: 0.9558\n",
      "Epoch 87/100\n",
      "4687/4687 [==============================] - 3s 641us/step - loss: 0.1204 - accuracy: 0.9554\n",
      "Epoch 88/100\n",
      "4687/4687 [==============================] - 3s 643us/step - loss: 0.1203 - accuracy: 0.9555\n",
      "Epoch 89/100\n",
      "4687/4687 [==============================] - 3s 636us/step - loss: 0.1191 - accuracy: 0.9560\n",
      "Epoch 90/100\n",
      "4687/4687 [==============================] - 3s 632us/step - loss: 0.1198 - accuracy: 0.9561\n",
      "Epoch 91/100\n",
      "4687/4687 [==============================] - 3s 628us/step - loss: 0.1179 - accuracy: 0.9569\n",
      "Epoch 92/100\n",
      "4687/4687 [==============================] - 3s 627us/step - loss: 0.1178 - accuracy: 0.9568\n",
      "Epoch 93/100\n",
      "4687/4687 [==============================] - 3s 633us/step - loss: 0.1165 - accuracy: 0.9576\n",
      "Epoch 94/100\n",
      "4687/4687 [==============================] - 3s 636us/step - loss: 0.1165 - accuracy: 0.9573\n",
      "Epoch 95/100\n",
      "4687/4687 [==============================] - 3s 637us/step - loss: 0.1161 - accuracy: 0.9576\n",
      "Epoch 96/100\n",
      "4687/4687 [==============================] - 3s 638us/step - loss: 0.1144 - accuracy: 0.9584\n",
      "Epoch 97/100\n",
      "4687/4687 [==============================] - 3s 636us/step - loss: 0.1144 - accuracy: 0.9578\n",
      "Epoch 98/100\n",
      "4687/4687 [==============================] - 3s 645us/step - loss: 0.1152 - accuracy: 0.9585\n",
      "Epoch 99/100\n",
      "4687/4687 [==============================] - 3s 634us/step - loss: 0.1135 - accuracy: 0.9589\n",
      "Epoch 100/100\n",
      "4687/4687 [==============================] - 3s 628us/step - loss: 0.1132 - accuracy: 0.9586\n",
      "888/888 - 0s - loss: 1.1382 - accuracy: 0.8312 - 409ms/epoch - 460us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 5 continued\n",
    "# Get the best model and show its parameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)\n",
    "for params in best_hyper:\n",
    "    print(params.values)\n",
    "\n",
    "# Get a summary of the best model\n",
    "best_model = tuner.get_best_models()\n",
    "best_model[0].summary()\n",
    "\n",
    "# Build the best model and train it on the data\n",
    "model = tuner.hypermodel.build(best_hyper[0])\n",
    "model.fit(X_ros_5, y_ros_5, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_5, model_accuracy_5 = model.evaluate(X_test_scaled_5, y_test, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_5 dictionary\n",
    "iteration_5[\"Loss\"] = model_loss_5\n",
    "iteration_5[\"Accuracy\"] = model_accuracy_5\n",
    "\n",
    "# Add the iteration_5 dictionary to the iterations list\n",
    "iterations.append(iteration_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss: 0.35030612349510193, Accuracy: 0.870543897151947\n",
      "Iteration 2: Loss: 0.3182514011859894, Accuracy: 0.8752289414405823\n",
      "Iteration 3: Loss: 0.5668026804924011, Accuracy: 0.774587869644165\n",
      "Iteration 4: Loss: 0.5580213069915771, Accuracy: 0.7820557951927185\n",
      "Iteration 5: Loss: 1.138231873512268, Accuracy: 0.8311963081359863\n"
     ]
    }
   ],
   "source": [
    "# Print Final Statistics:\n",
    "for i in range(len(iterations)):\n",
    "    print(f\"Iteration {i+1}: Loss: {iterations[i]['Loss']}, Accuracy: {iterations[i]['Accuracy']}\")\n",
    "\n",
    "# Add final statistics to a dataframe and output to a csv\n",
    "stats_df = pd.DataFrame(iterations)\n",
    "stats_df.to_csv(\"summary_stats/binary_popularity_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
