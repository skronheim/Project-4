{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import findspark\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark_dist_explore import hist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from ann_visualizer.visualize import ann_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize findspark\n",
    "findspark.init()\n",
    "\n",
    "# Initialize the spark session\n",
    "spark = SparkSession.builder.appName(\"SK_categorical_nn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|   primary_artist|          track_name|popularity|explicit|danceability|energy|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|      duration_min|time_signature_0|time_signature_1|time_signature_3|time_signature_4|time_signature_5|key_0|key_1|key_2|key_3|key_4|key_5|key_6|key_7|key_8|key_9|key_10|key_11|num_artists_binned_1|num_artists_binned_2|num_artists_binned_3|num_artists_binned_4|num_artists_binned_5|num_artists_binned_6|track_genre_0|track_genre_1|track_genre_2|track_genre_3|track_genre_4|track_genre_5|track_genre_6|\n",
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|      Gen Hoshino|              Comedy|        73|       0|       0.676| 0.461|  -6.746|   0|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|3.8444333333333334|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Ben Woodward|    Ghost - Acoustic|        55|       0|        0.42| 0.166| -17.235|   1|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|            2.4935|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|Ingrid Michaelson|      To Begin Again|        57|       0|       0.438| 0.359|  -9.734|   1|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|3.5137666666666667|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Kina Grannis|Can't Help Fallin...|        71|       0|       0.266|0.0596| -18.515|   1|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|           3.36555|               0|               0|               1|               0|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "| Chord Overstreet|             Hold On|        82|       0|       0.618| 0.443|  -9.681|   1|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949| 3.314216666666667|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Tyrone Wells|Days I Will Remember|        58|       0|       0.688| 0.481|  -8.807|   1|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017| 3.570666666666667|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|A Great Big World|       Say Something|        74|       0|       0.407| 0.147|  -8.822|   1|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|3.8233333333333333|               0|               0|               1|               0|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|           I'm Yours|        80|       0|       0.703| 0.444|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|            4.0491|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     1|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|               Lucky|        74|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|   Ross Copperman|              Hunger|        56|       0|       0.442| 0.632|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|3.4265666666666665|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Zack Tabudlo|Give Me Your Forever|        74|       0|       0.627| 0.363|  -8.127|   1|     0.0291|       0.279|             0.0|  0.0928|  0.301| 99.905|              4.08|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|     I Won't Give Up|        69|       0|       0.483| 0.303| -10.058|   1|     0.0429|       0.694|             0.0|   0.115|  0.139|133.406|           4.00275|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|         Dan Berk|                Solo|        52|       0|       0.489| 0.314|  -9.245|   0|     0.0331|       0.749|             0.0|   0.113|  0.607|124.234|3.3118666666666665|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|    Anna Hamilton|            Bad Liar|        62|       0|       0.691| 0.234|  -6.441|   1|     0.0285|       0.777|             0.0|    0.12|  0.209| 87.103|            4.1408|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "| Chord Overstreet|     Hold On - Remix|        56|       0|       0.755|  0.78|  -6.084|   1|     0.0327|       0.124|         2.83E-5|   0.121|  0.387|120.004|           3.13555|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|      Landon Pigg|Falling in Love a...|        58|       0|       0.489| 0.561|  -7.933|   1|     0.0274|         0.2|         4.56E-5|   0.179|  0.238| 83.457|            4.0831|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Andrew Foy|ily (i love you b...|        56|       0|       0.706| 0.112| -18.098|   1|     0.0391|       0.827|         4.03E-6|   0.125|  0.414|110.154|            2.1625|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Andrew Foy|         At My Worst|        54|       0|       0.795|0.0841|  -18.09|   0|     0.0461|       0.742|         1.17E-5|  0.0853|  0.609| 91.803|            2.8288|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     1|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|               Lucky|        68|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Boyce Avenue|          Photograph|        67|       0|       0.717|  0.32|  -8.393|   1|     0.0283|        0.83|             0.0|   0.107|  0.322|107.946| 4.336433333333333|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the pre-processed data\n",
    "s_df = spark.read.csv(\"../Resources/filtered_encoded_dataset.csv\", sep=\",\", header=True, inferSchema=True)\n",
    "s_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|popularity|explicit|danceability|energy|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|      duration_min|time_signature_0|time_signature_1|time_signature_3|time_signature_4|time_signature_5|key_0|key_1|key_2|key_3|key_4|key_5|key_6|key_7|key_8|key_9|key_10|key_11|num_artists_binned_1|num_artists_binned_2|num_artists_binned_3|num_artists_binned_4|num_artists_binned_5|num_artists_binned_6|track_genre_0|track_genre_1|track_genre_2|track_genre_3|track_genre_4|track_genre_5|track_genre_6|\n",
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|        73|       0|       0.676| 0.461|  -6.746|   0|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|3.8444333333333334|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        55|       0|        0.42| 0.166| -17.235|   1|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|            2.4935|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        57|       0|       0.438| 0.359|  -9.734|   1|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|3.5137666666666667|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        71|       0|       0.266|0.0596| -18.515|   1|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|           3.36555|               0|               0|               1|               0|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        82|       0|       0.618| 0.443|  -9.681|   1|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949| 3.314216666666667|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        58|       0|       0.688| 0.481|  -8.807|   1|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017| 3.570666666666667|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.407| 0.147|  -8.822|   1|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|3.8233333333333333|               0|               0|               1|               0|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        80|       0|       0.703| 0.444|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|            4.0491|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     1|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.442| 0.632|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|3.4265666666666665|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.627| 0.363|  -8.127|   1|     0.0291|       0.279|             0.0|  0.0928|  0.301| 99.905|              4.08|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        69|       0|       0.483| 0.303| -10.058|   1|     0.0429|       0.694|             0.0|   0.115|  0.139|133.406|           4.00275|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        52|       0|       0.489| 0.314|  -9.245|   0|     0.0331|       0.749|             0.0|   0.113|  0.607|124.234|3.3118666666666665|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        62|       0|       0.691| 0.234|  -6.441|   1|     0.0285|       0.777|             0.0|    0.12|  0.209| 87.103|            4.1408|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.755|  0.78|  -6.084|   1|     0.0327|       0.124|         2.83E-5|   0.121|  0.387|120.004|           3.13555|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        58|       0|       0.489| 0.561|  -7.933|   1|     0.0274|         0.2|         4.56E-5|   0.179|  0.238| 83.457|            4.0831|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.706| 0.112| -18.098|   1|     0.0391|       0.827|         4.03E-6|   0.125|  0.414|110.154|            2.1625|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        54|       0|       0.795|0.0841|  -18.09|   0|     0.0461|       0.742|         1.17E-5|  0.0853|  0.609| 91.803|            2.8288|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     1|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        68|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        67|       0|       0.717|  0.32|  -8.393|   1|     0.0283|        0.83|             0.0|   0.107|  0.322|107.946| 4.336433333333333|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove primary_artist and track_name from the dataset\n",
    "s_df = s_df.drop(\"primary_artist\", \"track_name\")\n",
    "s_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([22457.,  9279., 18354., 14910., 19205., 14532.,  9344.,  4269.,\n",
       "         1101.,    98.]),\n",
       " array([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check popularity values\n",
    "fig, ax = plt.subplots()\n",
    "hist(ax, s_df.select(\"popularity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>...</th>\n",
       "      <th>num_artists_binned_4</th>\n",
       "      <th>num_artists_binned_5</th>\n",
       "      <th>num_artists_binned_6</th>\n",
       "      <th>track_genre_0</th>\n",
       "      <th>track_genre_1</th>\n",
       "      <th>track_genre_2</th>\n",
       "      <th>track_genre_3</th>\n",
       "      <th>track_genre_4</th>\n",
       "      <th>track_genre_5</th>\n",
       "      <th>track_genre_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   popularity  explicit  danceability  energy  loudness  mode  speechiness   \n",
       "0          73         0         0.676  0.4610    -6.746     0       0.1430  \\\n",
       "1          55         0         0.420  0.1660   -17.235     1       0.0763   \n",
       "2          57         0         0.438  0.3590    -9.734     1       0.0557   \n",
       "3          71         0         0.266  0.0596   -18.515     1       0.0363   \n",
       "4          82         0         0.618  0.4430    -9.681     1       0.0526   \n",
       "\n",
       "   acousticness  instrumentalness  liveness  ...  num_artists_binned_4   \n",
       "0        0.0322          0.000001    0.3580  ...                     0  \\\n",
       "1        0.9240          0.000006    0.1010  ...                     0   \n",
       "2        0.2100          0.000000    0.1170  ...                     0   \n",
       "3        0.9050          0.000071    0.1320  ...                     0   \n",
       "4        0.4690          0.000000    0.0829  ...                     0   \n",
       "\n",
       "   num_artists_binned_5  num_artists_binned_6  track_genre_0  track_genre_1   \n",
       "0                     0                     0              0              0  \\\n",
       "1                     0                     0              0              0   \n",
       "2                     0                     0              0              0   \n",
       "3                     0                     0              0              0   \n",
       "4                     0                     0              0              0   \n",
       "\n",
       "   track_genre_2  track_genre_3  track_genre_4  track_genre_5  track_genre_6  \n",
       "0              0              0              0              0              1  \n",
       "1              0              0              0              0              1  \n",
       "2              0              0              0              0              1  \n",
       "3              0              0              0              0              1  \n",
       "4              0              0              0              0              1  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data to a pandas Dataframe\n",
    "df = s_df.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popularity\n",
       "0      15843\n",
       "1       2116\n",
       "2       1025\n",
       "3        570\n",
       "4        377\n",
       "       ...  \n",
       "96         7\n",
       "97         8\n",
       "98         7\n",
       "99         1\n",
       "100        2\n",
       "Name: count, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show value counts of popularity to help determine proper bins\n",
    "df['popularity'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity_4bins\n",
      "2    48631\n",
      "1    35514\n",
      "0    15843\n",
      "3    13561\n",
      "Name: count, dtype: int64\n",
      "popularity_3bins\n",
      "0    51357\n",
      "1    48631\n",
      "2    13561\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Bin the popularity into: 0, low=1 (0<pop<=30), medium=2 (30<pop<=60), high=3 (60<pop)\n",
    "# Set up a list of bins\n",
    "pop_4bins = [0 ,1, 2, 3]\n",
    "# Set up list of conditions\n",
    "pop_4conditions = [\n",
    "    (df[\"popularity\"] == 0), \n",
    "    (df[\"popularity\"] > 0) & (df[\"popularity\"] <= 30),\n",
    "    (df[\"popularity\"] > 30) & (df[\"popularity\"] <= 60),\n",
    "    (df[\"popularity\"] > 60)\n",
    "]\n",
    "# Set up the column with bins\n",
    "df[\"popularity_4bins\"] = np.select(pop_4conditions, pop_4bins)\n",
    "\n",
    "# Set up a list of 3 bins: low=0 (pop<=30), medium=1 (30<pop<=60), high=2 (60<pop)\n",
    "pop_3bins = [0, 1, 2]\n",
    "# Set up a list of conditions\n",
    "pop_3conditions = [\n",
    "    (df[\"popularity\"] <= 30),\n",
    "    (df[\"popularity\"] > 30) & (df[\"popularity\"] <= 60),\n",
    "    (df[\"popularity\"] > 60)\n",
    "]\n",
    "# Set up the column with bins\n",
    "df[\"popularity_3bins\"] = np.select(pop_3conditions, pop_3bins)\n",
    "\n",
    "# Confirm binning\n",
    "print(df['popularity_4bins'].value_counts())\n",
    "print(df['popularity_3bins'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>...</th>\n",
       "      <th>num_artists_binned_6</th>\n",
       "      <th>track_genre_0</th>\n",
       "      <th>track_genre_1</th>\n",
       "      <th>track_genre_2</th>\n",
       "      <th>track_genre_3</th>\n",
       "      <th>track_genre_4</th>\n",
       "      <th>track_genre_5</th>\n",
       "      <th>track_genre_6</th>\n",
       "      <th>popularity_4bins</th>\n",
       "      <th>popularity_3bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   explicit  danceability  energy  loudness  mode  speechiness  acousticness   \n",
       "0         0         0.676  0.4610    -6.746     0       0.1430        0.0322  \\\n",
       "1         0         0.420  0.1660   -17.235     1       0.0763        0.9240   \n",
       "2         0         0.438  0.3590    -9.734     1       0.0557        0.2100   \n",
       "3         0         0.266  0.0596   -18.515     1       0.0363        0.9050   \n",
       "4         0         0.618  0.4430    -9.681     1       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence  ...  num_artists_binned_6   \n",
       "0          0.000001    0.3580    0.715  ...                     0  \\\n",
       "1          0.000006    0.1010    0.267  ...                     0   \n",
       "2          0.000000    0.1170    0.120  ...                     0   \n",
       "3          0.000071    0.1320    0.143  ...                     0   \n",
       "4          0.000000    0.0829    0.167  ...                     0   \n",
       "\n",
       "   track_genre_0  track_genre_1  track_genre_2  track_genre_3  track_genre_4   \n",
       "0              0              0              0              0              0  \\\n",
       "1              0              0              0              0              0   \n",
       "2              0              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "\n",
       "   track_genre_5  track_genre_6  popularity_4bins  popularity_3bins  \n",
       "0              0              1                 3                 2  \n",
       "1              0              1                 2                 1  \n",
       "2              0              1                 2                 1  \n",
       "3              0              1                 3                 2  \n",
       "4              0              1                 3                 2  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the original popularity column\n",
    "df.drop(columns=\"popularity\", inplace=True)\n",
    "# Check removal\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate training and testing for the 4-bin model\n",
    "# Separate the prediction (song popularity) from the rest of the features\n",
    "y_4b = df.popularity_4bins.values\n",
    "X_4b = df.drop(columns=[\"popularity_4bins\", 'popularity_3bins'])\n",
    "\n",
    "# Split the training and testing datasets\n",
    "X_train_4b, X_test_4b, y_train_4b, y_test_4b = train_test_split(X_4b, y_4b, random_state=1, stratify=y_4b)\n",
    "\n",
    "# Set up one-hot encoding to encode popularity\n",
    "y_train_4b = to_categorical(y_train_4b, 4)\n",
    "y_test_4b = to_categorical(y_test_4b, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate training and testing for the 3-bin model\n",
    "# Separate the prediction (song popularity) from the rest of the features\n",
    "y_3b = df.popularity_3bins.values\n",
    "X_3b = df.drop(columns=[\"popularity_4bins\", 'popularity_3bins'])\n",
    "\n",
    "# Split the training and testing datasets\n",
    "X_train_3b, X_test_3b, y_train_3b, y_test_3b = train_test_split(X_3b, y_3b, random_state=1, stratify=y_3b)\n",
    "\n",
    "# Set up one-hot encoding to encode popularity\n",
    "y_train_3b = to_categorical(y_train_3b, 3)\n",
    "y_test_3b = to_categorical(y_test_3b, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for dictionaries to hold the summary and accuracy for each iteration\n",
    "iterations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,694\n",
      "Trainable params: 7,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2662/2662 [==============================] - 2s 578us/step - loss: 1.0767 - accuracy: 0.5440\n",
      "Epoch 2/100\n",
      "2662/2662 [==============================] - 2s 571us/step - loss: 0.9126 - accuracy: 0.6319\n",
      "Epoch 3/100\n",
      "2662/2662 [==============================] - 2s 578us/step - loss: 0.8530 - accuracy: 0.6561\n",
      "Epoch 4/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.8268 - accuracy: 0.6658\n",
      "Epoch 5/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.8114 - accuracy: 0.6716\n",
      "Epoch 6/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.8020 - accuracy: 0.6737\n",
      "Epoch 7/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.7956 - accuracy: 0.6766\n",
      "Epoch 8/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.7891 - accuracy: 0.6792\n",
      "Epoch 9/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.7839 - accuracy: 0.6810\n",
      "Epoch 10/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.7789 - accuracy: 0.6828\n",
      "Epoch 11/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.7749 - accuracy: 0.6851\n",
      "Epoch 12/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.7726 - accuracy: 0.6857\n",
      "Epoch 13/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7688 - accuracy: 0.6866\n",
      "Epoch 14/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.7660 - accuracy: 0.6883\n",
      "Epoch 15/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.7627 - accuracy: 0.6898\n",
      "Epoch 16/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.7609 - accuracy: 0.6900\n",
      "Epoch 17/100\n",
      "2662/2662 [==============================] - 2s 577us/step - loss: 0.7575 - accuracy: 0.6911\n",
      "Epoch 18/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.7555 - accuracy: 0.6921\n",
      "Epoch 19/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.7536 - accuracy: 0.6931\n",
      "Epoch 20/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.7519 - accuracy: 0.6944\n",
      "Epoch 21/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.7502 - accuracy: 0.6941\n",
      "Epoch 22/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.7482 - accuracy: 0.6960\n",
      "Epoch 23/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.7466 - accuracy: 0.6969\n",
      "Epoch 24/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.7458 - accuracy: 0.6960\n",
      "Epoch 25/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.7438 - accuracy: 0.6980\n",
      "Epoch 26/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.7423 - accuracy: 0.6977\n",
      "Epoch 27/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.7414 - accuracy: 0.6983\n",
      "Epoch 28/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.7399 - accuracy: 0.6985\n",
      "Epoch 29/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.7384 - accuracy: 0.7001\n",
      "Epoch 30/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.7376 - accuracy: 0.6997\n",
      "Epoch 31/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.7356 - accuracy: 0.6999\n",
      "Epoch 32/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.7350 - accuracy: 0.6999\n",
      "Epoch 33/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.7336 - accuracy: 0.7015\n",
      "Epoch 34/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.7325 - accuracy: 0.7024\n",
      "Epoch 35/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7319 - accuracy: 0.7028\n",
      "Epoch 36/100\n",
      "2662/2662 [==============================] - 2s 575us/step - loss: 0.7304 - accuracy: 0.7021\n",
      "Epoch 37/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7297 - accuracy: 0.7046\n",
      "Epoch 38/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.7296 - accuracy: 0.7032\n",
      "Epoch 39/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.7288 - accuracy: 0.7032\n",
      "Epoch 40/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.7276 - accuracy: 0.7048\n",
      "Epoch 41/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7272 - accuracy: 0.7042\n",
      "Epoch 42/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.7261 - accuracy: 0.7046\n",
      "Epoch 43/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.7255 - accuracy: 0.7058\n",
      "Epoch 44/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.7245 - accuracy: 0.7062\n",
      "Epoch 45/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.7234 - accuracy: 0.7062\n",
      "Epoch 46/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.7229 - accuracy: 0.7056\n",
      "Epoch 47/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7228 - accuracy: 0.7063\n",
      "Epoch 48/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7220 - accuracy: 0.7059\n",
      "Epoch 49/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.7215 - accuracy: 0.7068\n",
      "Epoch 50/100\n",
      "2662/2662 [==============================] - 2s 576us/step - loss: 0.7211 - accuracy: 0.7054\n",
      "Epoch 51/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.7210 - accuracy: 0.7071\n",
      "Epoch 52/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.7194 - accuracy: 0.7055\n",
      "Epoch 53/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.7200 - accuracy: 0.7069\n",
      "Epoch 54/100\n",
      "2662/2662 [==============================] - 2s 569us/step - loss: 0.7188 - accuracy: 0.7066\n",
      "Epoch 55/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.7188 - accuracy: 0.7071\n",
      "Epoch 56/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.7177 - accuracy: 0.7064\n",
      "Epoch 57/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.7178 - accuracy: 0.7076\n",
      "Epoch 58/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7162 - accuracy: 0.7082\n",
      "Epoch 59/100\n",
      "2662/2662 [==============================] - 2s 569us/step - loss: 0.7162 - accuracy: 0.7068\n",
      "Epoch 60/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.7159 - accuracy: 0.7090\n",
      "Epoch 61/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.7147 - accuracy: 0.7077\n",
      "Epoch 62/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.7144 - accuracy: 0.7082\n",
      "Epoch 63/100\n",
      "2662/2662 [==============================] - 2s 574us/step - loss: 0.7141 - accuracy: 0.7092\n",
      "Epoch 64/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.7140 - accuracy: 0.7103\n",
      "Epoch 65/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.7139 - accuracy: 0.7086\n",
      "Epoch 66/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7130 - accuracy: 0.7092\n",
      "Epoch 67/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.7130 - accuracy: 0.7094\n",
      "Epoch 68/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.7121 - accuracy: 0.7091\n",
      "Epoch 69/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.7116 - accuracy: 0.7110\n",
      "Epoch 70/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.7123 - accuracy: 0.7097\n",
      "Epoch 71/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7118 - accuracy: 0.7098\n",
      "Epoch 72/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7105 - accuracy: 0.7098\n",
      "Epoch 73/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.7107 - accuracy: 0.7109\n",
      "Epoch 74/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.7104 - accuracy: 0.7120\n",
      "Epoch 75/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.7101 - accuracy: 0.7107\n",
      "Epoch 76/100\n",
      "2662/2662 [==============================] - 2s 563us/step - loss: 0.7096 - accuracy: 0.7108\n",
      "Epoch 77/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.7094 - accuracy: 0.7113\n",
      "Epoch 78/100\n",
      "2662/2662 [==============================] - 2s 580us/step - loss: 0.7091 - accuracy: 0.7112\n",
      "Epoch 79/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.7093 - accuracy: 0.7112\n",
      "Epoch 80/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.7084 - accuracy: 0.7117\n",
      "Epoch 81/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.7084 - accuracy: 0.7104\n",
      "Epoch 82/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.7080 - accuracy: 0.7116\n",
      "Epoch 83/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.7079 - accuracy: 0.7132\n",
      "Epoch 84/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7067 - accuracy: 0.7120\n",
      "Epoch 85/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7073 - accuracy: 0.7127\n",
      "Epoch 86/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7066 - accuracy: 0.7128\n",
      "Epoch 87/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7070 - accuracy: 0.7128\n",
      "Epoch 88/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.7069 - accuracy: 0.7125\n",
      "Epoch 89/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.7059 - accuracy: 0.7132\n",
      "Epoch 90/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.7057 - accuracy: 0.7133\n",
      "Epoch 91/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.7053 - accuracy: 0.7134\n",
      "Epoch 92/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7051 - accuracy: 0.7136\n",
      "Epoch 93/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7045 - accuracy: 0.7136\n",
      "Epoch 94/100\n",
      "2662/2662 [==============================] - 1s 548us/step - loss: 0.7060 - accuracy: 0.7129\n",
      "Epoch 95/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7047 - accuracy: 0.7130\n",
      "Epoch 96/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.7047 - accuracy: 0.7128\n",
      "Epoch 97/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.7047 - accuracy: 0.7139\n",
      "Epoch 98/100\n",
      "2662/2662 [==============================] - 2s 595us/step - loss: 0.7035 - accuracy: 0.7137\n",
      "Epoch 99/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.7036 - accuracy: 0.7141\n",
      "Epoch 100/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.7046 - accuracy: 0.7137\n",
      "888/888 - 0s - loss: 0.8347 - accuracy: 0.6675 - 466ms/epoch - 525us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 1: Standard Scaler normalization, 4 bins for popularity\n",
    "# Create a dictionary for iteration 1\n",
    "iteration_1 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_1[\"Iteration\"] = 1\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_1 = scaler1.fit(X_train_4b)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_1 = X_scaler_1.transform(X_train_4b)\n",
    "X_test_scaled_1 = X_scaler_1.transform(X_test_4b)\n",
    "\n",
    "# Define the model\n",
    "nn_1 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=4, activation=\"softmax\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_1.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_1 = nn_1.fit(X_train_scaled_1, y_train_4b, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_1, model_accuracy_1 = nn_1.evaluate(X_test_scaled_1, y_test_4b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_1 dictionary\n",
    "iteration_1[\"Loss\"] = model_loss_1\n",
    "iteration_1[\"Accuracy\"] = model_accuracy_1\n",
    "\n",
    "# Add the iteration_1 dictionary to the iterations list\n",
    "iterations.append(iteration_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,643\n",
      "Trainable params: 7,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2662/2662 [==============================] - 2s 586us/step - loss: 0.8546 - accuracy: 0.5995\n",
      "Epoch 2/100\n",
      "2662/2662 [==============================] - 2s 597us/step - loss: 0.7396 - accuracy: 0.6763\n",
      "Epoch 3/100\n",
      "2662/2662 [==============================] - 2s 589us/step - loss: 0.6933 - accuracy: 0.6980\n",
      "Epoch 4/100\n",
      "2662/2662 [==============================] - 2s 592us/step - loss: 0.6716 - accuracy: 0.7058\n",
      "Epoch 5/100\n",
      "2662/2662 [==============================] - 2s 628us/step - loss: 0.6603 - accuracy: 0.7106\n",
      "Epoch 6/100\n",
      "2662/2662 [==============================] - 2s 599us/step - loss: 0.6512 - accuracy: 0.7153\n",
      "Epoch 7/100\n",
      "2662/2662 [==============================] - 2s 599us/step - loss: 0.6447 - accuracy: 0.7182\n",
      "Epoch 8/100\n",
      "2662/2662 [==============================] - 2s 586us/step - loss: 0.6407 - accuracy: 0.7211\n",
      "Epoch 9/100\n",
      "2662/2662 [==============================] - 2s 595us/step - loss: 0.6355 - accuracy: 0.7232\n",
      "Epoch 10/100\n",
      "2662/2662 [==============================] - 2s 590us/step - loss: 0.6315 - accuracy: 0.7243\n",
      "Epoch 11/100\n",
      "2662/2662 [==============================] - 2s 595us/step - loss: 0.6280 - accuracy: 0.7265\n",
      "Epoch 12/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.6249 - accuracy: 0.7292\n",
      "Epoch 13/100\n",
      "2662/2662 [==============================] - 2s 595us/step - loss: 0.6219 - accuracy: 0.7296\n",
      "Epoch 14/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.6198 - accuracy: 0.7304\n",
      "Epoch 15/100\n",
      "2662/2662 [==============================] - 2s 586us/step - loss: 0.6174 - accuracy: 0.7308\n",
      "Epoch 16/100\n",
      "2662/2662 [==============================] - 2s 579us/step - loss: 0.6148 - accuracy: 0.7324\n",
      "Epoch 17/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.6133 - accuracy: 0.7326\n",
      "Epoch 18/100\n",
      "2662/2662 [==============================] - 2s 595us/step - loss: 0.6102 - accuracy: 0.7357\n",
      "Epoch 19/100\n",
      "2662/2662 [==============================] - 2s 589us/step - loss: 0.6098 - accuracy: 0.7350\n",
      "Epoch 20/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.6072 - accuracy: 0.7361\n",
      "Epoch 21/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.6056 - accuracy: 0.7362\n",
      "Epoch 22/100\n",
      "2662/2662 [==============================] - 2s 589us/step - loss: 0.6039 - accuracy: 0.7374\n",
      "Epoch 23/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.6024 - accuracy: 0.7370\n",
      "Epoch 24/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.6012 - accuracy: 0.7384\n",
      "Epoch 25/100\n",
      "2662/2662 [==============================] - 2s 586us/step - loss: 0.5994 - accuracy: 0.7409\n",
      "Epoch 26/100\n",
      "2662/2662 [==============================] - 2s 576us/step - loss: 0.5984 - accuracy: 0.7410\n",
      "Epoch 27/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5968 - accuracy: 0.7402\n",
      "Epoch 28/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5958 - accuracy: 0.7414\n",
      "Epoch 29/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5943 - accuracy: 0.7433\n",
      "Epoch 30/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.5938 - accuracy: 0.7432\n",
      "Epoch 31/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5920 - accuracy: 0.7425\n",
      "Epoch 32/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.5916 - accuracy: 0.7442\n",
      "Epoch 33/100\n",
      "2662/2662 [==============================] - 1s 550us/step - loss: 0.5908 - accuracy: 0.7457\n",
      "Epoch 34/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.5897 - accuracy: 0.7450\n",
      "Epoch 35/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5887 - accuracy: 0.7446\n",
      "Epoch 36/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.5879 - accuracy: 0.7461\n",
      "Epoch 37/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.5870 - accuracy: 0.7476\n",
      "Epoch 38/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5859 - accuracy: 0.7470\n",
      "Epoch 39/100\n",
      "2662/2662 [==============================] - 1s 550us/step - loss: 0.5850 - accuracy: 0.7490\n",
      "Epoch 40/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5848 - accuracy: 0.7475\n",
      "Epoch 41/100\n",
      "2662/2662 [==============================] - 1s 547us/step - loss: 0.5836 - accuracy: 0.7490\n",
      "Epoch 42/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5832 - accuracy: 0.7491\n",
      "Epoch 43/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5824 - accuracy: 0.7482\n",
      "Epoch 44/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5822 - accuracy: 0.7489\n",
      "Epoch 45/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.5807 - accuracy: 0.7501\n",
      "Epoch 46/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.5802 - accuracy: 0.7496\n",
      "Epoch 47/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.5794 - accuracy: 0.7498\n",
      "Epoch 48/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.5788 - accuracy: 0.7510\n",
      "Epoch 49/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.5781 - accuracy: 0.7509\n",
      "Epoch 50/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.5777 - accuracy: 0.7517\n",
      "Epoch 51/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.5767 - accuracy: 0.7520\n",
      "Epoch 52/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5760 - accuracy: 0.7530\n",
      "Epoch 53/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5747 - accuracy: 0.7531\n",
      "Epoch 54/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5750 - accuracy: 0.7533\n",
      "Epoch 55/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.5744 - accuracy: 0.7517\n",
      "Epoch 56/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5746 - accuracy: 0.7526\n",
      "Epoch 57/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5736 - accuracy: 0.7536\n",
      "Epoch 58/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5726 - accuracy: 0.7525\n",
      "Epoch 59/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5721 - accuracy: 0.7544\n",
      "Epoch 60/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5717 - accuracy: 0.7546\n",
      "Epoch 61/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5713 - accuracy: 0.7536\n",
      "Epoch 62/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5710 - accuracy: 0.7540\n",
      "Epoch 63/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5701 - accuracy: 0.7544\n",
      "Epoch 64/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5703 - accuracy: 0.7544\n",
      "Epoch 65/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5694 - accuracy: 0.7560\n",
      "Epoch 66/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5694 - accuracy: 0.7537\n",
      "Epoch 67/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.5688 - accuracy: 0.7547\n",
      "Epoch 68/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5683 - accuracy: 0.7566\n",
      "Epoch 69/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5676 - accuracy: 0.7561\n",
      "Epoch 70/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5679 - accuracy: 0.7560\n",
      "Epoch 71/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.5670 - accuracy: 0.7563\n",
      "Epoch 72/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5673 - accuracy: 0.7564\n",
      "Epoch 73/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.5661 - accuracy: 0.7572\n",
      "Epoch 74/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5656 - accuracy: 0.7564\n",
      "Epoch 75/100\n",
      "2662/2662 [==============================] - 1s 550us/step - loss: 0.5658 - accuracy: 0.7576\n",
      "Epoch 76/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5649 - accuracy: 0.7567\n",
      "Epoch 77/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.5646 - accuracy: 0.7573\n",
      "Epoch 78/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5638 - accuracy: 0.7586\n",
      "Epoch 79/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5640 - accuracy: 0.7583\n",
      "Epoch 80/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.5633 - accuracy: 0.7583\n",
      "Epoch 81/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.5634 - accuracy: 0.7577\n",
      "Epoch 82/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.5629 - accuracy: 0.7587\n",
      "Epoch 83/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5635 - accuracy: 0.7582\n",
      "Epoch 84/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.5626 - accuracy: 0.7587\n",
      "Epoch 85/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5619 - accuracy: 0.7592\n",
      "Epoch 86/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5625 - accuracy: 0.7592\n",
      "Epoch 87/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5616 - accuracy: 0.7583\n",
      "Epoch 88/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5617 - accuracy: 0.7588\n",
      "Epoch 89/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5607 - accuracy: 0.7578\n",
      "Epoch 90/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.5606 - accuracy: 0.7591\n",
      "Epoch 91/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5608 - accuracy: 0.7584\n",
      "Epoch 92/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.5600 - accuracy: 0.7588\n",
      "Epoch 93/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.5591 - accuracy: 0.7601\n",
      "Epoch 94/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.5596 - accuracy: 0.7598\n",
      "Epoch 95/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5591 - accuracy: 0.7600\n",
      "Epoch 96/100\n",
      "2662/2662 [==============================] - 1s 550us/step - loss: 0.5593 - accuracy: 0.7609\n",
      "Epoch 97/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.5587 - accuracy: 0.7610\n",
      "Epoch 98/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.5587 - accuracy: 0.7600\n",
      "Epoch 99/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5587 - accuracy: 0.7607\n",
      "Epoch 100/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5577 - accuracy: 0.7588\n",
      "888/888 - 0s - loss: 0.6881 - accuracy: 0.7082 - 415ms/epoch - 468us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 2: Standard Scaler normalization, 3 bins for popularity\n",
    "# Create a dictionary for iteration 2\n",
    "iteration_2 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_2[\"Iteration\"] = 2\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_2 = scaler2.fit(X_train_3b)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_2 = X_scaler_2.transform(X_train_3b)\n",
    "X_test_scaled_2 = X_scaler_2.transform(X_test_3b)\n",
    "\n",
    "# Define the model\n",
    "nn_2 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_2.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_2 = nn_2.fit(X_train_scaled_2, y_train_3b, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_2, model_accuracy_2 = nn_2.evaluate(X_test_scaled_2, y_test_3b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_2 dictionary\n",
    "iteration_2[\"Loss\"] = model_loss_2\n",
    "iteration_2[\"Accuracy\"] = model_accuracy_2\n",
    "\n",
    "# Add the iteration_2 dictionary to the iterations list\n",
    "iterations.append(iteration_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,643\n",
      "Trainable params: 7,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.8511 - accuracy: 0.6077\n",
      "Epoch 2/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.7299 - accuracy: 0.6815\n",
      "Epoch 3/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.6946 - accuracy: 0.6962\n",
      "Epoch 4/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.6797 - accuracy: 0.7027\n",
      "Epoch 5/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.6710 - accuracy: 0.7064\n",
      "Epoch 6/100\n",
      "2662/2662 [==============================] - 2s 583us/step - loss: 0.6650 - accuracy: 0.7085\n",
      "Epoch 7/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.6592 - accuracy: 0.7111\n",
      "Epoch 8/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.6548 - accuracy: 0.7129\n",
      "Epoch 9/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.6517 - accuracy: 0.7129\n",
      "Epoch 10/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.6483 - accuracy: 0.7160\n",
      "Epoch 11/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.6456 - accuracy: 0.7152\n",
      "Epoch 12/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.6433 - accuracy: 0.7172\n",
      "Epoch 13/100\n",
      "2662/2662 [==============================] - 2s 597us/step - loss: 0.6412 - accuracy: 0.7178\n",
      "Epoch 14/100\n",
      "2662/2662 [==============================] - 2s 597us/step - loss: 0.6389 - accuracy: 0.7186\n",
      "Epoch 15/100\n",
      "2662/2662 [==============================] - 2s 600us/step - loss: 0.6366 - accuracy: 0.7206\n",
      "Epoch 16/100\n",
      "2662/2662 [==============================] - 2s 597us/step - loss: 0.6349 - accuracy: 0.7202\n",
      "Epoch 17/100\n",
      "2662/2662 [==============================] - 2s 586us/step - loss: 0.6332 - accuracy: 0.7219\n",
      "Epoch 18/100\n",
      "2662/2662 [==============================] - 2s 579us/step - loss: 0.6311 - accuracy: 0.7243\n",
      "Epoch 19/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.6296 - accuracy: 0.7238\n",
      "Epoch 20/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.6273 - accuracy: 0.7241\n",
      "Epoch 21/100\n",
      "2662/2662 [==============================] - 2s 606us/step - loss: 0.6266 - accuracy: 0.7252\n",
      "Epoch 22/100\n",
      "2662/2662 [==============================] - 2s 595us/step - loss: 0.6249 - accuracy: 0.7258\n",
      "Epoch 23/100\n",
      "2662/2662 [==============================] - 2s 598us/step - loss: 0.6241 - accuracy: 0.7261\n",
      "Epoch 24/100\n",
      "2662/2662 [==============================] - 2s 591us/step - loss: 0.6218 - accuracy: 0.7276\n",
      "Epoch 25/100\n",
      "2662/2662 [==============================] - 2s 598us/step - loss: 0.6209 - accuracy: 0.7285\n",
      "Epoch 26/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.6201 - accuracy: 0.7275\n",
      "Epoch 27/100\n",
      "2662/2662 [==============================] - 2s 610us/step - loss: 0.6183 - accuracy: 0.7284\n",
      "Epoch 28/100\n",
      "2662/2662 [==============================] - 2s 604us/step - loss: 0.6174 - accuracy: 0.7283\n",
      "Epoch 29/100\n",
      "2662/2662 [==============================] - 2s 610us/step - loss: 0.6160 - accuracy: 0.7307\n",
      "Epoch 30/100\n",
      "2662/2662 [==============================] - 2s 596us/step - loss: 0.6149 - accuracy: 0.7311\n",
      "Epoch 31/100\n",
      "2662/2662 [==============================] - 2s 603us/step - loss: 0.6136 - accuracy: 0.7319\n",
      "Epoch 32/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.6129 - accuracy: 0.7325\n",
      "Epoch 33/100\n",
      "2662/2662 [==============================] - 2s 616us/step - loss: 0.6120 - accuracy: 0.7329\n",
      "Epoch 34/100\n",
      "2662/2662 [==============================] - 2s 583us/step - loss: 0.6108 - accuracy: 0.7321\n",
      "Epoch 35/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.6096 - accuracy: 0.7337\n",
      "Epoch 36/100\n",
      "2662/2662 [==============================] - 2s 596us/step - loss: 0.6087 - accuracy: 0.7345\n",
      "Epoch 37/100\n",
      "2662/2662 [==============================] - 2s 606us/step - loss: 0.6077 - accuracy: 0.7343\n",
      "Epoch 38/100\n",
      "2662/2662 [==============================] - 2s 599us/step - loss: 0.6066 - accuracy: 0.7344\n",
      "Epoch 39/100\n",
      "2662/2662 [==============================] - 2s 671us/step - loss: 0.6057 - accuracy: 0.7364\n",
      "Epoch 40/100\n",
      "2662/2662 [==============================] - 2s 657us/step - loss: 0.6052 - accuracy: 0.7352\n",
      "Epoch 41/100\n",
      "2662/2662 [==============================] - 2s 612us/step - loss: 0.6051 - accuracy: 0.7368\n",
      "Epoch 42/100\n",
      "2662/2662 [==============================] - 2s 619us/step - loss: 0.6039 - accuracy: 0.7376\n",
      "Epoch 43/100\n",
      "2662/2662 [==============================] - 2s 611us/step - loss: 0.6034 - accuracy: 0.7370\n",
      "Epoch 44/100\n",
      "2662/2662 [==============================] - 2s 671us/step - loss: 0.6025 - accuracy: 0.7375\n",
      "Epoch 45/100\n",
      "2662/2662 [==============================] - 2s 613us/step - loss: 0.6020 - accuracy: 0.7371\n",
      "Epoch 46/100\n",
      "2662/2662 [==============================] - 2s 606us/step - loss: 0.6008 - accuracy: 0.7366\n",
      "Epoch 47/100\n",
      "2662/2662 [==============================] - 2s 598us/step - loss: 0.6005 - accuracy: 0.7388\n",
      "Epoch 48/100\n",
      "2662/2662 [==============================] - 2s 576us/step - loss: 0.5999 - accuracy: 0.7383\n",
      "Epoch 49/100\n",
      "2662/2662 [==============================] - 2s 576us/step - loss: 0.5990 - accuracy: 0.7382\n",
      "Epoch 50/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.5980 - accuracy: 0.7394\n",
      "Epoch 51/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.5969 - accuracy: 0.7399\n",
      "Epoch 52/100\n",
      "2662/2662 [==============================] - 2s 578us/step - loss: 0.5970 - accuracy: 0.7394\n",
      "Epoch 53/100\n",
      "2662/2662 [==============================] - 2s 591us/step - loss: 0.5967 - accuracy: 0.7392\n",
      "Epoch 54/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.5957 - accuracy: 0.7401\n",
      "Epoch 55/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.5954 - accuracy: 0.7405\n",
      "Epoch 56/100\n",
      "2662/2662 [==============================] - 2s 590us/step - loss: 0.5947 - accuracy: 0.7423\n",
      "Epoch 57/100\n",
      "2662/2662 [==============================] - 2s 583us/step - loss: 0.5942 - accuracy: 0.7413\n",
      "Epoch 58/100\n",
      "2662/2662 [==============================] - 2s 590us/step - loss: 0.5935 - accuracy: 0.7414\n",
      "Epoch 59/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.5931 - accuracy: 0.7409\n",
      "Epoch 60/100\n",
      "2662/2662 [==============================] - 2s 585us/step - loss: 0.5924 - accuracy: 0.7425\n",
      "Epoch 61/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.5917 - accuracy: 0.7423\n",
      "Epoch 62/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.5912 - accuracy: 0.7425\n",
      "Epoch 63/100\n",
      "2662/2662 [==============================] - 2s 589us/step - loss: 0.5907 - accuracy: 0.7431\n",
      "Epoch 64/100\n",
      "2662/2662 [==============================] - 2s 585us/step - loss: 0.5905 - accuracy: 0.7439\n",
      "Epoch 65/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.5895 - accuracy: 0.7435\n",
      "Epoch 66/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.5883 - accuracy: 0.7439\n",
      "Epoch 67/100\n",
      "2662/2662 [==============================] - 2s 592us/step - loss: 0.5880 - accuracy: 0.7440\n",
      "Epoch 68/100\n",
      "2662/2662 [==============================] - 2s 589us/step - loss: 0.5878 - accuracy: 0.7445\n",
      "Epoch 69/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.5874 - accuracy: 0.7445\n",
      "Epoch 70/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.5868 - accuracy: 0.7436\n",
      "Epoch 71/100\n",
      "2662/2662 [==============================] - 2s 585us/step - loss: 0.5870 - accuracy: 0.7447\n",
      "Epoch 72/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.5857 - accuracy: 0.7440\n",
      "Epoch 73/100\n",
      "2662/2662 [==============================] - 2s 589us/step - loss: 0.5862 - accuracy: 0.7448\n",
      "Epoch 74/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.5850 - accuracy: 0.7455\n",
      "Epoch 75/100\n",
      "2662/2662 [==============================] - 2s 582us/step - loss: 0.5847 - accuracy: 0.7456\n",
      "Epoch 76/100\n",
      "2662/2662 [==============================] - 2s 589us/step - loss: 0.5845 - accuracy: 0.7462\n",
      "Epoch 77/100\n",
      "2662/2662 [==============================] - 2s 597us/step - loss: 0.5841 - accuracy: 0.7459\n",
      "Epoch 78/100\n",
      "2662/2662 [==============================] - 2s 590us/step - loss: 0.5827 - accuracy: 0.7457\n",
      "Epoch 79/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.5832 - accuracy: 0.7466\n",
      "Epoch 80/100\n",
      "2662/2662 [==============================] - 2s 591us/step - loss: 0.5827 - accuracy: 0.7466\n",
      "Epoch 81/100\n",
      "2662/2662 [==============================] - 2s 597us/step - loss: 0.5826 - accuracy: 0.7468\n",
      "Epoch 82/100\n",
      "2662/2662 [==============================] - 2s 587us/step - loss: 0.5818 - accuracy: 0.7465\n",
      "Epoch 83/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.5816 - accuracy: 0.7474\n",
      "Epoch 84/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.5815 - accuracy: 0.7477\n",
      "Epoch 85/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.5809 - accuracy: 0.7475\n",
      "Epoch 86/100\n",
      "2662/2662 [==============================] - 2s 591us/step - loss: 0.5808 - accuracy: 0.7475\n",
      "Epoch 87/100\n",
      "2662/2662 [==============================] - 2s 579us/step - loss: 0.5799 - accuracy: 0.7480\n",
      "Epoch 88/100\n",
      "2662/2662 [==============================] - 2s 585us/step - loss: 0.5799 - accuracy: 0.7484\n",
      "Epoch 89/100\n",
      "2662/2662 [==============================] - 2s 580us/step - loss: 0.5795 - accuracy: 0.7480\n",
      "Epoch 90/100\n",
      "2662/2662 [==============================] - 2s 581us/step - loss: 0.5798 - accuracy: 0.7485\n",
      "Epoch 91/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.5792 - accuracy: 0.7464\n",
      "Epoch 92/100\n",
      "2662/2662 [==============================] - 2s 594us/step - loss: 0.5785 - accuracy: 0.7473\n",
      "Epoch 93/100\n",
      "2662/2662 [==============================] - 2s 577us/step - loss: 0.5783 - accuracy: 0.7490\n",
      "Epoch 94/100\n",
      "2662/2662 [==============================] - 2s 581us/step - loss: 0.5777 - accuracy: 0.7469\n",
      "Epoch 95/100\n",
      "2662/2662 [==============================] - 2s 586us/step - loss: 0.5774 - accuracy: 0.7488\n",
      "Epoch 96/100\n",
      "2662/2662 [==============================] - 2s 589us/step - loss: 0.5772 - accuracy: 0.7483\n",
      "Epoch 97/100\n",
      "2662/2662 [==============================] - 2s 583us/step - loss: 0.5770 - accuracy: 0.7489\n",
      "Epoch 98/100\n",
      "2662/2662 [==============================] - 2s 580us/step - loss: 0.5763 - accuracy: 0.7495\n",
      "Epoch 99/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.5762 - accuracy: 0.7500\n",
      "Epoch 100/100\n",
      "2662/2662 [==============================] - 2s 576us/step - loss: 0.5753 - accuracy: 0.7513\n",
      "888/888 - 0s - loss: 0.6840 - accuracy: 0.7089 - 442ms/epoch - 498us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 3: Same as above, but MinMax normalization\n",
    "# Create a dictionary for iteration 3\n",
    "iteration_3 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_3[\"Iteration\"] = 3\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler3 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_3 = scaler3.fit(X_train_3b)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_3 = X_scaler_3.transform(X_train_3b)\n",
    "X_test_scaled_3 = X_scaler_3.transform(X_test_3b)\n",
    "\n",
    "# Define the model\n",
    "nn_3 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_3.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_3 = nn_3.fit(X_train_scaled_3, y_train_3b, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_3, model_accuracy_3 = nn_3.evaluate(X_test_scaled_3, y_test_3b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_3 dictionary\n",
    "iteration_3[\"Loss\"] = model_loss_3\n",
    "iteration_3[\"Accuracy\"] = model_accuracy_3\n",
    "\n",
    "# Add the iteration_3 dictionary to the iterations list\n",
    "iterations.append(iteration_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,643\n",
      "Trainable params: 7,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3611/3611 [==============================] - 2s 577us/step - loss: 0.9279 - accuracy: 0.5525\n",
      "Epoch 2/100\n",
      "3611/3611 [==============================] - 2s 564us/step - loss: 0.7905 - accuracy: 0.6431\n",
      "Epoch 3/100\n",
      "3611/3611 [==============================] - 2s 572us/step - loss: 0.7570 - accuracy: 0.6594\n",
      "Epoch 4/100\n",
      "3611/3611 [==============================] - 2s 586us/step - loss: 0.7415 - accuracy: 0.6662\n",
      "Epoch 5/100\n",
      "3611/3611 [==============================] - 2s 573us/step - loss: 0.7328 - accuracy: 0.6695\n",
      "Epoch 6/100\n",
      "3611/3611 [==============================] - 2s 569us/step - loss: 0.7261 - accuracy: 0.6730\n",
      "Epoch 7/100\n",
      "3611/3611 [==============================] - 2s 570us/step - loss: 0.7196 - accuracy: 0.6751\n",
      "Epoch 8/100\n",
      "3611/3611 [==============================] - 2s 577us/step - loss: 0.7140 - accuracy: 0.6793\n",
      "Epoch 9/100\n",
      "3611/3611 [==============================] - 2s 614us/step - loss: 0.7101 - accuracy: 0.6826\n",
      "Epoch 10/100\n",
      "3611/3611 [==============================] - 2s 585us/step - loss: 0.7061 - accuracy: 0.6822\n",
      "Epoch 11/100\n",
      "3611/3611 [==============================] - 2s 592us/step - loss: 0.7013 - accuracy: 0.6863\n",
      "Epoch 12/100\n",
      "3611/3611 [==============================] - 2s 588us/step - loss: 0.6982 - accuracy: 0.6879\n",
      "Epoch 13/100\n",
      "3611/3611 [==============================] - 2s 591us/step - loss: 0.6949 - accuracy: 0.6904\n",
      "Epoch 14/100\n",
      "3611/3611 [==============================] - 2s 591us/step - loss: 0.6915 - accuracy: 0.6916\n",
      "Epoch 15/100\n",
      "3611/3611 [==============================] - 2s 586us/step - loss: 0.6872 - accuracy: 0.6953\n",
      "Epoch 16/100\n",
      "3611/3611 [==============================] - 2s 593us/step - loss: 0.6853 - accuracy: 0.6952\n",
      "Epoch 17/100\n",
      "3611/3611 [==============================] - 2s 589us/step - loss: 0.6829 - accuracy: 0.6968\n",
      "Epoch 18/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6805 - accuracy: 0.6977\n",
      "Epoch 19/100\n",
      "3611/3611 [==============================] - 2s 588us/step - loss: 0.6780 - accuracy: 0.6997\n",
      "Epoch 20/100\n",
      "3611/3611 [==============================] - 2s 588us/step - loss: 0.6757 - accuracy: 0.7021\n",
      "Epoch 21/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6739 - accuracy: 0.7029\n",
      "Epoch 22/100\n",
      "3611/3611 [==============================] - 2s 593us/step - loss: 0.6710 - accuracy: 0.7046\n",
      "Epoch 23/100\n",
      "3611/3611 [==============================] - 2s 587us/step - loss: 0.6689 - accuracy: 0.7058\n",
      "Epoch 24/100\n",
      "3611/3611 [==============================] - 2s 588us/step - loss: 0.6672 - accuracy: 0.7061\n",
      "Epoch 25/100\n",
      "3611/3611 [==============================] - 2s 590us/step - loss: 0.6653 - accuracy: 0.7069\n",
      "Epoch 26/100\n",
      "3611/3611 [==============================] - 2s 590us/step - loss: 0.6643 - accuracy: 0.7077\n",
      "Epoch 27/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6632 - accuracy: 0.7087\n",
      "Epoch 28/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6613 - accuracy: 0.7096\n",
      "Epoch 29/100\n",
      "3611/3611 [==============================] - 2s 585us/step - loss: 0.6587 - accuracy: 0.7105\n",
      "Epoch 30/100\n",
      "3611/3611 [==============================] - 2s 578us/step - loss: 0.6577 - accuracy: 0.7117\n",
      "Epoch 31/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.6569 - accuracy: 0.7119\n",
      "Epoch 32/100\n",
      "3611/3611 [==============================] - 2s 579us/step - loss: 0.6552 - accuracy: 0.7126\n",
      "Epoch 33/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.6542 - accuracy: 0.7139\n",
      "Epoch 34/100\n",
      "3611/3611 [==============================] - 2s 583us/step - loss: 0.6532 - accuracy: 0.7147\n",
      "Epoch 35/100\n",
      "3611/3611 [==============================] - 2s 586us/step - loss: 0.6513 - accuracy: 0.7149\n",
      "Epoch 36/100\n",
      "3611/3611 [==============================] - 2s 586us/step - loss: 0.6507 - accuracy: 0.7151\n",
      "Epoch 37/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.6495 - accuracy: 0.7172\n",
      "Epoch 38/100\n",
      "3611/3611 [==============================] - 2s 581us/step - loss: 0.6489 - accuracy: 0.7168\n",
      "Epoch 39/100\n",
      "3611/3611 [==============================] - 2s 582us/step - loss: 0.6477 - accuracy: 0.7171\n",
      "Epoch 40/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.6462 - accuracy: 0.7178\n",
      "Epoch 41/100\n",
      "3611/3611 [==============================] - 2s 582us/step - loss: 0.6453 - accuracy: 0.7193\n",
      "Epoch 42/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.6442 - accuracy: 0.7190\n",
      "Epoch 43/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6443 - accuracy: 0.7184\n",
      "Epoch 44/100\n",
      "3611/3611 [==============================] - 2s 577us/step - loss: 0.6423 - accuracy: 0.7203\n",
      "Epoch 45/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6414 - accuracy: 0.7205\n",
      "Epoch 46/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6408 - accuracy: 0.7217\n",
      "Epoch 47/100\n",
      "3611/3611 [==============================] - 2s 578us/step - loss: 0.6396 - accuracy: 0.7213\n",
      "Epoch 48/100\n",
      "3611/3611 [==============================] - 2s 588us/step - loss: 0.6390 - accuracy: 0.7218\n",
      "Epoch 49/100\n",
      "3611/3611 [==============================] - 2s 583us/step - loss: 0.6384 - accuracy: 0.7232\n",
      "Epoch 50/100\n",
      "3611/3611 [==============================] - 2s 585us/step - loss: 0.6378 - accuracy: 0.7227\n",
      "Epoch 51/100\n",
      "3611/3611 [==============================] - 2s 588us/step - loss: 0.6368 - accuracy: 0.7235\n",
      "Epoch 52/100\n",
      "3611/3611 [==============================] - 2s 591us/step - loss: 0.6360 - accuracy: 0.7230\n",
      "Epoch 53/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6357 - accuracy: 0.7243\n",
      "Epoch 54/100\n",
      "3611/3611 [==============================] - 2s 577us/step - loss: 0.6342 - accuracy: 0.7244\n",
      "Epoch 55/100\n",
      "3611/3611 [==============================] - 2s 579us/step - loss: 0.6341 - accuracy: 0.7250\n",
      "Epoch 56/100\n",
      "3611/3611 [==============================] - 2s 586us/step - loss: 0.6339 - accuracy: 0.7250\n",
      "Epoch 57/100\n",
      "3611/3611 [==============================] - 2s 583us/step - loss: 0.6327 - accuracy: 0.7262\n",
      "Epoch 58/100\n",
      "3611/3611 [==============================] - 2s 578us/step - loss: 0.6321 - accuracy: 0.7261\n",
      "Epoch 59/100\n",
      "3611/3611 [==============================] - 2s 585us/step - loss: 0.6315 - accuracy: 0.7261\n",
      "Epoch 60/100\n",
      "3611/3611 [==============================] - 2s 587us/step - loss: 0.6304 - accuracy: 0.7272\n",
      "Epoch 61/100\n",
      "3611/3611 [==============================] - 2s 578us/step - loss: 0.6304 - accuracy: 0.7259\n",
      "Epoch 62/100\n",
      "3611/3611 [==============================] - 2s 588us/step - loss: 0.6294 - accuracy: 0.7279\n",
      "Epoch 63/100\n",
      "3611/3611 [==============================] - 2s 582us/step - loss: 0.6288 - accuracy: 0.7269\n",
      "Epoch 64/100\n",
      "3611/3611 [==============================] - 2s 582us/step - loss: 0.6277 - accuracy: 0.7285\n",
      "Epoch 65/100\n",
      "3611/3611 [==============================] - 2s 575us/step - loss: 0.6271 - accuracy: 0.7278\n",
      "Epoch 66/100\n",
      "3611/3611 [==============================] - 2s 587us/step - loss: 0.6272 - accuracy: 0.7283\n",
      "Epoch 67/100\n",
      "3611/3611 [==============================] - 2s 579us/step - loss: 0.6259 - accuracy: 0.7290\n",
      "Epoch 68/100\n",
      "3611/3611 [==============================] - 2s 576us/step - loss: 0.6243 - accuracy: 0.7301\n",
      "Epoch 69/100\n",
      "3611/3611 [==============================] - 2s 571us/step - loss: 0.6247 - accuracy: 0.7295\n",
      "Epoch 70/100\n",
      "3611/3611 [==============================] - 2s 575us/step - loss: 0.6248 - accuracy: 0.7298\n",
      "Epoch 71/100\n",
      "3611/3611 [==============================] - 2s 582us/step - loss: 0.6237 - accuracy: 0.7306\n",
      "Epoch 72/100\n",
      "3611/3611 [==============================] - 2s 574us/step - loss: 0.6229 - accuracy: 0.7306\n",
      "Epoch 73/100\n",
      "3611/3611 [==============================] - 2s 582us/step - loss: 0.6222 - accuracy: 0.7306\n",
      "Epoch 74/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.6214 - accuracy: 0.7319\n",
      "Epoch 75/100\n",
      "3611/3611 [==============================] - 2s 578us/step - loss: 0.6210 - accuracy: 0.7318\n",
      "Epoch 76/100\n",
      "3611/3611 [==============================] - 2s 578us/step - loss: 0.6215 - accuracy: 0.7307\n",
      "Epoch 77/100\n",
      "3611/3611 [==============================] - 2s 581us/step - loss: 0.6201 - accuracy: 0.7330\n",
      "Epoch 78/100\n",
      "3611/3611 [==============================] - 2s 583us/step - loss: 0.6197 - accuracy: 0.7322\n",
      "Epoch 79/100\n",
      "3611/3611 [==============================] - 2s 582us/step - loss: 0.6184 - accuracy: 0.7331\n",
      "Epoch 80/100\n",
      "3611/3611 [==============================] - 2s 574us/step - loss: 0.6183 - accuracy: 0.7329\n",
      "Epoch 81/100\n",
      "3611/3611 [==============================] - 2s 576us/step - loss: 0.6183 - accuracy: 0.7330\n",
      "Epoch 82/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.6170 - accuracy: 0.7331\n",
      "Epoch 83/100\n",
      "3611/3611 [==============================] - 2s 577us/step - loss: 0.6171 - accuracy: 0.7342\n",
      "Epoch 84/100\n",
      "3611/3611 [==============================] - 2s 578us/step - loss: 0.6170 - accuracy: 0.7335\n",
      "Epoch 85/100\n",
      "3611/3611 [==============================] - 2s 620us/step - loss: 0.6161 - accuracy: 0.7338\n",
      "Epoch 86/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6164 - accuracy: 0.7357\n",
      "Epoch 87/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6154 - accuracy: 0.7346\n",
      "Epoch 88/100\n",
      "3611/3611 [==============================] - 2s 582us/step - loss: 0.6153 - accuracy: 0.7348\n",
      "Epoch 89/100\n",
      "3611/3611 [==============================] - 2s 574us/step - loss: 0.6149 - accuracy: 0.7361\n",
      "Epoch 90/100\n",
      "3611/3611 [==============================] - 2s 582us/step - loss: 0.6142 - accuracy: 0.7356\n",
      "Epoch 91/100\n",
      "3611/3611 [==============================] - 2s 587us/step - loss: 0.6139 - accuracy: 0.7354\n",
      "Epoch 92/100\n",
      "3611/3611 [==============================] - 2s 592us/step - loss: 0.6145 - accuracy: 0.7358\n",
      "Epoch 93/100\n",
      "3611/3611 [==============================] - 2s 603us/step - loss: 0.6133 - accuracy: 0.7374\n",
      "Epoch 94/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.6128 - accuracy: 0.7369\n",
      "Epoch 95/100\n",
      "3611/3611 [==============================] - 2s 577us/step - loss: 0.6119 - accuracy: 0.7368\n",
      "Epoch 96/100\n",
      "3611/3611 [==============================] - 2s 583us/step - loss: 0.6119 - accuracy: 0.7373\n",
      "Epoch 97/100\n",
      "3611/3611 [==============================] - 2s 586us/step - loss: 0.6121 - accuracy: 0.7369\n",
      "Epoch 98/100\n",
      "3611/3611 [==============================] - 2s 581us/step - loss: 0.6115 - accuracy: 0.7377\n",
      "Epoch 99/100\n",
      "3611/3611 [==============================] - 2s 586us/step - loss: 0.6112 - accuracy: 0.7371\n",
      "Epoch 100/100\n",
      "3611/3611 [==============================] - 2s 582us/step - loss: 0.6106 - accuracy: 0.7377\n",
      "888/888 - 0s - loss: 0.7900 - accuracy: 0.6514 - 439ms/epoch - 495us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 4: MinMaxScaler normalization with random oversampling\n",
    "# Create a dictionary for iteration 4\n",
    "iteration_4 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_4[\"Iteration\"] = 4\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler4 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_4 = scaler4.fit(X_train_3b)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_4 = X_scaler_4.transform(X_train_3b)\n",
    "X_test_scaled_4 = X_scaler_4.transform(X_test_3b)\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_4 = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_4, y_ros_4 = ros_4.fit_resample(X_train_scaled_4, y_train_3b)\n",
    "\n",
    "# Define the model\n",
    "nn_4 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_4.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_4.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_4 = nn_4.fit(X_ros_4, y_ros_4, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_4, model_accuracy_4 = nn_4.evaluate(X_test_scaled_4, y_test_3b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_4 dictionary\n",
    "iteration_4[\"Loss\"] = model_loss_4\n",
    "iteration_4[\"Accuracy\"] = model_accuracy_4\n",
    "\n",
    "# Add the iteration_4 dictionary to the iterations list\n",
    "iterations.append(iteration_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,643\n",
      "Trainable params: 7,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3611/3611 [==============================] - 2s 564us/step - loss: 0.9202 - accuracy: 0.5561\n",
      "Epoch 2/100\n",
      "3611/3611 [==============================] - 2s 570us/step - loss: 0.7843 - accuracy: 0.6459\n",
      "Epoch 3/100\n",
      "3611/3611 [==============================] - 2s 566us/step - loss: 0.7510 - accuracy: 0.6618\n",
      "Epoch 4/100\n",
      "3611/3611 [==============================] - 2s 577us/step - loss: 0.7370 - accuracy: 0.6661\n",
      "Epoch 5/100\n",
      "3611/3611 [==============================] - 2s 570us/step - loss: 0.7264 - accuracy: 0.6733\n",
      "Epoch 6/100\n",
      "3611/3611 [==============================] - 2s 547us/step - loss: 0.7192 - accuracy: 0.6759\n",
      "Epoch 7/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.7124 - accuracy: 0.6809\n",
      "Epoch 8/100\n",
      "3611/3611 [==============================] - 2s 554us/step - loss: 0.7060 - accuracy: 0.6828\n",
      "Epoch 9/100\n",
      "3611/3611 [==============================] - 2s 560us/step - loss: 0.7002 - accuracy: 0.6870\n",
      "Epoch 10/100\n",
      "3611/3611 [==============================] - 2s 563us/step - loss: 0.6956 - accuracy: 0.6899\n",
      "Epoch 11/100\n",
      "3611/3611 [==============================] - 2s 556us/step - loss: 0.6910 - accuracy: 0.6921\n",
      "Epoch 12/100\n",
      "3611/3611 [==============================] - 2s 562us/step - loss: 0.6876 - accuracy: 0.6932\n",
      "Epoch 13/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6837 - accuracy: 0.6959\n",
      "Epoch 14/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6807 - accuracy: 0.6988\n",
      "Epoch 15/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.6778 - accuracy: 0.6999\n",
      "Epoch 16/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.6740 - accuracy: 0.7015\n",
      "Epoch 17/100\n",
      "3611/3611 [==============================] - 2s 549us/step - loss: 0.6705 - accuracy: 0.7040\n",
      "Epoch 18/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.6684 - accuracy: 0.7043\n",
      "Epoch 19/100\n",
      "3611/3611 [==============================] - 2s 561us/step - loss: 0.6664 - accuracy: 0.7051\n",
      "Epoch 20/100\n",
      "3611/3611 [==============================] - 2s 567us/step - loss: 0.6630 - accuracy: 0.7069\n",
      "Epoch 21/100\n",
      "3611/3611 [==============================] - 2s 569us/step - loss: 0.6613 - accuracy: 0.7098\n",
      "Epoch 22/100\n",
      "3611/3611 [==============================] - 2s 561us/step - loss: 0.6586 - accuracy: 0.7106\n",
      "Epoch 23/100\n",
      "3611/3611 [==============================] - 2s 563us/step - loss: 0.6574 - accuracy: 0.7109\n",
      "Epoch 24/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6545 - accuracy: 0.7128\n",
      "Epoch 25/100\n",
      "3611/3611 [==============================] - 2s 552us/step - loss: 0.6521 - accuracy: 0.7135\n",
      "Epoch 26/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.6505 - accuracy: 0.7145\n",
      "Epoch 27/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.6484 - accuracy: 0.7153\n",
      "Epoch 28/100\n",
      "3611/3611 [==============================] - 2s 564us/step - loss: 0.6465 - accuracy: 0.7170\n",
      "Epoch 29/100\n",
      "3611/3611 [==============================] - 2s 597us/step - loss: 0.6451 - accuracy: 0.7187\n",
      "Epoch 30/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6436 - accuracy: 0.7194\n",
      "Epoch 31/100\n",
      "3611/3611 [==============================] - 2s 560us/step - loss: 0.6422 - accuracy: 0.7191\n",
      "Epoch 32/100\n",
      "3611/3611 [==============================] - 2s 576us/step - loss: 0.6401 - accuracy: 0.7210\n",
      "Epoch 33/100\n",
      "3611/3611 [==============================] - 2s 590us/step - loss: 0.6394 - accuracy: 0.7208\n",
      "Epoch 34/100\n",
      "3611/3611 [==============================] - 2s 592us/step - loss: 0.6375 - accuracy: 0.7220\n",
      "Epoch 35/100\n",
      "3611/3611 [==============================] - 2s 587us/step - loss: 0.6365 - accuracy: 0.7218\n",
      "Epoch 36/100\n",
      "3611/3611 [==============================] - 2s 577us/step - loss: 0.6353 - accuracy: 0.7226\n",
      "Epoch 37/100\n",
      "3611/3611 [==============================] - 2s 581us/step - loss: 0.6341 - accuracy: 0.7236\n",
      "Epoch 38/100\n",
      "3611/3611 [==============================] - 2s 578us/step - loss: 0.6325 - accuracy: 0.7253\n",
      "Epoch 39/100\n",
      "3611/3611 [==============================] - 2s 579us/step - loss: 0.6314 - accuracy: 0.7259\n",
      "Epoch 40/100\n",
      "3611/3611 [==============================] - 2s 577us/step - loss: 0.6306 - accuracy: 0.7262\n",
      "Epoch 41/100\n",
      "3611/3611 [==============================] - 2s 583us/step - loss: 0.6290 - accuracy: 0.7267\n",
      "Epoch 42/100\n",
      "3611/3611 [==============================] - 2s 583us/step - loss: 0.6286 - accuracy: 0.7265\n",
      "Epoch 43/100\n",
      "3611/3611 [==============================] - 2s 573us/step - loss: 0.6281 - accuracy: 0.7267\n",
      "Epoch 44/100\n",
      "3611/3611 [==============================] - 2s 569us/step - loss: 0.6270 - accuracy: 0.7287\n",
      "Epoch 45/100\n",
      "3611/3611 [==============================] - 2s 573us/step - loss: 0.6257 - accuracy: 0.7284\n",
      "Epoch 46/100\n",
      "3611/3611 [==============================] - 2s 590us/step - loss: 0.6248 - accuracy: 0.7290\n",
      "Epoch 47/100\n",
      "3611/3611 [==============================] - 2s 594us/step - loss: 0.6236 - accuracy: 0.7286\n",
      "Epoch 48/100\n",
      "3611/3611 [==============================] - 2s 588us/step - loss: 0.6226 - accuracy: 0.7301\n",
      "Epoch 49/100\n",
      "3611/3611 [==============================] - 2s 576us/step - loss: 0.6219 - accuracy: 0.7312\n",
      "Epoch 50/100\n",
      "3611/3611 [==============================] - 2s 578us/step - loss: 0.6212 - accuracy: 0.7302\n",
      "Epoch 51/100\n",
      "3611/3611 [==============================] - 2s 586us/step - loss: 0.6209 - accuracy: 0.7310\n",
      "Epoch 52/100\n",
      "3611/3611 [==============================] - 2s 587us/step - loss: 0.6198 - accuracy: 0.7321\n",
      "Epoch 53/100\n",
      "3611/3611 [==============================] - 2s 573us/step - loss: 0.6192 - accuracy: 0.7316\n",
      "Epoch 54/100\n",
      "3611/3611 [==============================] - 2s 592us/step - loss: 0.6185 - accuracy: 0.7320\n",
      "Epoch 55/100\n",
      "3611/3611 [==============================] - 2s 586us/step - loss: 0.6181 - accuracy: 0.7321\n",
      "Epoch 56/100\n",
      "3611/3611 [==============================] - 2s 581us/step - loss: 0.6165 - accuracy: 0.7333\n",
      "Epoch 57/100\n",
      "3611/3611 [==============================] - 2s 568us/step - loss: 0.6162 - accuracy: 0.7336\n",
      "Epoch 58/100\n",
      "3611/3611 [==============================] - 2s 589us/step - loss: 0.6153 - accuracy: 0.7351\n",
      "Epoch 59/100\n",
      "3611/3611 [==============================] - 2s 590us/step - loss: 0.6144 - accuracy: 0.7349\n",
      "Epoch 60/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6141 - accuracy: 0.7351\n",
      "Epoch 61/100\n",
      "3611/3611 [==============================] - 2s 594us/step - loss: 0.6132 - accuracy: 0.7357\n",
      "Epoch 62/100\n",
      "3611/3611 [==============================] - 2s 583us/step - loss: 0.6122 - accuracy: 0.7352\n",
      "Epoch 63/100\n",
      "3611/3611 [==============================] - 2s 581us/step - loss: 0.6126 - accuracy: 0.7354\n",
      "Epoch 64/100\n",
      "3611/3611 [==============================] - 2s 573us/step - loss: 0.6120 - accuracy: 0.7364\n",
      "Epoch 65/100\n",
      "3611/3611 [==============================] - 2s 573us/step - loss: 0.6109 - accuracy: 0.7369\n",
      "Epoch 66/100\n",
      "3611/3611 [==============================] - 2s 575us/step - loss: 0.6100 - accuracy: 0.7370\n",
      "Epoch 67/100\n",
      "3611/3611 [==============================] - 2s 579us/step - loss: 0.6097 - accuracy: 0.7369\n",
      "Epoch 68/100\n",
      "3611/3611 [==============================] - 2s 579us/step - loss: 0.6093 - accuracy: 0.7358\n",
      "Epoch 69/100\n",
      "3611/3611 [==============================] - 2s 577us/step - loss: 0.6086 - accuracy: 0.7374\n",
      "Epoch 70/100\n",
      "3611/3611 [==============================] - 2s 588us/step - loss: 0.6077 - accuracy: 0.7380\n",
      "Epoch 71/100\n",
      "3611/3611 [==============================] - 2s 592us/step - loss: 0.6073 - accuracy: 0.7383\n",
      "Epoch 72/100\n",
      "3611/3611 [==============================] - 2s 597us/step - loss: 0.6066 - accuracy: 0.7377\n",
      "Epoch 73/100\n",
      "3611/3611 [==============================] - 2s 583us/step - loss: 0.6066 - accuracy: 0.7381\n",
      "Epoch 74/100\n",
      "3611/3611 [==============================] - 2s 586us/step - loss: 0.6058 - accuracy: 0.7395\n",
      "Epoch 75/100\n",
      "3611/3611 [==============================] - 2s 590us/step - loss: 0.6044 - accuracy: 0.7391\n",
      "Epoch 76/100\n",
      "3611/3611 [==============================] - 2s 587us/step - loss: 0.6044 - accuracy: 0.7399\n",
      "Epoch 77/100\n",
      "3611/3611 [==============================] - 2s 588us/step - loss: 0.6042 - accuracy: 0.7401\n",
      "Epoch 78/100\n",
      "3611/3611 [==============================] - 2s 583us/step - loss: 0.6038 - accuracy: 0.7404\n",
      "Epoch 79/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6033 - accuracy: 0.7411\n",
      "Epoch 80/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.6035 - accuracy: 0.7404\n",
      "Epoch 81/100\n",
      "3611/3611 [==============================] - 2s 579us/step - loss: 0.6020 - accuracy: 0.7419\n",
      "Epoch 82/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6012 - accuracy: 0.7414\n",
      "Epoch 83/100\n",
      "3611/3611 [==============================] - 2s 575us/step - loss: 0.6013 - accuracy: 0.7415\n",
      "Epoch 84/100\n",
      "3611/3611 [==============================] - 2s 584us/step - loss: 0.6003 - accuracy: 0.7422\n",
      "Epoch 85/100\n",
      "3611/3611 [==============================] - 2s 579us/step - loss: 0.5999 - accuracy: 0.7432\n",
      "Epoch 86/100\n",
      "3611/3611 [==============================] - 2s 613us/step - loss: 0.5996 - accuracy: 0.7420\n",
      "Epoch 87/100\n",
      "3611/3611 [==============================] - 2s 581us/step - loss: 0.5994 - accuracy: 0.7421\n",
      "Epoch 88/100\n",
      "3611/3611 [==============================] - 2s 581us/step - loss: 0.5994 - accuracy: 0.7431\n",
      "Epoch 89/100\n",
      "3611/3611 [==============================] - 2s 581us/step - loss: 0.5984 - accuracy: 0.7442\n",
      "Epoch 90/100\n",
      "3611/3611 [==============================] - 2s 574us/step - loss: 0.5982 - accuracy: 0.7435\n",
      "Epoch 91/100\n",
      "3611/3611 [==============================] - 2s 574us/step - loss: 0.5977 - accuracy: 0.7439\n",
      "Epoch 92/100\n",
      "3611/3611 [==============================] - 2s 572us/step - loss: 0.5975 - accuracy: 0.7426\n",
      "Epoch 93/100\n",
      "3611/3611 [==============================] - 2s 573us/step - loss: 0.5974 - accuracy: 0.7433\n",
      "Epoch 94/100\n",
      "3611/3611 [==============================] - 2s 570us/step - loss: 0.5961 - accuracy: 0.7449\n",
      "Epoch 95/100\n",
      "3611/3611 [==============================] - 2s 573us/step - loss: 0.5965 - accuracy: 0.7441\n",
      "Epoch 96/100\n",
      "3611/3611 [==============================] - 2s 596us/step - loss: 0.5965 - accuracy: 0.7447\n",
      "Epoch 97/100\n",
      "3611/3611 [==============================] - 2s 576us/step - loss: 0.5946 - accuracy: 0.7458\n",
      "Epoch 98/100\n",
      "3611/3611 [==============================] - 2s 576us/step - loss: 0.5943 - accuracy: 0.7459\n",
      "Epoch 99/100\n",
      "3611/3611 [==============================] - 2s 571us/step - loss: 0.5947 - accuracy: 0.7451\n",
      "Epoch 100/100\n",
      "3611/3611 [==============================] - 2s 576us/step - loss: 0.5942 - accuracy: 0.7452\n",
      "888/888 - 0s - loss: 0.7940 - accuracy: 0.6546 - 436ms/epoch - 491us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 5: MinMaxScaler normalization on only a subset of columns, with random oversampling\n",
    "# Create a dictionary for iteration 5\n",
    "iteration_5 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_5[\"Iteration\"] = 5\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler5 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_5 = scaler5.fit(X_train_3b[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_5 = X_train_3b\n",
    "X_train_scaled_5[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_5.transform(X_train_3b[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "X_test_scaled_5 = X_test_3b\n",
    "X_test_scaled_5[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_5.transform(X_test_3b[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_5 = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_5, y_ros_5 = ros_5.fit_resample(X_train_scaled_5, y_train_3b)\n",
    "\n",
    "# Define the model\n",
    "nn_5 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_5.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_5.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_5.add(tf.keras.layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_5.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_5.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_5 = nn_5.fit(X_ros_5, y_ros_5, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_5, model_accuracy_5 = nn_5.evaluate(X_test_scaled_5, y_test_3b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_5 dictionary\n",
    "iteration_5[\"Loss\"] = model_loss_5\n",
    "iteration_5[\"Accuracy\"] = model_accuracy_5\n",
    "\n",
    "# Add the iteration_5 dictionary to the iterations list\n",
    "iterations.append(iteration_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model structure (this is the same for iterations 1-5 so is only done once)\n",
    "ann_viz(nn_5, title=\"Categorical Popularity Testing Structure\", filename=\"summary_stats/categorical_popularity_testing_nn_structure.gv\", view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 01m 00s]\n",
      "val_accuracy: 0.6562632322311401\n",
      "\n",
      "Best val_accuracy So Far: 0.6727842688560486\n",
      "Total elapsed time: 00h 24m 57s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "### Iteration 6: Hyperparameter tuning with iteration 4 model\n",
    "# Create a dictionary for iteration 6\n",
    "iteration_6 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_6[\"Iteration\"] = 6\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler6 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_6 = scaler6.fit(X_train_3b)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_6 = X_scaler_6.transform(X_train_3b)\n",
    "X_test_scaled_6 = X_scaler_6.transform(X_test_3b)\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_6 = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_6, y_ros_6 = ros_6.fit_resample(X_train_scaled_6, y_train_3b)\n",
    "\n",
    "# Create a method that creates a Sequential model with hyperparameter tuning\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Determine the activation functions for each layer\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "\n",
    "    # Determine neurons in the first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units', \n",
    "        min_value=80, \n",
    "        max_value=120, \n",
    "        step=20), \n",
    "        activation=activation, input_dim=42))\n",
    "    \n",
    "    # Determine the number of hidden layers and neurons in them\n",
    "    for i in range(hp.Int('num_layers', 2, 4)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_'+str(i),\n",
    "            min_value=50, \n",
    "            max_value=90,\n",
    "            step=20),\n",
    "            activation=activation))\n",
    "    \n",
    "    # Set up the output layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return nn_model\n",
    "\n",
    "# Set up the kerastuner instance\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\", \n",
    "    max_epochs=20, \n",
    "    hyperband_iterations=2,\n",
    "    overwrite=True)\n",
    "\n",
    "# Run the kerastuner\n",
    "tuner.search(X_ros_6, y_ros_6, epochs=20, validation_data=(X_test_scaled_6, y_test_3b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'tanh', 'first_units': 80, 'num_layers': 3, 'units_0': 70, 'units_1': 90, 'units_2': 90, 'units_3': 50, 'tuner/epochs': 20, 'tuner/initial_epoch': 7, 'tuner/bracket': 2, 'tuner/round': 2, 'tuner/trial_id': '0014'}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 70)                5670      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 90)                6390      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 90)                8190      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 273       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,963\n",
      "Trainable params: 23,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3611/3611 [==============================] - 3s 660us/step - loss: 0.9155 - accuracy: 0.5559\n",
      "Epoch 2/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.7689 - accuracy: 0.6499\n",
      "Epoch 3/100\n",
      "3611/3611 [==============================] - 2s 653us/step - loss: 0.7494 - accuracy: 0.6587\n",
      "Epoch 4/100\n",
      "3611/3611 [==============================] - 2s 657us/step - loss: 0.7372 - accuracy: 0.6639\n",
      "Epoch 5/100\n",
      "3611/3611 [==============================] - 2s 656us/step - loss: 0.7259 - accuracy: 0.6710\n",
      "Epoch 6/100\n",
      "3611/3611 [==============================] - 2s 652us/step - loss: 0.7158 - accuracy: 0.6768\n",
      "Epoch 7/100\n",
      "3611/3611 [==============================] - 2s 656us/step - loss: 0.7055 - accuracy: 0.6821\n",
      "Epoch 8/100\n",
      "3611/3611 [==============================] - 2s 658us/step - loss: 0.6960 - accuracy: 0.6886\n",
      "Epoch 9/100\n",
      "3611/3611 [==============================] - 2s 654us/step - loss: 0.6869 - accuracy: 0.6930\n",
      "Epoch 10/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.6766 - accuracy: 0.6996\n",
      "Epoch 11/100\n",
      "3611/3611 [==============================] - 2s 656us/step - loss: 0.6671 - accuracy: 0.7047\n",
      "Epoch 12/100\n",
      "3611/3611 [==============================] - 2s 648us/step - loss: 0.6586 - accuracy: 0.7083\n",
      "Epoch 13/100\n",
      "3611/3611 [==============================] - 2s 656us/step - loss: 0.6496 - accuracy: 0.7144\n",
      "Epoch 14/100\n",
      "3611/3611 [==============================] - 2s 651us/step - loss: 0.6389 - accuracy: 0.7207\n",
      "Epoch 15/100\n",
      "3611/3611 [==============================] - 2s 652us/step - loss: 0.6307 - accuracy: 0.7261\n",
      "Epoch 16/100\n",
      "3611/3611 [==============================] - 2s 653us/step - loss: 0.6219 - accuracy: 0.7300\n",
      "Epoch 17/100\n",
      "3611/3611 [==============================] - 2s 652us/step - loss: 0.6135 - accuracy: 0.7344\n",
      "Epoch 18/100\n",
      "3611/3611 [==============================] - 2s 650us/step - loss: 0.6055 - accuracy: 0.7396\n",
      "Epoch 19/100\n",
      "3611/3611 [==============================] - 2s 654us/step - loss: 0.5979 - accuracy: 0.7418\n",
      "Epoch 20/100\n",
      "3611/3611 [==============================] - 2s 657us/step - loss: 0.5901 - accuracy: 0.7464\n",
      "Epoch 21/100\n",
      "3611/3611 [==============================] - 2s 650us/step - loss: 0.5832 - accuracy: 0.7506\n",
      "Epoch 22/100\n",
      "3611/3611 [==============================] - 2s 655us/step - loss: 0.5765 - accuracy: 0.7541\n",
      "Epoch 23/100\n",
      "3611/3611 [==============================] - 2s 653us/step - loss: 0.5695 - accuracy: 0.7561\n",
      "Epoch 24/100\n",
      "3611/3611 [==============================] - 2s 657us/step - loss: 0.5630 - accuracy: 0.7611\n",
      "Epoch 25/100\n",
      "3611/3611 [==============================] - 2s 651us/step - loss: 0.5571 - accuracy: 0.7633\n",
      "Epoch 26/100\n",
      "3611/3611 [==============================] - 2s 658us/step - loss: 0.5516 - accuracy: 0.7673\n",
      "Epoch 27/100\n",
      "3611/3611 [==============================] - 2s 658us/step - loss: 0.5458 - accuracy: 0.7697\n",
      "Epoch 28/100\n",
      "3611/3611 [==============================] - 2s 659us/step - loss: 0.5399 - accuracy: 0.7729\n",
      "Epoch 29/100\n",
      "3611/3611 [==============================] - 2s 654us/step - loss: 0.5349 - accuracy: 0.7747\n",
      "Epoch 30/100\n",
      "3611/3611 [==============================] - 2s 663us/step - loss: 0.5303 - accuracy: 0.7775\n",
      "Epoch 31/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.5248 - accuracy: 0.7798\n",
      "Epoch 32/100\n",
      "3611/3611 [==============================] - 2s 663us/step - loss: 0.5215 - accuracy: 0.7816\n",
      "Epoch 33/100\n",
      "3611/3611 [==============================] - 2s 660us/step - loss: 0.5165 - accuracy: 0.7839\n",
      "Epoch 34/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.5119 - accuracy: 0.7844\n",
      "Epoch 35/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.5080 - accuracy: 0.7874\n",
      "Epoch 36/100\n",
      "3611/3611 [==============================] - 2s 657us/step - loss: 0.5035 - accuracy: 0.7903\n",
      "Epoch 37/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.5010 - accuracy: 0.7904\n",
      "Epoch 38/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.4957 - accuracy: 0.7951\n",
      "Epoch 39/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.4922 - accuracy: 0.7961\n",
      "Epoch 40/100\n",
      "3611/3611 [==============================] - 2s 655us/step - loss: 0.4901 - accuracy: 0.7958\n",
      "Epoch 41/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.4865 - accuracy: 0.7974\n",
      "Epoch 42/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.4833 - accuracy: 0.8004\n",
      "Epoch 43/100\n",
      "3611/3611 [==============================] - 2s 660us/step - loss: 0.4800 - accuracy: 0.8015\n",
      "Epoch 44/100\n",
      "3611/3611 [==============================] - 2s 658us/step - loss: 0.4782 - accuracy: 0.8027\n",
      "Epoch 45/100\n",
      "3611/3611 [==============================] - 2s 665us/step - loss: 0.4729 - accuracy: 0.8040\n",
      "Epoch 46/100\n",
      "3611/3611 [==============================] - 2s 664us/step - loss: 0.4704 - accuracy: 0.8053\n",
      "Epoch 47/100\n",
      "3611/3611 [==============================] - 2s 667us/step - loss: 0.4672 - accuracy: 0.8072\n",
      "Epoch 48/100\n",
      "3611/3611 [==============================] - 2s 659us/step - loss: 0.4652 - accuracy: 0.8075\n",
      "Epoch 49/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.4625 - accuracy: 0.8086\n",
      "Epoch 50/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.4598 - accuracy: 0.8100\n",
      "Epoch 51/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.4568 - accuracy: 0.8129\n",
      "Epoch 52/100\n",
      "3611/3611 [==============================] - 2s 656us/step - loss: 0.4544 - accuracy: 0.8121\n",
      "Epoch 53/100\n",
      "3611/3611 [==============================] - 2s 660us/step - loss: 0.4512 - accuracy: 0.8137\n",
      "Epoch 54/100\n",
      "3611/3611 [==============================] - 2s 660us/step - loss: 0.4505 - accuracy: 0.8137\n",
      "Epoch 55/100\n",
      "3611/3611 [==============================] - 2s 658us/step - loss: 0.4470 - accuracy: 0.8162\n",
      "Epoch 56/100\n",
      "3611/3611 [==============================] - 2s 654us/step - loss: 0.4456 - accuracy: 0.8170\n",
      "Epoch 57/100\n",
      "3611/3611 [==============================] - 2s 659us/step - loss: 0.4434 - accuracy: 0.8181\n",
      "Epoch 58/100\n",
      "3611/3611 [==============================] - 2s 659us/step - loss: 0.4388 - accuracy: 0.8191\n",
      "Epoch 59/100\n",
      "3611/3611 [==============================] - 2s 660us/step - loss: 0.4372 - accuracy: 0.8204\n",
      "Epoch 60/100\n",
      "3611/3611 [==============================] - 2s 658us/step - loss: 0.4360 - accuracy: 0.8211\n",
      "Epoch 61/100\n",
      "3611/3611 [==============================] - 2s 664us/step - loss: 0.4347 - accuracy: 0.8201\n",
      "Epoch 62/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.4335 - accuracy: 0.8221\n",
      "Epoch 63/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.4303 - accuracy: 0.8234\n",
      "Epoch 64/100\n",
      "3611/3611 [==============================] - 2s 663us/step - loss: 0.4287 - accuracy: 0.8225\n",
      "Epoch 65/100\n",
      "3611/3611 [==============================] - 2s 660us/step - loss: 0.4261 - accuracy: 0.8248\n",
      "Epoch 66/100\n",
      "3611/3611 [==============================] - 2s 663us/step - loss: 0.4248 - accuracy: 0.8249\n",
      "Epoch 67/100\n",
      "3611/3611 [==============================] - 2s 664us/step - loss: 0.4238 - accuracy: 0.8251\n",
      "Epoch 68/100\n",
      "3611/3611 [==============================] - 2s 663us/step - loss: 0.4221 - accuracy: 0.8268\n",
      "Epoch 69/100\n",
      "3611/3611 [==============================] - 2s 658us/step - loss: 0.4199 - accuracy: 0.8275\n",
      "Epoch 70/100\n",
      "3611/3611 [==============================] - 2s 658us/step - loss: 0.4189 - accuracy: 0.8277\n",
      "Epoch 71/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.4162 - accuracy: 0.8295\n",
      "Epoch 72/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.4163 - accuracy: 0.8290\n",
      "Epoch 73/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.4139 - accuracy: 0.8298\n",
      "Epoch 74/100\n",
      "3611/3611 [==============================] - 2s 659us/step - loss: 0.4131 - accuracy: 0.8306\n",
      "Epoch 75/100\n",
      "3611/3611 [==============================] - 2s 663us/step - loss: 0.4122 - accuracy: 0.8312\n",
      "Epoch 76/100\n",
      "3611/3611 [==============================] - 2s 663us/step - loss: 0.4081 - accuracy: 0.8323\n",
      "Epoch 77/100\n",
      "3611/3611 [==============================] - 2s 664us/step - loss: 0.4075 - accuracy: 0.8330\n",
      "Epoch 78/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.4074 - accuracy: 0.8331\n",
      "Epoch 79/100\n",
      "3611/3611 [==============================] - 2s 666us/step - loss: 0.4072 - accuracy: 0.8328\n",
      "Epoch 80/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.4057 - accuracy: 0.8341\n",
      "Epoch 81/100\n",
      "3611/3611 [==============================] - 2s 666us/step - loss: 0.4045 - accuracy: 0.8348\n",
      "Epoch 82/100\n",
      "3611/3611 [==============================] - 2s 663us/step - loss: 0.4026 - accuracy: 0.8355\n",
      "Epoch 83/100\n",
      "3611/3611 [==============================] - 2s 664us/step - loss: 0.4026 - accuracy: 0.8343\n",
      "Epoch 84/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.4025 - accuracy: 0.8338\n",
      "Epoch 85/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.3982 - accuracy: 0.8376\n",
      "Epoch 86/100\n",
      "3611/3611 [==============================] - 2s 664us/step - loss: 0.3985 - accuracy: 0.8366\n",
      "Epoch 87/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.3961 - accuracy: 0.8388\n",
      "Epoch 88/100\n",
      "3611/3611 [==============================] - 2s 664us/step - loss: 0.3961 - accuracy: 0.8375\n",
      "Epoch 89/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.3957 - accuracy: 0.8377\n",
      "Epoch 90/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.3948 - accuracy: 0.8387\n",
      "Epoch 91/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.3938 - accuracy: 0.8379\n",
      "Epoch 92/100\n",
      "3611/3611 [==============================] - 2s 660us/step - loss: 0.3919 - accuracy: 0.8403\n",
      "Epoch 93/100\n",
      "3611/3611 [==============================] - 2s 661us/step - loss: 0.3915 - accuracy: 0.8396\n",
      "Epoch 94/100\n",
      "3611/3611 [==============================] - 2s 662us/step - loss: 0.3918 - accuracy: 0.8401\n",
      "Epoch 95/100\n",
      "3611/3611 [==============================] - 2s 660us/step - loss: 0.3898 - accuracy: 0.8409\n",
      "Epoch 96/100\n",
      "3611/3611 [==============================] - 2s 663us/step - loss: 0.3885 - accuracy: 0.8416\n",
      "Epoch 97/100\n",
      "3611/3611 [==============================] - 2s 665us/step - loss: 0.3879 - accuracy: 0.8415\n",
      "Epoch 98/100\n",
      "3611/3611 [==============================] - 2s 663us/step - loss: 0.3871 - accuracy: 0.8414\n",
      "Epoch 99/100\n",
      "3611/3611 [==============================] - 2s 665us/step - loss: 0.3865 - accuracy: 0.8418\n",
      "Epoch 100/100\n",
      "3611/3611 [==============================] - 2s 665us/step - loss: 0.3854 - accuracy: 0.8421\n",
      "888/888 - 0s - loss: 1.0088 - accuracy: 0.6675 - 452ms/epoch - 509us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 6 continued\n",
    "# Get the best model and show its parameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)\n",
    "for params in best_hyper:\n",
    "    print(params.values)\n",
    "\n",
    "# Get a summary of the best model\n",
    "best_model = tuner.get_best_models()\n",
    "best_model[0].summary()\n",
    "\n",
    "# Build the best model and train it on the data\n",
    "model = tuner.hypermodel.build(best_hyper[0])\n",
    "model.fit(X_ros_6, y_ros_6, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_6, model_accuracy_6 = model.evaluate(X_test_scaled_6, y_test_3b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_6 dictionary\n",
    "iteration_6[\"Loss\"] = model_loss_6\n",
    "iteration_6[\"Accuracy\"] = model_accuracy_6\n",
    "\n",
    "# Add the iteration_6 dictionary to the iterations list\n",
    "iterations.append(iteration_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the final model structure\n",
    "ann_viz(model, title=\"Categorical Popularity Final Structure\", filename=\"summary_stats/categorical_popularity_final_nn_structure.gv\", view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss: 0.8346540927886963, Accuracy: 0.6674651503562927\n",
      "Iteration 2: Loss: 0.6880972385406494, Accuracy: 0.7081513404846191\n",
      "Iteration 3: Loss: 0.6839743852615356, Accuracy: 0.7088558673858643\n",
      "Iteration 4: Loss: 0.7899882793426514, Accuracy: 0.6514372229576111\n",
      "Iteration 5: Loss: 0.793960452079773, Accuracy: 0.6546075940132141\n",
      "Iteration 6: Loss: 1.0087584257125854, Accuracy: 0.6674651503562927\n"
     ]
    }
   ],
   "source": [
    "# Print Final Statistics:\n",
    "for i in range(len(iterations)):\n",
    "    print(f\"Iteration {i+1}: Loss: {iterations[i]['Loss']}, Accuracy: {iterations[i]['Accuracy']}\")\n",
    "\n",
    "# Add final statistics to a dataframe and output to a csv\n",
    "stats_df = pd.DataFrame(iterations)\n",
    "stats_df.to_csv(\"summary_stats/categorical_popularity_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
