{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import findspark\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark_dist_explore import hist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize findspark\n",
    "findspark.init()\n",
    "\n",
    "# Initialize the spark session\n",
    "spark = SparkSession.builder.appName(\"SK_model\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|   primary_artist|          track_name|popularity|explicit|danceability|energy|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|      duration_min|time_signature_0|time_signature_1|time_signature_3|time_signature_4|time_signature_5|key_0|key_1|key_2|key_3|key_4|key_5|key_6|key_7|key_8|key_9|key_10|key_11|num_artists_binned_1|num_artists_binned_2|num_artists_binned_3|num_artists_binned_4|num_artists_binned_5|num_artists_binned_6|track_genre_0|track_genre_1|track_genre_2|track_genre_3|track_genre_4|track_genre_5|track_genre_6|\n",
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|      Gen Hoshino|              Comedy|        73|       0|       0.676| 0.461|  -6.746|   0|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|3.8444333333333334|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Ben Woodward|    Ghost - Acoustic|        55|       0|        0.42| 0.166| -17.235|   1|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|            2.4935|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|Ingrid Michaelson|      To Begin Again|        57|       0|       0.438| 0.359|  -9.734|   1|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|3.5137666666666667|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Kina Grannis|Can't Help Fallin...|        71|       0|       0.266|0.0596| -18.515|   1|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|           3.36555|               0|               0|               1|               0|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "| Chord Overstreet|             Hold On|        82|       0|       0.618| 0.443|  -9.681|   1|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949| 3.314216666666667|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Tyrone Wells|Days I Will Remember|        58|       0|       0.688| 0.481|  -8.807|   1|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017| 3.570666666666667|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|A Great Big World|       Say Something|        74|       0|       0.407| 0.147|  -8.822|   1|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|3.8233333333333333|               0|               0|               1|               0|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|           I'm Yours|        80|       0|       0.703| 0.444|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|            4.0491|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     1|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|               Lucky|        74|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|   Ross Copperman|              Hunger|        56|       0|       0.442| 0.632|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|3.4265666666666665|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Zack Tabudlo|Give Me Your Forever|        74|       0|       0.627| 0.363|  -8.127|   1|     0.0291|       0.279|             0.0|  0.0928|  0.301| 99.905|              4.08|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|     I Won't Give Up|        69|       0|       0.483| 0.303| -10.058|   1|     0.0429|       0.694|             0.0|   0.115|  0.139|133.406|           4.00275|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|         Dan Berk|                Solo|        52|       0|       0.489| 0.314|  -9.245|   0|     0.0331|       0.749|             0.0|   0.113|  0.607|124.234|3.3118666666666665|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|    Anna Hamilton|            Bad Liar|        62|       0|       0.691| 0.234|  -6.441|   1|     0.0285|       0.777|             0.0|    0.12|  0.209| 87.103|            4.1408|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "| Chord Overstreet|     Hold On - Remix|        56|       0|       0.755|  0.78|  -6.084|   1|     0.0327|       0.124|         2.83E-5|   0.121|  0.387|120.004|           3.13555|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|      Landon Pigg|Falling in Love a...|        58|       0|       0.489| 0.561|  -7.933|   1|     0.0274|         0.2|         4.56E-5|   0.179|  0.238| 83.457|            4.0831|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Andrew Foy|ily (i love you b...|        56|       0|       0.706| 0.112| -18.098|   1|     0.0391|       0.827|         4.03E-6|   0.125|  0.414|110.154|            2.1625|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Andrew Foy|         At My Worst|        54|       0|       0.795|0.0841|  -18.09|   0|     0.0461|       0.742|         1.17E-5|  0.0853|  0.609| 91.803|            2.8288|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     1|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|       Jason Mraz|               Lucky|        68|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|     Boyce Avenue|          Photograph|        67|       0|       0.717|  0.32|  -8.393|   1|     0.0283|        0.83|             0.0|   0.107|  0.322|107.946| 4.336433333333333|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "+-----------------+--------------------+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the pre-processed data\n",
    "s_df = spark.read.csv(\"../Resources/filtered_encoded_dataset.csv\", sep=\",\", header=True, inferSchema=True)\n",
    "s_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|popularity|explicit|danceability|energy|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|      duration_min|time_signature_0|time_signature_1|time_signature_3|time_signature_4|time_signature_5|key_0|key_1|key_2|key_3|key_4|key_5|key_6|key_7|key_8|key_9|key_10|key_11|num_artists_binned_1|num_artists_binned_2|num_artists_binned_3|num_artists_binned_4|num_artists_binned_5|num_artists_binned_6|track_genre_0|track_genre_1|track_genre_2|track_genre_3|track_genre_4|track_genre_5|track_genre_6|\n",
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|        73|       0|       0.676| 0.461|  -6.746|   0|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|3.8444333333333334|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        55|       0|        0.42| 0.166| -17.235|   1|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|            2.4935|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        57|       0|       0.438| 0.359|  -9.734|   1|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|3.5137666666666667|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        71|       0|       0.266|0.0596| -18.515|   1|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|           3.36555|               0|               0|               1|               0|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        82|       0|       0.618| 0.443|  -9.681|   1|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949| 3.314216666666667|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        58|       0|       0.688| 0.481|  -8.807|   1|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017| 3.570666666666667|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.407| 0.147|  -8.822|   1|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|3.8233333333333333|               0|               0|               1|               0|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        80|       0|       0.703| 0.444|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|            4.0491|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     1|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.442| 0.632|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|3.4265666666666665|               0|               0|               0|               1|               0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        74|       0|       0.627| 0.363|  -8.127|   1|     0.0291|       0.279|             0.0|  0.0928|  0.301| 99.905|              4.08|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        69|       0|       0.483| 0.303| -10.058|   1|     0.0429|       0.694|             0.0|   0.115|  0.139|133.406|           4.00275|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        52|       0|       0.489| 0.314|  -9.245|   0|     0.0331|       0.749|             0.0|   0.113|  0.607|124.234|3.3118666666666665|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        62|       0|       0.691| 0.234|  -6.441|   1|     0.0285|       0.777|             0.0|    0.12|  0.209| 87.103|            4.1408|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.755|  0.78|  -6.084|   1|     0.0327|       0.124|         2.83E-5|   0.121|  0.387|120.004|           3.13555|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        58|       0|       0.489| 0.561|  -7.933|   1|     0.0274|         0.2|         4.56E-5|   0.179|  0.238| 83.457|            4.0831|               0|               0|               1|               0|               0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|     0|     0|                   1|                   0|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        56|       0|       0.706| 0.112| -18.098|   1|     0.0391|       0.827|         4.03E-6|   0.125|  0.414|110.154|            2.1625|               0|               0|               0|               1|               0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        54|       0|       0.795|0.0841|  -18.09|   0|     0.0461|       0.742|         1.17E-5|  0.0853|  0.609| 91.803|            2.8288|               0|               0|               0|               1|               0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|     1|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        68|       0|       0.625| 0.414|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|3.1602166666666665|               0|               0|               0|               1|               0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "|        67|       0|       0.717|  0.32|  -8.393|   1|     0.0283|        0.83|             0.0|   0.107|  0.322|107.946| 4.336433333333333|               0|               0|               0|               1|               0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|     0|     0|                   0|                   1|                   0|                   0|                   0|                   0|            0|            0|            0|            0|            0|            0|            1|\n",
      "+----------+--------+------------+------+--------+----+-----------+------------+----------------+--------+-------+-------+------------------+----------------+----------------+----------------+----------------+----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove primary_artist and track_name from the dataset\n",
    "s_df = s_df.drop(\"primary_artist\", \"track_name\")\n",
    "s_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([22457.,  9279., 18354., 14910., 19205., 14532.,  9344.,  4269.,\n",
       "         1101.,    98.]),\n",
       " array([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check popularity values\n",
    "fig, ax = plt.subplots()\n",
    "hist(ax, s_df.select(\"popularity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>...</th>\n",
       "      <th>num_artists_binned_4</th>\n",
       "      <th>num_artists_binned_5</th>\n",
       "      <th>num_artists_binned_6</th>\n",
       "      <th>track_genre_0</th>\n",
       "      <th>track_genre_1</th>\n",
       "      <th>track_genre_2</th>\n",
       "      <th>track_genre_3</th>\n",
       "      <th>track_genre_4</th>\n",
       "      <th>track_genre_5</th>\n",
       "      <th>track_genre_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   popularity  explicit  danceability  energy  loudness  mode  speechiness   \n",
       "0          73         0         0.676  0.4610    -6.746     0       0.1430  \\\n",
       "1          55         0         0.420  0.1660   -17.235     1       0.0763   \n",
       "2          57         0         0.438  0.3590    -9.734     1       0.0557   \n",
       "3          71         0         0.266  0.0596   -18.515     1       0.0363   \n",
       "4          82         0         0.618  0.4430    -9.681     1       0.0526   \n",
       "\n",
       "   acousticness  instrumentalness  liveness  ...  num_artists_binned_4   \n",
       "0        0.0322          0.000001    0.3580  ...                     0  \\\n",
       "1        0.9240          0.000006    0.1010  ...                     0   \n",
       "2        0.2100          0.000000    0.1170  ...                     0   \n",
       "3        0.9050          0.000071    0.1320  ...                     0   \n",
       "4        0.4690          0.000000    0.0829  ...                     0   \n",
       "\n",
       "   num_artists_binned_5  num_artists_binned_6  track_genre_0  track_genre_1   \n",
       "0                     0                     0              0              0  \\\n",
       "1                     0                     0              0              0   \n",
       "2                     0                     0              0              0   \n",
       "3                     0                     0              0              0   \n",
       "4                     0                     0              0              0   \n",
       "\n",
       "   track_genre_2  track_genre_3  track_genre_4  track_genre_5  track_genre_6  \n",
       "0              0              0              0              0              1  \n",
       "1              0              0              0              0              1  \n",
       "2              0              0              0              0              1  \n",
       "3              0              0              0              0              1  \n",
       "4              0              0              0              0              1  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data to a pandas Dataframe\n",
    "df = s_df.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popularity\n",
       "0      15843\n",
       "1       2116\n",
       "2       1025\n",
       "3        570\n",
       "4        377\n",
       "       ...  \n",
       "96         7\n",
       "97         8\n",
       "98         7\n",
       "99         1\n",
       "100        2\n",
       "Name: count, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show value counts of popularity to help determine proper bins\n",
    "df['popularity'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity_4bins\n",
      "2    48631\n",
      "1    35514\n",
      "0    15843\n",
      "3    13561\n",
      "Name: count, dtype: int64\n",
      "popularity_3bins\n",
      "0    51357\n",
      "1    48631\n",
      "2    13561\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Bin the popularity into: 0, low=1 (0<pop<=30), medium=2 (30<pop<=60), high=3 (60<pop)\n",
    "# Set up a list of bins\n",
    "pop_4bins = [0 ,1, 2, 3]\n",
    "# Set up list of conditions\n",
    "pop_4conditions = [\n",
    "    (df[\"popularity\"] == 0), \n",
    "    (df[\"popularity\"] > 0) & (df[\"popularity\"] <= 30),\n",
    "    (df[\"popularity\"] > 30) & (df[\"popularity\"] <= 60),\n",
    "    (df[\"popularity\"] > 60)\n",
    "]\n",
    "# Set up the column with bins\n",
    "df[\"popularity_4bins\"] = np.select(pop_4conditions, pop_4bins)\n",
    "\n",
    "# Set up a list of 3 bins: low=0 (pop<=30), medium=1 (30<pop<=60), high=2 (60<pop)\n",
    "pop_3bins = [0, 1, 2]\n",
    "# Set up a list of conditions\n",
    "pop_3conditions = [\n",
    "    (df[\"popularity\"] <= 30),\n",
    "    (df[\"popularity\"] > 30) & (df[\"popularity\"] <= 60),\n",
    "    (df[\"popularity\"] > 60)\n",
    "]\n",
    "# Set up the column with bins\n",
    "df[\"popularity_3bins\"] = np.select(pop_3conditions, pop_3bins)\n",
    "\n",
    "# Confirm binning\n",
    "print(df['popularity_4bins'].value_counts())\n",
    "print(df['popularity_3bins'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>...</th>\n",
       "      <th>num_artists_binned_6</th>\n",
       "      <th>track_genre_0</th>\n",
       "      <th>track_genre_1</th>\n",
       "      <th>track_genre_2</th>\n",
       "      <th>track_genre_3</th>\n",
       "      <th>track_genre_4</th>\n",
       "      <th>track_genre_5</th>\n",
       "      <th>track_genre_6</th>\n",
       "      <th>popularity_4bins</th>\n",
       "      <th>popularity_3bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   explicit  danceability  energy  loudness  mode  speechiness  acousticness   \n",
       "0         0         0.676  0.4610    -6.746     0       0.1430        0.0322  \\\n",
       "1         0         0.420  0.1660   -17.235     1       0.0763        0.9240   \n",
       "2         0         0.438  0.3590    -9.734     1       0.0557        0.2100   \n",
       "3         0         0.266  0.0596   -18.515     1       0.0363        0.9050   \n",
       "4         0         0.618  0.4430    -9.681     1       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence  ...  num_artists_binned_6   \n",
       "0          0.000001    0.3580    0.715  ...                     0  \\\n",
       "1          0.000006    0.1010    0.267  ...                     0   \n",
       "2          0.000000    0.1170    0.120  ...                     0   \n",
       "3          0.000071    0.1320    0.143  ...                     0   \n",
       "4          0.000000    0.0829    0.167  ...                     0   \n",
       "\n",
       "   track_genre_0  track_genre_1  track_genre_2  track_genre_3  track_genre_4   \n",
       "0              0              0              0              0              0  \\\n",
       "1              0              0              0              0              0   \n",
       "2              0              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "\n",
       "   track_genre_5  track_genre_6  popularity_4bins  popularity_3bins  \n",
       "0              0              1                 3                 2  \n",
       "1              0              1                 2                 1  \n",
       "2              0              1                 2                 1  \n",
       "3              0              1                 3                 2  \n",
       "4              0              1                 3                 2  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the original popularity column\n",
    "df.drop(columns=\"popularity\", inplace=True)\n",
    "# Check removal\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate training and testing for the 4-bin model\n",
    "# Separate the prediction (song popularity) from the rest of the features\n",
    "y_4b = df.popularity_4bins.values\n",
    "X_4b = df.drop(columns=[\"popularity_4bins\", 'popularity_3bins'])\n",
    "\n",
    "# Split the training and testing datasets\n",
    "X_train_4b, X_test_4b, y_train_4b, y_test_4b = train_test_split(X_4b, y_4b, random_state=1, stratify=y_4b)\n",
    "\n",
    "# Set up one-hot encoding to encode popularity\n",
    "y_train_4b = to_categorical(y_train_4b, 4)\n",
    "y_test_4b = to_categorical(y_test_4b, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate training and testing for the 3-bin model\n",
    "# Separate the prediction (song popularity) from the rest of the features\n",
    "y_3b = df.popularity_3bins.values\n",
    "X_3b = df.drop(columns=[\"popularity_4bins\", 'popularity_3bins'])\n",
    "\n",
    "# Split the training and testing datasets\n",
    "X_train_3b, X_test_3b, y_train_3b, y_test_3b = train_test_split(X_3b, y_3b, random_state=1, stratify=y_3b)\n",
    "\n",
    "# Set up one-hot encoding to encode popularity\n",
    "y_train_3b = to_categorical(y_train_3b, 3)\n",
    "y_test_3b = to_categorical(y_test_3b, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for dictionaries to hold the summary and accuracy for each iteration\n",
    "iterations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,694\n",
      "Trainable params: 7,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2662/2662 [==============================] - 2s 586us/step - loss: 1.0838 - accuracy: 0.5418\n",
      "Epoch 2/100\n",
      "2662/2662 [==============================] - 2s 585us/step - loss: 0.9186 - accuracy: 0.6304\n",
      "Epoch 3/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.8528 - accuracy: 0.6571\n",
      "Epoch 4/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.8269 - accuracy: 0.6653\n",
      "Epoch 5/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.8135 - accuracy: 0.6694\n",
      "Epoch 6/100\n",
      "2662/2662 [==============================] - 2s 580us/step - loss: 0.8048 - accuracy: 0.6728\n",
      "Epoch 7/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.7977 - accuracy: 0.6756\n",
      "Epoch 8/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.7914 - accuracy: 0.6787\n",
      "Epoch 9/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.7863 - accuracy: 0.6806\n",
      "Epoch 10/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.7818 - accuracy: 0.6821\n",
      "Epoch 11/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.7787 - accuracy: 0.6821\n",
      "Epoch 12/100\n",
      "2662/2662 [==============================] - 1s 542us/step - loss: 0.7753 - accuracy: 0.6851\n",
      "Epoch 13/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.7716 - accuracy: 0.6855\n",
      "Epoch 14/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7682 - accuracy: 0.6871\n",
      "Epoch 15/100\n",
      "2662/2662 [==============================] - 1s 544us/step - loss: 0.7655 - accuracy: 0.6883\n",
      "Epoch 16/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.7628 - accuracy: 0.6888\n",
      "Epoch 17/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.7601 - accuracy: 0.6907\n",
      "Epoch 18/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.7583 - accuracy: 0.6917\n",
      "Epoch 19/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.7559 - accuracy: 0.6921\n",
      "Epoch 20/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.7552 - accuracy: 0.6917\n",
      "Epoch 21/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.7522 - accuracy: 0.6927\n",
      "Epoch 22/100\n",
      "2662/2662 [==============================] - 2s 569us/step - loss: 0.7505 - accuracy: 0.6933\n",
      "Epoch 23/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.7497 - accuracy: 0.6940\n",
      "Epoch 24/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.7471 - accuracy: 0.6956\n",
      "Epoch 25/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.7463 - accuracy: 0.6966\n",
      "Epoch 26/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.7443 - accuracy: 0.6967\n",
      "Epoch 27/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7422 - accuracy: 0.6987\n",
      "Epoch 28/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.7423 - accuracy: 0.6976\n",
      "Epoch 29/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.7399 - accuracy: 0.6984\n",
      "Epoch 30/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.7392 - accuracy: 0.6981\n",
      "Epoch 31/100\n",
      "2662/2662 [==============================] - 2s 593us/step - loss: 0.7383 - accuracy: 0.6990\n",
      "Epoch 32/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.7368 - accuracy: 0.7014\n",
      "Epoch 33/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.7363 - accuracy: 0.6997\n",
      "Epoch 34/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.7350 - accuracy: 0.7002\n",
      "Epoch 35/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7339 - accuracy: 0.6997\n",
      "Epoch 36/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.7326 - accuracy: 0.7017\n",
      "Epoch 37/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.7328 - accuracy: 0.7005\n",
      "Epoch 38/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.7310 - accuracy: 0.7011\n",
      "Epoch 39/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.7304 - accuracy: 0.7029\n",
      "Epoch 40/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.7296 - accuracy: 0.7023\n",
      "Epoch 41/100\n",
      "2662/2662 [==============================] - 2s 589us/step - loss: 0.7285 - accuracy: 0.7026\n",
      "Epoch 42/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.7281 - accuracy: 0.7026\n",
      "Epoch 43/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.7269 - accuracy: 0.7030\n",
      "Epoch 44/100\n",
      "2662/2662 [==============================] - 2s 576us/step - loss: 0.7264 - accuracy: 0.7031\n",
      "Epoch 45/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.7252 - accuracy: 0.7037\n",
      "Epoch 46/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.7254 - accuracy: 0.7050\n",
      "Epoch 47/100\n",
      "2662/2662 [==============================] - 2s 591us/step - loss: 0.7241 - accuracy: 0.7053\n",
      "Epoch 48/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.7240 - accuracy: 0.7047\n",
      "Epoch 49/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.7231 - accuracy: 0.7054\n",
      "Epoch 50/100\n",
      "2662/2662 [==============================] - 2s 592us/step - loss: 0.7228 - accuracy: 0.7056\n",
      "Epoch 51/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.7220 - accuracy: 0.7062\n",
      "Epoch 52/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.7209 - accuracy: 0.7064\n",
      "Epoch 53/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.7204 - accuracy: 0.7064\n",
      "Epoch 54/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.7201 - accuracy: 0.7063\n",
      "Epoch 55/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.7194 - accuracy: 0.7072\n",
      "Epoch 56/100\n",
      "2662/2662 [==============================] - 1s 546us/step - loss: 0.7199 - accuracy: 0.7072\n",
      "Epoch 57/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7186 - accuracy: 0.7069\n",
      "Epoch 58/100\n",
      "2662/2662 [==============================] - 1s 546us/step - loss: 0.7185 - accuracy: 0.7072\n",
      "Epoch 59/100\n",
      "2662/2662 [==============================] - 1s 547us/step - loss: 0.7189 - accuracy: 0.7070\n",
      "Epoch 60/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7172 - accuracy: 0.7092\n",
      "Epoch 61/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.7169 - accuracy: 0.7085\n",
      "Epoch 62/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.7162 - accuracy: 0.7087\n",
      "Epoch 63/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.7157 - accuracy: 0.7087\n",
      "Epoch 64/100\n",
      "2662/2662 [==============================] - 1s 542us/step - loss: 0.7149 - accuracy: 0.7091\n",
      "Epoch 65/100\n",
      "2662/2662 [==============================] - 1s 550us/step - loss: 0.7145 - accuracy: 0.7084\n",
      "Epoch 66/100\n",
      "2662/2662 [==============================] - 1s 543us/step - loss: 0.7148 - accuracy: 0.7099\n",
      "Epoch 67/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.7134 - accuracy: 0.7103\n",
      "Epoch 68/100\n",
      "2662/2662 [==============================] - 1s 544us/step - loss: 0.7138 - accuracy: 0.7085\n",
      "Epoch 69/100\n",
      "2662/2662 [==============================] - 1s 546us/step - loss: 0.7140 - accuracy: 0.7082\n",
      "Epoch 70/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.7126 - accuracy: 0.7105\n",
      "Epoch 71/100\n",
      "2662/2662 [==============================] - 1s 545us/step - loss: 0.7132 - accuracy: 0.7092\n",
      "Epoch 72/100\n",
      "2662/2662 [==============================] - 1s 548us/step - loss: 0.7125 - accuracy: 0.7086\n",
      "Epoch 73/100\n",
      "2662/2662 [==============================] - 1s 542us/step - loss: 0.7117 - accuracy: 0.7099\n",
      "Epoch 74/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.7118 - accuracy: 0.7102\n",
      "Epoch 75/100\n",
      "2662/2662 [==============================] - 1s 543us/step - loss: 0.7115 - accuracy: 0.7111\n",
      "Epoch 76/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7113 - accuracy: 0.7109\n",
      "Epoch 77/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.7110 - accuracy: 0.7103\n",
      "Epoch 78/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.7110 - accuracy: 0.7102\n",
      "Epoch 79/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.7100 - accuracy: 0.7103\n",
      "Epoch 80/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.7102 - accuracy: 0.7114\n",
      "Epoch 81/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.7096 - accuracy: 0.7105\n",
      "Epoch 82/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.7093 - accuracy: 0.7103\n",
      "Epoch 83/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.7084 - accuracy: 0.7106\n",
      "Epoch 84/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.7077 - accuracy: 0.7127\n",
      "Epoch 85/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.7076 - accuracy: 0.7117\n",
      "Epoch 86/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.7079 - accuracy: 0.7125\n",
      "Epoch 87/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.7076 - accuracy: 0.7122\n",
      "Epoch 88/100\n",
      "2662/2662 [==============================] - 2s 577us/step - loss: 0.7070 - accuracy: 0.7118\n",
      "Epoch 89/100\n",
      "2662/2662 [==============================] - 1s 542us/step - loss: 0.7069 - accuracy: 0.7124\n",
      "Epoch 90/100\n",
      "2662/2662 [==============================] - 1s 542us/step - loss: 0.7069 - accuracy: 0.7118\n",
      "Epoch 91/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.7065 - accuracy: 0.7125\n",
      "Epoch 92/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7064 - accuracy: 0.7124\n",
      "Epoch 93/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.7054 - accuracy: 0.7122\n",
      "Epoch 94/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.7056 - accuracy: 0.7118\n",
      "Epoch 95/100\n",
      "2662/2662 [==============================] - 1s 548us/step - loss: 0.7057 - accuracy: 0.7138\n",
      "Epoch 96/100\n",
      "2662/2662 [==============================] - 1s 550us/step - loss: 0.7051 - accuracy: 0.7122\n",
      "Epoch 97/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.7055 - accuracy: 0.7124\n",
      "Epoch 98/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.7054 - accuracy: 0.7123\n",
      "Epoch 99/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.7050 - accuracy: 0.7131\n",
      "Epoch 100/100\n",
      "2662/2662 [==============================] - 2s 576us/step - loss: 0.7040 - accuracy: 0.7128\n",
      "888/888 - 0s - loss: 0.8320 - accuracy: 0.6680 - 452ms/epoch - 509us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 1: Standard Scaler normalization, 4 bins for popularity\n",
    "# Create a dictionary for iteration 1\n",
    "iteration_1 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_1[\"Iteration\"] = 1\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_1 = scaler1.fit(X_train_4b)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_1 = X_scaler_1.transform(X_train_4b)\n",
    "X_test_scaled_1 = X_scaler_1.transform(X_test_4b)\n",
    "\n",
    "# Define the model\n",
    "nn_1 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_1.add(tf.keras.layers.Dense(units=4, activation=\"softmax\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_1.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_1 = nn_1.fit(X_train_scaled_1, y_train_4b, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_1, model_accuracy_1 = nn_1.evaluate(X_test_scaled_1, y_test_4b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_1 dictionary\n",
    "iteration_1[\"Loss\"] = model_loss_1\n",
    "iteration_1[\"Accuracy\"] = model_accuracy_1\n",
    "\n",
    "# Add the iteration_1 dictionary to the iterations list\n",
    "iterations.append(iteration_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,643\n",
      "Trainable params: 7,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2662/2662 [==============================] - 2s 563us/step - loss: 0.8562 - accuracy: 0.5981\n",
      "Epoch 2/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.7343 - accuracy: 0.6805\n",
      "Epoch 3/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.6899 - accuracy: 0.6998\n",
      "Epoch 4/100\n",
      "2662/2662 [==============================] - 2s 569us/step - loss: 0.6697 - accuracy: 0.7087\n",
      "Epoch 5/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.6582 - accuracy: 0.7126\n",
      "Epoch 6/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.6504 - accuracy: 0.7161\n",
      "Epoch 7/100\n",
      "2662/2662 [==============================] - 2s 591us/step - loss: 0.6444 - accuracy: 0.7190\n",
      "Epoch 8/100\n",
      "2662/2662 [==============================] - 2s 569us/step - loss: 0.6397 - accuracy: 0.7220\n",
      "Epoch 9/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.6351 - accuracy: 0.7242\n",
      "Epoch 10/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.6316 - accuracy: 0.7259\n",
      "Epoch 11/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.6285 - accuracy: 0.7277\n",
      "Epoch 12/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.6263 - accuracy: 0.7280\n",
      "Epoch 13/100\n",
      "2662/2662 [==============================] - 1s 542us/step - loss: 0.6231 - accuracy: 0.7301\n",
      "Epoch 14/100\n",
      "2662/2662 [==============================] - 1s 543us/step - loss: 0.6212 - accuracy: 0.7310\n",
      "Epoch 15/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.6185 - accuracy: 0.7309\n",
      "Epoch 16/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.6165 - accuracy: 0.7326\n",
      "Epoch 17/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.6141 - accuracy: 0.7342\n",
      "Epoch 18/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.6119 - accuracy: 0.7338\n",
      "Epoch 19/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.6107 - accuracy: 0.7357\n",
      "Epoch 20/100\n",
      "2662/2662 [==============================] - 2s 571us/step - loss: 0.6089 - accuracy: 0.7356\n",
      "Epoch 21/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.6069 - accuracy: 0.7373\n",
      "Epoch 22/100\n",
      "2662/2662 [==============================] - 2s 569us/step - loss: 0.6059 - accuracy: 0.7384\n",
      "Epoch 23/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.6047 - accuracy: 0.7382\n",
      "Epoch 24/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.6029 - accuracy: 0.7388\n",
      "Epoch 25/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.6021 - accuracy: 0.7388\n",
      "Epoch 26/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.6010 - accuracy: 0.7403\n",
      "Epoch 27/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.6000 - accuracy: 0.7414\n",
      "Epoch 28/100\n",
      "2662/2662 [==============================] - 1s 551us/step - loss: 0.5984 - accuracy: 0.7415\n",
      "Epoch 29/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.5979 - accuracy: 0.7408\n",
      "Epoch 30/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.5962 - accuracy: 0.7420\n",
      "Epoch 31/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5954 - accuracy: 0.7441\n",
      "Epoch 32/100\n",
      "2662/2662 [==============================] - 2s 574us/step - loss: 0.5938 - accuracy: 0.7446\n",
      "Epoch 33/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5930 - accuracy: 0.7447\n",
      "Epoch 34/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.5920 - accuracy: 0.7453\n",
      "Epoch 35/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.5919 - accuracy: 0.7459\n",
      "Epoch 36/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.5899 - accuracy: 0.7463\n",
      "Epoch 37/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5897 - accuracy: 0.7457\n",
      "Epoch 38/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.5887 - accuracy: 0.7470\n",
      "Epoch 39/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5878 - accuracy: 0.7472\n",
      "Epoch 40/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5877 - accuracy: 0.7471\n",
      "Epoch 41/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5858 - accuracy: 0.7482\n",
      "Epoch 42/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.5851 - accuracy: 0.7488\n",
      "Epoch 43/100\n",
      "2662/2662 [==============================] - 2s 573us/step - loss: 0.5847 - accuracy: 0.7489\n",
      "Epoch 44/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.5838 - accuracy: 0.7493\n",
      "Epoch 45/100\n",
      "2662/2662 [==============================] - 2s 574us/step - loss: 0.5833 - accuracy: 0.7496\n",
      "Epoch 46/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.5826 - accuracy: 0.7506\n",
      "Epoch 47/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.5821 - accuracy: 0.7492\n",
      "Epoch 48/100\n",
      "2662/2662 [==============================] - 2s 569us/step - loss: 0.5809 - accuracy: 0.7507\n",
      "Epoch 49/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.5808 - accuracy: 0.7502\n",
      "Epoch 50/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.5798 - accuracy: 0.7503\n",
      "Epoch 51/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5788 - accuracy: 0.7515\n",
      "Epoch 52/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5792 - accuracy: 0.7512\n",
      "Epoch 53/100\n",
      "2662/2662 [==============================] - 2s 571us/step - loss: 0.5787 - accuracy: 0.7519\n",
      "Epoch 54/100\n",
      "2662/2662 [==============================] - 2s 563us/step - loss: 0.5773 - accuracy: 0.7523\n",
      "Epoch 55/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.5776 - accuracy: 0.7522\n",
      "Epoch 56/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5763 - accuracy: 0.7534\n",
      "Epoch 57/100\n",
      "2662/2662 [==============================] - 2s 571us/step - loss: 0.5757 - accuracy: 0.7528\n",
      "Epoch 58/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.5748 - accuracy: 0.7536\n",
      "Epoch 59/100\n",
      "2662/2662 [==============================] - 2s 569us/step - loss: 0.5744 - accuracy: 0.7529\n",
      "Epoch 60/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.5744 - accuracy: 0.7535\n",
      "Epoch 61/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5731 - accuracy: 0.7553\n",
      "Epoch 62/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.5734 - accuracy: 0.7537\n",
      "Epoch 63/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.5736 - accuracy: 0.7541\n",
      "Epoch 64/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5729 - accuracy: 0.7540\n",
      "Epoch 65/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5719 - accuracy: 0.7548\n",
      "Epoch 66/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5717 - accuracy: 0.7543\n",
      "Epoch 67/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.5716 - accuracy: 0.7538\n",
      "Epoch 68/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.5709 - accuracy: 0.7553\n",
      "Epoch 69/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.5702 - accuracy: 0.7557\n",
      "Epoch 70/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5697 - accuracy: 0.7554\n",
      "Epoch 71/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5693 - accuracy: 0.7562\n",
      "Epoch 72/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.5689 - accuracy: 0.7565\n",
      "Epoch 73/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.5689 - accuracy: 0.7555\n",
      "Epoch 74/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5684 - accuracy: 0.7552\n",
      "Epoch 75/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.5678 - accuracy: 0.7557\n",
      "Epoch 76/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5680 - accuracy: 0.7564\n",
      "Epoch 77/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.5678 - accuracy: 0.7566\n",
      "Epoch 78/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5668 - accuracy: 0.7583\n",
      "Epoch 79/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5663 - accuracy: 0.7554\n",
      "Epoch 80/100\n",
      "2662/2662 [==============================] - 1s 547us/step - loss: 0.5664 - accuracy: 0.7571\n",
      "Epoch 81/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5658 - accuracy: 0.7576\n",
      "Epoch 82/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5656 - accuracy: 0.7579\n",
      "Epoch 83/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5652 - accuracy: 0.7582\n",
      "Epoch 84/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5652 - accuracy: 0.7579\n",
      "Epoch 85/100\n",
      "2662/2662 [==============================] - 2s 563us/step - loss: 0.5651 - accuracy: 0.7591\n",
      "Epoch 86/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.5649 - accuracy: 0.7579\n",
      "Epoch 87/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5643 - accuracy: 0.7588\n",
      "Epoch 88/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.5642 - accuracy: 0.7582\n",
      "Epoch 89/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5635 - accuracy: 0.7592\n",
      "Epoch 90/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.5636 - accuracy: 0.7593\n",
      "Epoch 91/100\n",
      "2662/2662 [==============================] - 1s 543us/step - loss: 0.5631 - accuracy: 0.7583\n",
      "Epoch 92/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.5633 - accuracy: 0.7576\n",
      "Epoch 93/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5626 - accuracy: 0.7595\n",
      "Epoch 94/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5625 - accuracy: 0.7581\n",
      "Epoch 95/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.5623 - accuracy: 0.7588\n",
      "Epoch 96/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5622 - accuracy: 0.7589\n",
      "Epoch 97/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5621 - accuracy: 0.7589\n",
      "Epoch 98/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.5615 - accuracy: 0.7597\n",
      "Epoch 99/100\n",
      "2662/2662 [==============================] - 1s 546us/step - loss: 0.5609 - accuracy: 0.7599\n",
      "Epoch 100/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5620 - accuracy: 0.7570\n",
      "888/888 - 0s - loss: 0.6908 - accuracy: 0.7067 - 405ms/epoch - 457us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 2: Standard Scaler normalization, 3 bins for popularity\n",
    "# Create a dictionary for iteration 2\n",
    "iteration_2 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_2[\"Iteration\"] = 2\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_2 = scaler2.fit(X_train_3b)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_2 = X_scaler_2.transform(X_train_3b)\n",
    "X_test_scaled_2 = X_scaler_2.transform(X_test_3b)\n",
    "\n",
    "# Define the model\n",
    "nn_2 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_2.add(tf.keras.layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_2.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_2 = nn_2.fit(X_train_scaled_2, y_train_3b, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_2, model_accuracy_2 = nn_2.evaluate(X_test_scaled_2, y_test_3b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_2 dictionary\n",
    "iteration_2[\"Loss\"] = model_loss_2\n",
    "iteration_2[\"Accuracy\"] = model_accuracy_2\n",
    "\n",
    "# Add the iteration_2 dictionary to the iterations list\n",
    "iterations.append(iteration_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,643\n",
      "Trainable params: 7,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.8545 - accuracy: 0.5995\n",
      "Epoch 2/100\n",
      "2662/2662 [==============================] - 2s 576us/step - loss: 0.7382 - accuracy: 0.6785\n",
      "Epoch 3/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.6993 - accuracy: 0.6952\n",
      "Epoch 4/100\n",
      "2662/2662 [==============================] - 2s 577us/step - loss: 0.6816 - accuracy: 0.7011\n",
      "Epoch 5/100\n",
      "2662/2662 [==============================] - 2s 584us/step - loss: 0.6716 - accuracy: 0.7048\n",
      "Epoch 6/100\n",
      "2662/2662 [==============================] - 2s 575us/step - loss: 0.6648 - accuracy: 0.7073\n",
      "Epoch 7/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.6585 - accuracy: 0.7113\n",
      "Epoch 8/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.6541 - accuracy: 0.7129\n",
      "Epoch 9/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.6505 - accuracy: 0.7128\n",
      "Epoch 10/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.6475 - accuracy: 0.7155\n",
      "Epoch 11/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.6442 - accuracy: 0.7168\n",
      "Epoch 12/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.6411 - accuracy: 0.7190\n",
      "Epoch 13/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.6390 - accuracy: 0.7196\n",
      "Epoch 14/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.6363 - accuracy: 0.7207\n",
      "Epoch 15/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.6341 - accuracy: 0.7221\n",
      "Epoch 16/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.6318 - accuracy: 0.7231\n",
      "Epoch 17/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.6309 - accuracy: 0.7230\n",
      "Epoch 18/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.6282 - accuracy: 0.7255\n",
      "Epoch 19/100\n",
      "2662/2662 [==============================] - 2s 588us/step - loss: 0.6273 - accuracy: 0.7241\n",
      "Epoch 20/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.6248 - accuracy: 0.7256\n",
      "Epoch 21/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.6247 - accuracy: 0.7269\n",
      "Epoch 22/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.6227 - accuracy: 0.7275\n",
      "Epoch 23/100\n",
      "2662/2662 [==============================] - 2s 574us/step - loss: 0.6219 - accuracy: 0.7264\n",
      "Epoch 24/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.6199 - accuracy: 0.7288\n",
      "Epoch 25/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.6187 - accuracy: 0.7296\n",
      "Epoch 26/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.6172 - accuracy: 0.7314\n",
      "Epoch 27/100\n",
      "2662/2662 [==============================] - 2s 571us/step - loss: 0.6164 - accuracy: 0.7307\n",
      "Epoch 28/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.6150 - accuracy: 0.7310\n",
      "Epoch 29/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.6145 - accuracy: 0.7316\n",
      "Epoch 30/100\n",
      "2662/2662 [==============================] - 2s 577us/step - loss: 0.6131 - accuracy: 0.7327\n",
      "Epoch 31/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.6120 - accuracy: 0.7334\n",
      "Epoch 32/100\n",
      "2662/2662 [==============================] - 2s 575us/step - loss: 0.6118 - accuracy: 0.7331\n",
      "Epoch 33/100\n",
      "2662/2662 [==============================] - 1s 552us/step - loss: 0.6097 - accuracy: 0.7342\n",
      "Epoch 34/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.6099 - accuracy: 0.7337\n",
      "Epoch 35/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.6085 - accuracy: 0.7347\n",
      "Epoch 36/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.6074 - accuracy: 0.7349\n",
      "Epoch 37/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.6069 - accuracy: 0.7365\n",
      "Epoch 38/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.6056 - accuracy: 0.7360\n",
      "Epoch 39/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.6054 - accuracy: 0.7371\n",
      "Epoch 40/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.6038 - accuracy: 0.7369\n",
      "Epoch 41/100\n",
      "2662/2662 [==============================] - 1s 549us/step - loss: 0.6030 - accuracy: 0.7373\n",
      "Epoch 42/100\n",
      "2662/2662 [==============================] - 1s 550us/step - loss: 0.6023 - accuracy: 0.7389\n",
      "Epoch 43/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.6016 - accuracy: 0.7379\n",
      "Epoch 44/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.6012 - accuracy: 0.7378\n",
      "Epoch 45/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.6005 - accuracy: 0.7385\n",
      "Epoch 46/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.5994 - accuracy: 0.7392\n",
      "Epoch 47/100\n",
      "2662/2662 [==============================] - 2s 585us/step - loss: 0.5989 - accuracy: 0.7394\n",
      "Epoch 48/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5982 - accuracy: 0.7401\n",
      "Epoch 49/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5972 - accuracy: 0.7400\n",
      "Epoch 50/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5971 - accuracy: 0.7397\n",
      "Epoch 51/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5959 - accuracy: 0.7415\n",
      "Epoch 52/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5950 - accuracy: 0.7420\n",
      "Epoch 53/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.5950 - accuracy: 0.7412\n",
      "Epoch 54/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.5937 - accuracy: 0.7420\n",
      "Epoch 55/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.5941 - accuracy: 0.7415\n",
      "Epoch 56/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5935 - accuracy: 0.7409\n",
      "Epoch 57/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.5922 - accuracy: 0.7423\n",
      "Epoch 58/100\n",
      "2662/2662 [==============================] - 2s 571us/step - loss: 0.5918 - accuracy: 0.7438\n",
      "Epoch 59/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5911 - accuracy: 0.7442\n",
      "Epoch 60/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.5908 - accuracy: 0.7442\n",
      "Epoch 61/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5907 - accuracy: 0.7430\n",
      "Epoch 62/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5895 - accuracy: 0.7449\n",
      "Epoch 63/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.5897 - accuracy: 0.7438\n",
      "Epoch 64/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5887 - accuracy: 0.7441\n",
      "Epoch 65/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5885 - accuracy: 0.7448\n",
      "Epoch 66/100\n",
      "2662/2662 [==============================] - 1s 548us/step - loss: 0.5882 - accuracy: 0.7449\n",
      "Epoch 67/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5878 - accuracy: 0.7460\n",
      "Epoch 68/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5872 - accuracy: 0.7449\n",
      "Epoch 69/100\n",
      "2662/2662 [==============================] - 1s 553us/step - loss: 0.5868 - accuracy: 0.7460\n",
      "Epoch 70/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.5869 - accuracy: 0.7460\n",
      "Epoch 71/100\n",
      "2662/2662 [==============================] - 1s 555us/step - loss: 0.5860 - accuracy: 0.7462\n",
      "Epoch 72/100\n",
      "2662/2662 [==============================] - 1s 558us/step - loss: 0.5851 - accuracy: 0.7462\n",
      "Epoch 73/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.5849 - accuracy: 0.7449\n",
      "Epoch 74/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5846 - accuracy: 0.7462\n",
      "Epoch 75/100\n",
      "2662/2662 [==============================] - 1s 556us/step - loss: 0.5838 - accuracy: 0.7457\n",
      "Epoch 76/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.5843 - accuracy: 0.7466\n",
      "Epoch 77/100\n",
      "2662/2662 [==============================] - 2s 603us/step - loss: 0.5838 - accuracy: 0.7471\n",
      "Epoch 78/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5836 - accuracy: 0.7471\n",
      "Epoch 79/100\n",
      "2662/2662 [==============================] - 1s 557us/step - loss: 0.5825 - accuracy: 0.7475\n",
      "Epoch 80/100\n",
      "2662/2662 [==============================] - 2s 565us/step - loss: 0.5825 - accuracy: 0.7477\n",
      "Epoch 81/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5822 - accuracy: 0.7483\n",
      "Epoch 82/100\n",
      "2662/2662 [==============================] - 2s 572us/step - loss: 0.5820 - accuracy: 0.7487\n",
      "Epoch 83/100\n",
      "2662/2662 [==============================] - 1s 563us/step - loss: 0.5810 - accuracy: 0.7481\n",
      "Epoch 84/100\n",
      "2662/2662 [==============================] - 2s 567us/step - loss: 0.5807 - accuracy: 0.7494\n",
      "Epoch 85/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5813 - accuracy: 0.7475\n",
      "Epoch 86/100\n",
      "2662/2662 [==============================] - 2s 564us/step - loss: 0.5801 - accuracy: 0.7489\n",
      "Epoch 87/100\n",
      "2662/2662 [==============================] - 1s 559us/step - loss: 0.5801 - accuracy: 0.7485\n",
      "Epoch 88/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5800 - accuracy: 0.7490\n",
      "Epoch 89/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5790 - accuracy: 0.7507\n",
      "Epoch 90/100\n",
      "2662/2662 [==============================] - 1s 561us/step - loss: 0.5791 - accuracy: 0.7493\n",
      "Epoch 91/100\n",
      "2662/2662 [==============================] - 2s 570us/step - loss: 0.5789 - accuracy: 0.7501\n",
      "Epoch 92/100\n",
      "2662/2662 [==============================] - 1s 560us/step - loss: 0.5778 - accuracy: 0.7501\n",
      "Epoch 93/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.5778 - accuracy: 0.7497\n",
      "Epoch 94/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5779 - accuracy: 0.7501\n",
      "Epoch 95/100\n",
      "2662/2662 [==============================] - 2s 571us/step - loss: 0.5778 - accuracy: 0.7511\n",
      "Epoch 96/100\n",
      "2662/2662 [==============================] - 1s 562us/step - loss: 0.5762 - accuracy: 0.7511\n",
      "Epoch 97/100\n",
      "2662/2662 [==============================] - 2s 568us/step - loss: 0.5766 - accuracy: 0.7499\n",
      "Epoch 98/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.5766 - accuracy: 0.7494\n",
      "Epoch 99/100\n",
      "2662/2662 [==============================] - 2s 566us/step - loss: 0.5755 - accuracy: 0.7509\n",
      "Epoch 100/100\n",
      "2662/2662 [==============================] - 1s 554us/step - loss: 0.5761 - accuracy: 0.7505\n",
      "888/888 - 0s - loss: 0.6803 - accuracy: 0.7121 - 414ms/epoch - 466us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 3: Same as above, but MinMax normalization\n",
    "# Create a dictionary for iteration 3\n",
    "iteration_3 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_3[\"Iteration\"] = 3\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler3 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_3 = scaler3.fit(X_train_3b)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_3 = X_scaler_3.transform(X_train_3b)\n",
    "X_test_scaled_3 = X_scaler_3.transform(X_test_3b)\n",
    "\n",
    "# Define the model\n",
    "nn_3 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_3.add(tf.keras.layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_3.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_3 = nn_3.fit(X_train_scaled_3, y_train_3b, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_3, model_accuracy_3 = nn_3.evaluate(X_test_scaled_3, y_test_3b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_3 dictionary\n",
    "iteration_3[\"Loss\"] = model_loss_3\n",
    "iteration_3[\"Accuracy\"] = model_accuracy_3\n",
    "\n",
    "# Add the iteration_3 dictionary to the iterations list\n",
    "iterations.append(iteration_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,643\n",
      "Trainable params: 7,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3611/3611 [==============================] - 2s 561us/step - loss: 0.9141 - accuracy: 0.5624\n",
      "Epoch 2/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.7820 - accuracy: 0.6484\n",
      "Epoch 3/100\n",
      "3611/3611 [==============================] - 2s 559us/step - loss: 0.7517 - accuracy: 0.6620\n",
      "Epoch 4/100\n",
      "3611/3611 [==============================] - 2s 560us/step - loss: 0.7378 - accuracy: 0.6663\n",
      "Epoch 5/100\n",
      "3611/3611 [==============================] - 2s 565us/step - loss: 0.7279 - accuracy: 0.6724\n",
      "Epoch 6/100\n",
      "3611/3611 [==============================] - 2s 559us/step - loss: 0.7206 - accuracy: 0.6767\n",
      "Epoch 7/100\n",
      "3611/3611 [==============================] - 2s 560us/step - loss: 0.7146 - accuracy: 0.6810\n",
      "Epoch 8/100\n",
      "3611/3611 [==============================] - 2s 555us/step - loss: 0.7084 - accuracy: 0.6829\n",
      "Epoch 9/100\n",
      "3611/3611 [==============================] - 2s 558us/step - loss: 0.7033 - accuracy: 0.6852\n",
      "Epoch 10/100\n",
      "3611/3611 [==============================] - 2s 561us/step - loss: 0.6996 - accuracy: 0.6872\n",
      "Epoch 11/100\n",
      "3611/3611 [==============================] - 2s 564us/step - loss: 0.6952 - accuracy: 0.6894\n",
      "Epoch 12/100\n",
      "3611/3611 [==============================] - 2s 560us/step - loss: 0.6907 - accuracy: 0.6933\n",
      "Epoch 13/100\n",
      "3611/3611 [==============================] - 2s 565us/step - loss: 0.6866 - accuracy: 0.6955\n",
      "Epoch 14/100\n",
      "3611/3611 [==============================] - 2s 560us/step - loss: 0.6830 - accuracy: 0.6985\n",
      "Epoch 15/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6805 - accuracy: 0.6980\n",
      "Epoch 16/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6767 - accuracy: 0.7015\n",
      "Epoch 17/100\n",
      "3611/3611 [==============================] - 2s 549us/step - loss: 0.6739 - accuracy: 0.7025\n",
      "Epoch 18/100\n",
      "3611/3611 [==============================] - 2s 560us/step - loss: 0.6712 - accuracy: 0.7030\n",
      "Epoch 19/100\n",
      "3611/3611 [==============================] - 2s 558us/step - loss: 0.6684 - accuracy: 0.7039\n",
      "Epoch 20/100\n",
      "3611/3611 [==============================] - 2s 564us/step - loss: 0.6662 - accuracy: 0.7067\n",
      "Epoch 21/100\n",
      "3611/3611 [==============================] - 2s 559us/step - loss: 0.6643 - accuracy: 0.7076\n",
      "Epoch 22/100\n",
      "3611/3611 [==============================] - 2s 561us/step - loss: 0.6614 - accuracy: 0.7083\n",
      "Epoch 23/100\n",
      "3611/3611 [==============================] - 2s 554us/step - loss: 0.6597 - accuracy: 0.7097\n",
      "Epoch 24/100\n",
      "3611/3611 [==============================] - 2s 562us/step - loss: 0.6576 - accuracy: 0.7119\n",
      "Epoch 25/100\n",
      "3611/3611 [==============================] - 2s 561us/step - loss: 0.6557 - accuracy: 0.7119\n",
      "Epoch 26/100\n",
      "3611/3611 [==============================] - 2s 561us/step - loss: 0.6534 - accuracy: 0.7134\n",
      "Epoch 27/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.6520 - accuracy: 0.7143\n",
      "Epoch 28/100\n",
      "3611/3611 [==============================] - 2s 565us/step - loss: 0.6501 - accuracy: 0.7158\n",
      "Epoch 29/100\n",
      "3611/3611 [==============================] - 2s 563us/step - loss: 0.6483 - accuracy: 0.7171\n",
      "Epoch 30/100\n",
      "3611/3611 [==============================] - 2s 566us/step - loss: 0.6473 - accuracy: 0.7173\n",
      "Epoch 31/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.6455 - accuracy: 0.7179\n",
      "Epoch 32/100\n",
      "3611/3611 [==============================] - 2s 558us/step - loss: 0.6437 - accuracy: 0.7186\n",
      "Epoch 33/100\n",
      "3611/3611 [==============================] - 2s 560us/step - loss: 0.6421 - accuracy: 0.7190\n",
      "Epoch 34/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.6403 - accuracy: 0.7206\n",
      "Epoch 35/100\n",
      "3611/3611 [==============================] - 2s 554us/step - loss: 0.6392 - accuracy: 0.7222\n",
      "Epoch 36/100\n",
      "3611/3611 [==============================] - 2s 552us/step - loss: 0.6377 - accuracy: 0.7233\n",
      "Epoch 37/100\n",
      "3611/3611 [==============================] - 2s 560us/step - loss: 0.6374 - accuracy: 0.7226\n",
      "Epoch 38/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.6356 - accuracy: 0.7230\n",
      "Epoch 39/100\n",
      "3611/3611 [==============================] - 2s 556us/step - loss: 0.6337 - accuracy: 0.7250\n",
      "Epoch 40/100\n",
      "3611/3611 [==============================] - 2s 552us/step - loss: 0.6328 - accuracy: 0.7246\n",
      "Epoch 41/100\n",
      "3611/3611 [==============================] - 2s 546us/step - loss: 0.6308 - accuracy: 0.7263\n",
      "Epoch 42/100\n",
      "3611/3611 [==============================] - 2s 539us/step - loss: 0.6302 - accuracy: 0.7273\n",
      "Epoch 43/100\n",
      "3611/3611 [==============================] - 2s 552us/step - loss: 0.6287 - accuracy: 0.7276\n",
      "Epoch 44/100\n",
      "3611/3611 [==============================] - 2s 547us/step - loss: 0.6285 - accuracy: 0.7283\n",
      "Epoch 45/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6271 - accuracy: 0.7295\n",
      "Epoch 46/100\n",
      "3611/3611 [==============================] - 2s 545us/step - loss: 0.6263 - accuracy: 0.7296\n",
      "Epoch 47/100\n",
      "3611/3611 [==============================] - 2s 547us/step - loss: 0.6254 - accuracy: 0.7297\n",
      "Epoch 48/100\n",
      "3611/3611 [==============================] - 2s 549us/step - loss: 0.6243 - accuracy: 0.7289\n",
      "Epoch 49/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6232 - accuracy: 0.7306\n",
      "Epoch 50/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6224 - accuracy: 0.7310\n",
      "Epoch 51/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.6223 - accuracy: 0.7318\n",
      "Epoch 52/100\n",
      "3611/3611 [==============================] - 2s 556us/step - loss: 0.6214 - accuracy: 0.7316\n",
      "Epoch 53/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6202 - accuracy: 0.7328\n",
      "Epoch 54/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.6191 - accuracy: 0.7327\n",
      "Epoch 55/100\n",
      "3611/3611 [==============================] - 2s 548us/step - loss: 0.6185 - accuracy: 0.7341\n",
      "Epoch 56/100\n",
      "3611/3611 [==============================] - 2s 552us/step - loss: 0.6178 - accuracy: 0.7335\n",
      "Epoch 57/100\n",
      "3611/3611 [==============================] - 2s 543us/step - loss: 0.6172 - accuracy: 0.7345\n",
      "Epoch 58/100\n",
      "3611/3611 [==============================] - 2s 548us/step - loss: 0.6159 - accuracy: 0.7355\n",
      "Epoch 59/100\n",
      "3611/3611 [==============================] - 2s 554us/step - loss: 0.6160 - accuracy: 0.7341\n",
      "Epoch 60/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6148 - accuracy: 0.7358\n",
      "Epoch 61/100\n",
      "3611/3611 [==============================] - 2s 548us/step - loss: 0.6144 - accuracy: 0.7355\n",
      "Epoch 62/100\n",
      "3611/3611 [==============================] - 2s 558us/step - loss: 0.6131 - accuracy: 0.7370\n",
      "Epoch 63/100\n",
      "3611/3611 [==============================] - 2s 560us/step - loss: 0.6131 - accuracy: 0.7358\n",
      "Epoch 64/100\n",
      "3611/3611 [==============================] - 2s 562us/step - loss: 0.6126 - accuracy: 0.7373\n",
      "Epoch 65/100\n",
      "3611/3611 [==============================] - 2s 556us/step - loss: 0.6118 - accuracy: 0.7369\n",
      "Epoch 66/100\n",
      "3611/3611 [==============================] - 2s 561us/step - loss: 0.6106 - accuracy: 0.7385\n",
      "Epoch 67/100\n",
      "3611/3611 [==============================] - 2s 555us/step - loss: 0.6108 - accuracy: 0.7377\n",
      "Epoch 68/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6093 - accuracy: 0.7377\n",
      "Epoch 69/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6095 - accuracy: 0.7388\n",
      "Epoch 70/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.6084 - accuracy: 0.7386\n",
      "Epoch 71/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6081 - accuracy: 0.7400\n",
      "Epoch 72/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6076 - accuracy: 0.7387\n",
      "Epoch 73/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6066 - accuracy: 0.7390\n",
      "Epoch 74/100\n",
      "3611/3611 [==============================] - 2s 548us/step - loss: 0.6070 - accuracy: 0.7401\n",
      "Epoch 75/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6060 - accuracy: 0.7405\n",
      "Epoch 76/100\n",
      "3611/3611 [==============================] - 2s 554us/step - loss: 0.6052 - accuracy: 0.7411\n",
      "Epoch 77/100\n",
      "3611/3611 [==============================] - 2s 552us/step - loss: 0.6050 - accuracy: 0.7422\n",
      "Epoch 78/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6048 - accuracy: 0.7416\n",
      "Epoch 79/100\n",
      "3611/3611 [==============================] - 2s 542us/step - loss: 0.6044 - accuracy: 0.7411\n",
      "Epoch 80/100\n",
      "3611/3611 [==============================] - 2s 545us/step - loss: 0.6029 - accuracy: 0.7418\n",
      "Epoch 81/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6035 - accuracy: 0.7420\n",
      "Epoch 82/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6025 - accuracy: 0.7425\n",
      "Epoch 83/100\n",
      "3611/3611 [==============================] - 2s 549us/step - loss: 0.6021 - accuracy: 0.7427\n",
      "Epoch 84/100\n",
      "3611/3611 [==============================] - 2s 556us/step - loss: 0.6015 - accuracy: 0.7434\n",
      "Epoch 85/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.6012 - accuracy: 0.7428\n",
      "Epoch 86/100\n",
      "3611/3611 [==============================] - 2s 570us/step - loss: 0.6010 - accuracy: 0.7428\n",
      "Epoch 87/100\n",
      "3611/3611 [==============================] - 2s 574us/step - loss: 0.6004 - accuracy: 0.7435\n",
      "Epoch 88/100\n",
      "3611/3611 [==============================] - 2s 568us/step - loss: 0.5998 - accuracy: 0.7434\n",
      "Epoch 89/100\n",
      "3611/3611 [==============================] - 2s 562us/step - loss: 0.5994 - accuracy: 0.7436\n",
      "Epoch 90/100\n",
      "3611/3611 [==============================] - 2s 554us/step - loss: 0.5991 - accuracy: 0.7438\n",
      "Epoch 91/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.5980 - accuracy: 0.7441\n",
      "Epoch 92/100\n",
      "3611/3611 [==============================] - 2s 555us/step - loss: 0.5978 - accuracy: 0.7448\n",
      "Epoch 93/100\n",
      "3611/3611 [==============================] - 2s 543us/step - loss: 0.5976 - accuracy: 0.7444\n",
      "Epoch 94/100\n",
      "3611/3611 [==============================] - 2s 552us/step - loss: 0.5977 - accuracy: 0.7445\n",
      "Epoch 95/100\n",
      "3611/3611 [==============================] - 2s 548us/step - loss: 0.5966 - accuracy: 0.7445\n",
      "Epoch 96/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.5968 - accuracy: 0.7451\n",
      "Epoch 97/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.5955 - accuracy: 0.7468\n",
      "Epoch 98/100\n",
      "3611/3611 [==============================] - 2s 547us/step - loss: 0.5951 - accuracy: 0.7469\n",
      "Epoch 99/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.5950 - accuracy: 0.7457\n",
      "Epoch 100/100\n",
      "3611/3611 [==============================] - 2s 554us/step - loss: 0.5947 - accuracy: 0.7455\n",
      "888/888 - 0s - loss: 0.8134 - accuracy: 0.6460 - 432ms/epoch - 487us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 4: MinMaxScaler normalization with random oversampling\n",
    "# Create a dictionary for iteration 4\n",
    "iteration_4 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_4[\"Iteration\"] = 4\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler4 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_4 = scaler4.fit(X_train_3b)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_4 = X_scaler_4.transform(X_train_3b)\n",
    "X_test_scaled_4 = X_scaler_4.transform(X_test_3b)\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_4 = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_4, y_ros_4 = ros_4.fit_resample(X_train_scaled_4, y_train_3b)\n",
    "\n",
    "# Define the model\n",
    "nn_4 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_4.add(tf.keras.layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_4.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_4.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_4 = nn_4.fit(X_ros_4, y_ros_4, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_4, model_accuracy_4 = nn_4.evaluate(X_test_scaled_4, y_test_3b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_4 dictionary\n",
    "iteration_4[\"Loss\"] = model_loss_4\n",
    "iteration_4[\"Accuracy\"] = model_accuracy_4\n",
    "\n",
    "# Add the iteration_4 dictionary to the iterations list\n",
    "iterations.append(iteration_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 80)                3440      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,643\n",
      "Trainable params: 7,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3611/3611 [==============================] - 2s 576us/step - loss: 0.9155 - accuracy: 0.5589\n",
      "Epoch 2/100\n",
      "3611/3611 [==============================] - 2s 600us/step - loss: 0.7886 - accuracy: 0.6432\n",
      "Epoch 3/100\n",
      "3611/3611 [==============================] - 2s 576us/step - loss: 0.7552 - accuracy: 0.6590\n",
      "Epoch 4/100\n",
      "3611/3611 [==============================] - 2s 585us/step - loss: 0.7409 - accuracy: 0.6657\n",
      "Epoch 5/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.7309 - accuracy: 0.6707\n",
      "Epoch 6/100\n",
      "3611/3611 [==============================] - 2s 571us/step - loss: 0.7242 - accuracy: 0.6727\n",
      "Epoch 7/100\n",
      "3611/3611 [==============================] - 2s 559us/step - loss: 0.7176 - accuracy: 0.6764\n",
      "Epoch 8/100\n",
      "3611/3611 [==============================] - 2s 558us/step - loss: 0.7132 - accuracy: 0.6782\n",
      "Epoch 9/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.7081 - accuracy: 0.6815\n",
      "Epoch 10/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.7031 - accuracy: 0.6848\n",
      "Epoch 11/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6988 - accuracy: 0.6871\n",
      "Epoch 12/100\n",
      "3611/3611 [==============================] - 2s 556us/step - loss: 0.6958 - accuracy: 0.6883\n",
      "Epoch 13/100\n",
      "3611/3611 [==============================] - 2s 558us/step - loss: 0.6913 - accuracy: 0.6895\n",
      "Epoch 14/100\n",
      "3611/3611 [==============================] - 2s 561us/step - loss: 0.6884 - accuracy: 0.6919\n",
      "Epoch 15/100\n",
      "3611/3611 [==============================] - 2s 556us/step - loss: 0.6846 - accuracy: 0.6956\n",
      "Epoch 16/100\n",
      "3611/3611 [==============================] - 2s 557us/step - loss: 0.6813 - accuracy: 0.6970\n",
      "Epoch 17/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6783 - accuracy: 0.6994\n",
      "Epoch 18/100\n",
      "3611/3611 [==============================] - 2s 562us/step - loss: 0.6752 - accuracy: 0.7003\n",
      "Epoch 19/100\n",
      "3611/3611 [==============================] - 2s 563us/step - loss: 0.6738 - accuracy: 0.7010\n",
      "Epoch 20/100\n",
      "3611/3611 [==============================] - 2s 572us/step - loss: 0.6717 - accuracy: 0.7019\n",
      "Epoch 21/100\n",
      "3611/3611 [==============================] - 2s 573us/step - loss: 0.6682 - accuracy: 0.7049\n",
      "Epoch 22/100\n",
      "3611/3611 [==============================] - 2s 567us/step - loss: 0.6668 - accuracy: 0.7058\n",
      "Epoch 23/100\n",
      "3611/3611 [==============================] - 2s 564us/step - loss: 0.6641 - accuracy: 0.7063\n",
      "Epoch 24/100\n",
      "3611/3611 [==============================] - 2s 569us/step - loss: 0.6620 - accuracy: 0.7077\n",
      "Epoch 25/100\n",
      "3611/3611 [==============================] - 2s 613us/step - loss: 0.6595 - accuracy: 0.7080\n",
      "Epoch 26/100\n",
      "3611/3611 [==============================] - 2s 574us/step - loss: 0.6588 - accuracy: 0.7096\n",
      "Epoch 27/100\n",
      "3611/3611 [==============================] - 2s 580us/step - loss: 0.6563 - accuracy: 0.7119\n",
      "Epoch 28/100\n",
      "3611/3611 [==============================] - 2s 608us/step - loss: 0.6555 - accuracy: 0.7110\n",
      "Epoch 29/100\n",
      "3611/3611 [==============================] - 2s 571us/step - loss: 0.6533 - accuracy: 0.7134\n",
      "Epoch 30/100\n",
      "3611/3611 [==============================] - 2s 564us/step - loss: 0.6513 - accuracy: 0.7144\n",
      "Epoch 31/100\n",
      "3611/3611 [==============================] - 2s 571us/step - loss: 0.6502 - accuracy: 0.7146\n",
      "Epoch 32/100\n",
      "3611/3611 [==============================] - 2s 568us/step - loss: 0.6489 - accuracy: 0.7147\n",
      "Epoch 33/100\n",
      "3611/3611 [==============================] - 2s 559us/step - loss: 0.6480 - accuracy: 0.7152\n",
      "Epoch 34/100\n",
      "3611/3611 [==============================] - 2s 558us/step - loss: 0.6461 - accuracy: 0.7171\n",
      "Epoch 35/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6441 - accuracy: 0.7177\n",
      "Epoch 36/100\n",
      "3611/3611 [==============================] - 2s 539us/step - loss: 0.6437 - accuracy: 0.7179\n",
      "Epoch 37/100\n",
      "3611/3611 [==============================] - 2s 581us/step - loss: 0.6420 - accuracy: 0.7184\n",
      "Epoch 38/100\n",
      "3611/3611 [==============================] - 2s 540us/step - loss: 0.6410 - accuracy: 0.7198\n",
      "Epoch 39/100\n",
      "3611/3611 [==============================] - 2s 543us/step - loss: 0.6395 - accuracy: 0.7201\n",
      "Epoch 40/100\n",
      "3611/3611 [==============================] - 2s 539us/step - loss: 0.6385 - accuracy: 0.7202\n",
      "Epoch 41/100\n",
      "3611/3611 [==============================] - 2s 546us/step - loss: 0.6374 - accuracy: 0.7210\n",
      "Epoch 42/100\n",
      "3611/3611 [==============================] - 2s 544us/step - loss: 0.6359 - accuracy: 0.7223\n",
      "Epoch 43/100\n",
      "3611/3611 [==============================] - 2s 554us/step - loss: 0.6355 - accuracy: 0.7219\n",
      "Epoch 44/100\n",
      "3611/3611 [==============================] - 2s 549us/step - loss: 0.6341 - accuracy: 0.7236\n",
      "Epoch 45/100\n",
      "3611/3611 [==============================] - 2s 547us/step - loss: 0.6335 - accuracy: 0.7240\n",
      "Epoch 46/100\n",
      "3611/3611 [==============================] - 2s 549us/step - loss: 0.6321 - accuracy: 0.7247\n",
      "Epoch 47/100\n",
      "3611/3611 [==============================] - 2s 552us/step - loss: 0.6311 - accuracy: 0.7265\n",
      "Epoch 48/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6307 - accuracy: 0.7250\n",
      "Epoch 49/100\n",
      "3611/3611 [==============================] - 2s 539us/step - loss: 0.6291 - accuracy: 0.7270\n",
      "Epoch 50/100\n",
      "3611/3611 [==============================] - 2s 540us/step - loss: 0.6280 - accuracy: 0.7280\n",
      "Epoch 51/100\n",
      "3611/3611 [==============================] - 2s 535us/step - loss: 0.6280 - accuracy: 0.7266\n",
      "Epoch 52/100\n",
      "3611/3611 [==============================] - 2s 537us/step - loss: 0.6265 - accuracy: 0.7276\n",
      "Epoch 53/100\n",
      "3611/3611 [==============================] - 2s 538us/step - loss: 0.6254 - accuracy: 0.7281\n",
      "Epoch 54/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.6244 - accuracy: 0.7278\n",
      "Epoch 55/100\n",
      "3611/3611 [==============================] - 2s 549us/step - loss: 0.6231 - accuracy: 0.7287\n",
      "Epoch 56/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.6231 - accuracy: 0.7288\n",
      "Epoch 57/100\n",
      "3611/3611 [==============================] - 2s 544us/step - loss: 0.6218 - accuracy: 0.7304\n",
      "Epoch 58/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.6216 - accuracy: 0.7300\n",
      "Epoch 59/100\n",
      "3611/3611 [==============================] - 2s 542us/step - loss: 0.6202 - accuracy: 0.7302\n",
      "Epoch 60/100\n",
      "3611/3611 [==============================] - 2s 549us/step - loss: 0.6208 - accuracy: 0.7310\n",
      "Epoch 61/100\n",
      "3611/3611 [==============================] - 2s 556us/step - loss: 0.6190 - accuracy: 0.7317\n",
      "Epoch 62/100\n",
      "3611/3611 [==============================] - 2s 538us/step - loss: 0.6181 - accuracy: 0.7322\n",
      "Epoch 63/100\n",
      "3611/3611 [==============================] - 2s 538us/step - loss: 0.6172 - accuracy: 0.7321\n",
      "Epoch 64/100\n",
      "3611/3611 [==============================] - 2s 545us/step - loss: 0.6164 - accuracy: 0.7341\n",
      "Epoch 65/100\n",
      "3611/3611 [==============================] - 2s 541us/step - loss: 0.6162 - accuracy: 0.7327\n",
      "Epoch 66/100\n",
      "3611/3611 [==============================] - 2s 535us/step - loss: 0.6153 - accuracy: 0.7330\n",
      "Epoch 67/100\n",
      "3611/3611 [==============================] - 2s 540us/step - loss: 0.6138 - accuracy: 0.7346\n",
      "Epoch 68/100\n",
      "3611/3611 [==============================] - 2s 539us/step - loss: 0.6134 - accuracy: 0.7350\n",
      "Epoch 69/100\n",
      "3611/3611 [==============================] - 2s 538us/step - loss: 0.6135 - accuracy: 0.7341\n",
      "Epoch 70/100\n",
      "3611/3611 [==============================] - 2s 543us/step - loss: 0.6125 - accuracy: 0.7349\n",
      "Epoch 71/100\n",
      "3611/3611 [==============================] - 2s 548us/step - loss: 0.6117 - accuracy: 0.7356\n",
      "Epoch 72/100\n",
      "3611/3611 [==============================] - 2s 546us/step - loss: 0.6111 - accuracy: 0.7365\n",
      "Epoch 73/100\n",
      "3611/3611 [==============================] - 2s 539us/step - loss: 0.6105 - accuracy: 0.7359\n",
      "Epoch 74/100\n",
      "3611/3611 [==============================] - 2s 536us/step - loss: 0.6098 - accuracy: 0.7379\n",
      "Epoch 75/100\n",
      "3611/3611 [==============================] - 2s 544us/step - loss: 0.6092 - accuracy: 0.7378\n",
      "Epoch 76/100\n",
      "3611/3611 [==============================] - 2s 542us/step - loss: 0.6087 - accuracy: 0.7374\n",
      "Epoch 77/100\n",
      "3611/3611 [==============================] - 2s 542us/step - loss: 0.6090 - accuracy: 0.7374\n",
      "Epoch 78/100\n",
      "3611/3611 [==============================] - 2s 541us/step - loss: 0.6075 - accuracy: 0.7375\n",
      "Epoch 79/100\n",
      "3611/3611 [==============================] - 2s 543us/step - loss: 0.6074 - accuracy: 0.7391\n",
      "Epoch 80/100\n",
      "3611/3611 [==============================] - 2s 533us/step - loss: 0.6068 - accuracy: 0.7381\n",
      "Epoch 81/100\n",
      "3611/3611 [==============================] - 2s 551us/step - loss: 0.6063 - accuracy: 0.7396\n",
      "Epoch 82/100\n",
      "3611/3611 [==============================] - 2s 542us/step - loss: 0.6053 - accuracy: 0.7403\n",
      "Epoch 83/100\n",
      "3611/3611 [==============================] - 2s 546us/step - loss: 0.6052 - accuracy: 0.7392\n",
      "Epoch 84/100\n",
      "3611/3611 [==============================] - 2s 550us/step - loss: 0.6046 - accuracy: 0.7408\n",
      "Epoch 85/100\n",
      "3611/3611 [==============================] - 2s 554us/step - loss: 0.6038 - accuracy: 0.7411\n",
      "Epoch 86/100\n",
      "3611/3611 [==============================] - 2s 541us/step - loss: 0.6034 - accuracy: 0.7402\n",
      "Epoch 87/100\n",
      "3611/3611 [==============================] - 2s 542us/step - loss: 0.6028 - accuracy: 0.7414\n",
      "Epoch 88/100\n",
      "3611/3611 [==============================] - 2s 535us/step - loss: 0.6023 - accuracy: 0.7420\n",
      "Epoch 89/100\n",
      "3611/3611 [==============================] - 2s 531us/step - loss: 0.6021 - accuracy: 0.7416\n",
      "Epoch 90/100\n",
      "3611/3611 [==============================] - 2s 535us/step - loss: 0.6016 - accuracy: 0.7422\n",
      "Epoch 91/100\n",
      "3611/3611 [==============================] - 2s 533us/step - loss: 0.6012 - accuracy: 0.7406\n",
      "Epoch 92/100\n",
      "3611/3611 [==============================] - 2s 540us/step - loss: 0.6007 - accuracy: 0.7420\n",
      "Epoch 93/100\n",
      "3611/3611 [==============================] - 2s 537us/step - loss: 0.6001 - accuracy: 0.7421\n",
      "Epoch 94/100\n",
      "3611/3611 [==============================] - 2s 541us/step - loss: 0.5998 - accuracy: 0.7420\n",
      "Epoch 95/100\n",
      "3611/3611 [==============================] - 2s 546us/step - loss: 0.5994 - accuracy: 0.7434\n",
      "Epoch 96/100\n",
      "3611/3611 [==============================] - 2s 538us/step - loss: 0.5988 - accuracy: 0.7427\n",
      "Epoch 97/100\n",
      "3611/3611 [==============================] - 2s 540us/step - loss: 0.5987 - accuracy: 0.7447\n",
      "Epoch 98/100\n",
      "3611/3611 [==============================] - 2s 547us/step - loss: 0.5984 - accuracy: 0.7441\n",
      "Epoch 99/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.5974 - accuracy: 0.7435\n",
      "Epoch 100/100\n",
      "3611/3611 [==============================] - 2s 553us/step - loss: 0.5970 - accuracy: 0.7435\n",
      "888/888 - 0s - loss: 0.7836 - accuracy: 0.6591 - 409ms/epoch - 461us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 5: MinMaxScaler normalization on only a subset of columns, with random oversampling\n",
    "# Create a dictionary for iteration 5\n",
    "iteration_5 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_5[\"Iteration\"] = 5\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler5 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_5 = scaler5.fit(X_train_3b[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_5 = X_train_3b\n",
    "X_train_scaled_5[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_5.transform(X_train_3b[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "X_test_scaled_5 = X_test_3b\n",
    "X_test_scaled_5[[\"loudness\", \"tempo\", \"duration_min\"]] = X_scaler_5.transform(X_test_3b[[\"loudness\", \"tempo\", \"duration_min\"]])\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_5 = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_5, y_ros_5 = ros_5.fit_resample(X_train_scaled_5, y_train_3b)\n",
    "\n",
    "# Define the model\n",
    "nn_5 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_5.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=42))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_5.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_5.add(tf.keras.layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "# Check model structure\n",
    "nn_5.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn_5.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_5 = nn_5.fit(X_ros_5, y_ros_5, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_5, model_accuracy_5 = nn_5.evaluate(X_test_scaled_5, y_test_3b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_5 dictionary\n",
    "iteration_5[\"Loss\"] = model_loss_5\n",
    "iteration_5[\"Accuracy\"] = model_accuracy_5\n",
    "\n",
    "# Add the iteration_5 dictionary to the iterations list\n",
    "iterations.append(iteration_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 01m 04s]\n",
      "val_accuracy: 0.6763421297073364\n",
      "\n",
      "Best val_accuracy So Far: 0.6763421297073364\n",
      "Total elapsed time: 00h 21m 58s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "### Iteration 6: Hyperparameter tuning with iteration 4 model\n",
    "# Create a dictionary for iteration 6\n",
    "iteration_6 = {} \n",
    "\n",
    "# Add the iteration\n",
    "iteration_6[\"Iteration\"] = 6\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler6 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler_6 = scaler6.fit(X_train_3b)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_6 = X_scaler_6.transform(X_train_3b)\n",
    "X_test_scaled_6 = X_scaler_6.transform(X_test_3b)\n",
    "\n",
    "# Instantiate random oversampler model\n",
    "ros_6 = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit the training data to the oversampler model\n",
    "X_ros_6, y_ros_6 = ros_6.fit_resample(X_train_scaled_6, y_train_3b)\n",
    "\n",
    "# Create a method that creates a Sequential model with hyperparameter tuning\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Determine the activation functions for each layer\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "\n",
    "    # Determine neurons in the first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units', \n",
    "        min_value=80, \n",
    "        max_value=120, \n",
    "        step=20), \n",
    "        activation=activation, input_dim=42))\n",
    "    \n",
    "    # Determine the number of hidden layers and neurons in them\n",
    "    for i in range(hp.Int('num_layers', 1, 4)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_'+str(i),\n",
    "            min_value=50, \n",
    "            max_value=90,\n",
    "            step=20),\n",
    "            activation=activation))\n",
    "    \n",
    "    # Set up the output layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return nn_model\n",
    "\n",
    "# Set up the kerastuner instance\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\", \n",
    "    max_epochs=20, \n",
    "    hyperband_iterations=2,\n",
    "    overwrite=True)\n",
    "\n",
    "# Run the kerastuner\n",
    "tuner.search(X_ros_6, y_ros_6, epochs=20, validation_data=(X_test_scaled_6, y_test_3b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'tanh', 'first_units': 100, 'num_layers': 4, 'units_0': 90, 'units_1': 70, 'units_2': 90, 'units_3': 70, 'tuner/epochs': 20, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               4300      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 90)                9090      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 70)                6370      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 90)                6390      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 70)                6370      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 213       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,733\n",
      "Trainable params: 32,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3611/3611 [==============================] - 3s 715us/step - loss: 0.8775 - accuracy: 0.5785\n",
      "Epoch 2/100\n",
      "3611/3611 [==============================] - 3s 704us/step - loss: 0.7629 - accuracy: 0.6501\n",
      "Epoch 3/100\n",
      "3611/3611 [==============================] - 3s 710us/step - loss: 0.7456 - accuracy: 0.6589\n",
      "Epoch 4/100\n",
      "3611/3611 [==============================] - 3s 704us/step - loss: 0.7336 - accuracy: 0.6649\n",
      "Epoch 5/100\n",
      "3611/3611 [==============================] - 2s 683us/step - loss: 0.7233 - accuracy: 0.6716\n",
      "Epoch 6/100\n",
      "3611/3611 [==============================] - 2s 685us/step - loss: 0.7124 - accuracy: 0.6756\n",
      "Epoch 7/100\n",
      "3611/3611 [==============================] - 2s 682us/step - loss: 0.7018 - accuracy: 0.6819\n",
      "Epoch 8/100\n",
      "3611/3611 [==============================] - 2s 678us/step - loss: 0.6911 - accuracy: 0.6890\n",
      "Epoch 9/100\n",
      "3611/3611 [==============================] - 2s 689us/step - loss: 0.6796 - accuracy: 0.6958\n",
      "Epoch 10/100\n",
      "3611/3611 [==============================] - 3s 695us/step - loss: 0.6682 - accuracy: 0.7024\n",
      "Epoch 11/100\n",
      "3611/3611 [==============================] - 2s 682us/step - loss: 0.6569 - accuracy: 0.7083\n",
      "Epoch 12/100\n",
      "3611/3611 [==============================] - 2s 688us/step - loss: 0.6464 - accuracy: 0.7145\n",
      "Epoch 13/100\n",
      "3611/3611 [==============================] - 2s 683us/step - loss: 0.6361 - accuracy: 0.7206\n",
      "Epoch 14/100\n",
      "3611/3611 [==============================] - 2s 689us/step - loss: 0.6239 - accuracy: 0.7276\n",
      "Epoch 15/100\n",
      "3611/3611 [==============================] - 2s 687us/step - loss: 0.6141 - accuracy: 0.7335\n",
      "Epoch 16/100\n",
      "3611/3611 [==============================] - 3s 707us/step - loss: 0.6033 - accuracy: 0.7395\n",
      "Epoch 17/100\n",
      "3611/3611 [==============================] - 3s 720us/step - loss: 0.5942 - accuracy: 0.7436\n",
      "Epoch 18/100\n",
      "3611/3611 [==============================] - 3s 719us/step - loss: 0.5847 - accuracy: 0.7481\n",
      "Epoch 19/100\n",
      "3611/3611 [==============================] - 3s 724us/step - loss: 0.5766 - accuracy: 0.7522\n",
      "Epoch 20/100\n",
      "3611/3611 [==============================] - 3s 713us/step - loss: 0.5672 - accuracy: 0.7565\n",
      "Epoch 21/100\n",
      "3611/3611 [==============================] - 3s 719us/step - loss: 0.5596 - accuracy: 0.7621\n",
      "Epoch 22/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.5538 - accuracy: 0.7650\n",
      "Epoch 23/100\n",
      "3611/3611 [==============================] - 3s 733us/step - loss: 0.5454 - accuracy: 0.7690\n",
      "Epoch 24/100\n",
      "3611/3611 [==============================] - 3s 725us/step - loss: 0.5389 - accuracy: 0.7733\n",
      "Epoch 25/100\n",
      "3611/3611 [==============================] - 3s 727us/step - loss: 0.5325 - accuracy: 0.7744\n",
      "Epoch 26/100\n",
      "3611/3611 [==============================] - 3s 735us/step - loss: 0.5254 - accuracy: 0.7786\n",
      "Epoch 27/100\n",
      "3611/3611 [==============================] - 3s 816us/step - loss: 0.5207 - accuracy: 0.7806\n",
      "Epoch 28/100\n",
      "3611/3611 [==============================] - 3s 760us/step - loss: 0.5154 - accuracy: 0.7825\n",
      "Epoch 29/100\n",
      "3611/3611 [==============================] - 3s 730us/step - loss: 0.5106 - accuracy: 0.7856\n",
      "Epoch 30/100\n",
      "3611/3611 [==============================] - 3s 729us/step - loss: 0.5034 - accuracy: 0.7884\n",
      "Epoch 31/100\n",
      "3611/3611 [==============================] - 3s 722us/step - loss: 0.5022 - accuracy: 0.7889\n",
      "Epoch 32/100\n",
      "3611/3611 [==============================] - 3s 731us/step - loss: 0.4963 - accuracy: 0.7925\n",
      "Epoch 33/100\n",
      "3611/3611 [==============================] - 3s 725us/step - loss: 0.4919 - accuracy: 0.7947\n",
      "Epoch 34/100\n",
      "3611/3611 [==============================] - 3s 729us/step - loss: 0.4892 - accuracy: 0.7952\n",
      "Epoch 35/100\n",
      "3611/3611 [==============================] - 3s 721us/step - loss: 0.4864 - accuracy: 0.7973\n",
      "Epoch 36/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.4813 - accuracy: 0.7987\n",
      "Epoch 37/100\n",
      "3611/3611 [==============================] - 3s 729us/step - loss: 0.4771 - accuracy: 0.8032\n",
      "Epoch 38/100\n",
      "3611/3611 [==============================] - 3s 724us/step - loss: 0.4755 - accuracy: 0.8021\n",
      "Epoch 39/100\n",
      "3611/3611 [==============================] - 3s 727us/step - loss: 0.4704 - accuracy: 0.8053\n",
      "Epoch 40/100\n",
      "3611/3611 [==============================] - 3s 726us/step - loss: 0.4686 - accuracy: 0.8052\n",
      "Epoch 41/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.4637 - accuracy: 0.8080\n",
      "Epoch 42/100\n",
      "3611/3611 [==============================] - 3s 718us/step - loss: 0.4607 - accuracy: 0.8093\n",
      "Epoch 43/100\n",
      "3611/3611 [==============================] - 3s 727us/step - loss: 0.4610 - accuracy: 0.8094\n",
      "Epoch 44/100\n",
      "3611/3611 [==============================] - 3s 733us/step - loss: 0.4547 - accuracy: 0.8122\n",
      "Epoch 45/100\n",
      "3611/3611 [==============================] - 3s 727us/step - loss: 0.4572 - accuracy: 0.8109\n",
      "Epoch 46/100\n",
      "3611/3611 [==============================] - 3s 721us/step - loss: 0.4510 - accuracy: 0.8145\n",
      "Epoch 47/100\n",
      "3611/3611 [==============================] - 3s 734us/step - loss: 0.4484 - accuracy: 0.8139\n",
      "Epoch 48/100\n",
      "3611/3611 [==============================] - 3s 729us/step - loss: 0.4474 - accuracy: 0.8144\n",
      "Epoch 49/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.4459 - accuracy: 0.8168\n",
      "Epoch 50/100\n",
      "3611/3611 [==============================] - 3s 721us/step - loss: 0.4437 - accuracy: 0.8172\n",
      "Epoch 51/100\n",
      "3611/3611 [==============================] - 3s 725us/step - loss: 0.4409 - accuracy: 0.8186\n",
      "Epoch 52/100\n",
      "3611/3611 [==============================] - 3s 723us/step - loss: 0.4376 - accuracy: 0.8187\n",
      "Epoch 53/100\n",
      "3611/3611 [==============================] - 3s 722us/step - loss: 0.4363 - accuracy: 0.8213\n",
      "Epoch 54/100\n",
      "3611/3611 [==============================] - 3s 718us/step - loss: 0.4366 - accuracy: 0.8203\n",
      "Epoch 55/100\n",
      "3611/3611 [==============================] - 3s 729us/step - loss: 0.4341 - accuracy: 0.8207\n",
      "Epoch 56/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.4322 - accuracy: 0.8221\n",
      "Epoch 57/100\n",
      "3611/3611 [==============================] - 3s 731us/step - loss: 0.4297 - accuracy: 0.8233\n",
      "Epoch 58/100\n",
      "3611/3611 [==============================] - 3s 721us/step - loss: 0.4283 - accuracy: 0.8227\n",
      "Epoch 59/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.4285 - accuracy: 0.8238\n",
      "Epoch 60/100\n",
      "3611/3611 [==============================] - 3s 726us/step - loss: 0.4267 - accuracy: 0.8249\n",
      "Epoch 61/100\n",
      "3611/3611 [==============================] - 3s 726us/step - loss: 0.4235 - accuracy: 0.8245\n",
      "Epoch 62/100\n",
      "3611/3611 [==============================] - 3s 723us/step - loss: 0.4219 - accuracy: 0.8262\n",
      "Epoch 63/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.4192 - accuracy: 0.8275\n",
      "Epoch 64/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.4180 - accuracy: 0.8287\n",
      "Epoch 65/100\n",
      "3611/3611 [==============================] - 3s 729us/step - loss: 0.4218 - accuracy: 0.8269\n",
      "Epoch 66/100\n",
      "3611/3611 [==============================] - 3s 723us/step - loss: 0.4176 - accuracy: 0.8289\n",
      "Epoch 67/100\n",
      "3611/3611 [==============================] - 3s 733us/step - loss: 0.4190 - accuracy: 0.8281\n",
      "Epoch 68/100\n",
      "3611/3611 [==============================] - 3s 732us/step - loss: 0.4163 - accuracy: 0.8296\n",
      "Epoch 69/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.4148 - accuracy: 0.8293\n",
      "Epoch 70/100\n",
      "3611/3611 [==============================] - 3s 727us/step - loss: 0.4118 - accuracy: 0.8310\n",
      "Epoch 71/100\n",
      "3611/3611 [==============================] - 3s 722us/step - loss: 0.4132 - accuracy: 0.8305\n",
      "Epoch 72/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.4118 - accuracy: 0.8316\n",
      "Epoch 73/100\n",
      "3611/3611 [==============================] - 3s 730us/step - loss: 0.4100 - accuracy: 0.8329\n",
      "Epoch 74/100\n",
      "3611/3611 [==============================] - 3s 729us/step - loss: 0.4093 - accuracy: 0.8322\n",
      "Epoch 75/100\n",
      "3611/3611 [==============================] - 3s 728us/step - loss: 0.4071 - accuracy: 0.8316\n",
      "Epoch 76/100\n",
      "3611/3611 [==============================] - 3s 721us/step - loss: 0.4086 - accuracy: 0.8327\n",
      "Epoch 77/100\n",
      "3611/3611 [==============================] - 3s 726us/step - loss: 0.4069 - accuracy: 0.8324\n",
      "Epoch 78/100\n",
      "3611/3611 [==============================] - 3s 731us/step - loss: 0.4044 - accuracy: 0.8344\n",
      "Epoch 79/100\n",
      "3611/3611 [==============================] - 3s 731us/step - loss: 0.4046 - accuracy: 0.8340\n",
      "Epoch 80/100\n",
      "3611/3611 [==============================] - 3s 727us/step - loss: 0.4068 - accuracy: 0.8337\n",
      "Epoch 81/100\n",
      "3611/3611 [==============================] - 3s 747us/step - loss: 0.4035 - accuracy: 0.8341\n",
      "Epoch 82/100\n",
      "3611/3611 [==============================] - 3s 754us/step - loss: 0.4018 - accuracy: 0.8357\n",
      "Epoch 83/100\n",
      "3611/3611 [==============================] - 3s 746us/step - loss: 0.4033 - accuracy: 0.8352\n",
      "Epoch 84/100\n",
      "3611/3611 [==============================] - 3s 826us/step - loss: 0.4014 - accuracy: 0.8355\n",
      "Epoch 85/100\n",
      "3611/3611 [==============================] - 3s 836us/step - loss: 0.3989 - accuracy: 0.8375\n",
      "Epoch 86/100\n",
      "3611/3611 [==============================] - 3s 730us/step - loss: 0.4002 - accuracy: 0.8359\n",
      "Epoch 87/100\n",
      "3611/3611 [==============================] - 3s 836us/step - loss: 0.4015 - accuracy: 0.8365\n",
      "Epoch 88/100\n",
      "3611/3611 [==============================] - 3s 861us/step - loss: 0.3981 - accuracy: 0.8380\n",
      "Epoch 89/100\n",
      "3611/3611 [==============================] - 3s 820us/step - loss: 0.3991 - accuracy: 0.8367\n",
      "Epoch 90/100\n",
      "3611/3611 [==============================] - 3s 852us/step - loss: 0.3974 - accuracy: 0.8365\n",
      "Epoch 91/100\n",
      "3611/3611 [==============================] - 3s 927us/step - loss: 0.3953 - accuracy: 0.8392\n",
      "Epoch 92/100\n",
      "3611/3611 [==============================] - 3s 895us/step - loss: 0.3960 - accuracy: 0.8378\n",
      "Epoch 93/100\n",
      "3611/3611 [==============================] - 3s 842us/step - loss: 0.3962 - accuracy: 0.8375\n",
      "Epoch 94/100\n",
      "3611/3611 [==============================] - 3s 788us/step - loss: 0.3954 - accuracy: 0.8382\n",
      "Epoch 95/100\n",
      "3611/3611 [==============================] - 3s 774us/step - loss: 0.3958 - accuracy: 0.8381\n",
      "Epoch 96/100\n",
      "3611/3611 [==============================] - 3s 795us/step - loss: 0.3923 - accuracy: 0.8396\n",
      "Epoch 97/100\n",
      "3611/3611 [==============================] - 3s 778us/step - loss: 0.3934 - accuracy: 0.8396\n",
      "Epoch 98/100\n",
      "3611/3611 [==============================] - 3s 758us/step - loss: 0.3901 - accuracy: 0.8423\n",
      "Epoch 99/100\n",
      "3611/3611 [==============================] - 3s 757us/step - loss: 0.3913 - accuracy: 0.8406\n",
      "Epoch 100/100\n",
      "3611/3611 [==============================] - 3s 765us/step - loss: 0.3945 - accuracy: 0.8386\n",
      "888/888 - 1s - loss: 0.9697 - accuracy: 0.6669 - 523ms/epoch - 588us/step\n"
     ]
    }
   ],
   "source": [
    "### Iteration 6 continued\n",
    "# Get the best model and show its parameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)\n",
    "for params in best_hyper:\n",
    "    print(params.values)\n",
    "\n",
    "# Get a summary of the best model\n",
    "best_model = tuner.get_best_models()\n",
    "best_model[0].summary()\n",
    "\n",
    "# Build the best model and train it on the data\n",
    "model = tuner.hypermodel.build(best_hyper[0])\n",
    "model.fit(X_ros_6, y_ros_6, epochs=100, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_loss_6, model_accuracy_6 = model.evaluate(X_test_scaled_6, y_test_3b, verbose=2)\n",
    "\n",
    "# Add the evaluation to the iteration_6 dictionary\n",
    "iteration_6[\"Loss\"] = model_loss_6\n",
    "iteration_6[\"Accuracy\"] = model_accuracy_6\n",
    "\n",
    "# Add the iteration_6 dictionary to the iterations list\n",
    "iterations.append(iteration_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss: 0.8319549560546875, Accuracy: 0.6680287718772888\n",
      "Iteration 2: Loss: 0.6907895803451538, Accuracy: 0.7067422866821289\n",
      "Iteration 3: Loss: 0.680270791053772, Accuracy: 0.7121318578720093\n",
      "Iteration 4: Loss: 0.813368558883667, Accuracy: 0.6459771990776062\n",
      "Iteration 5: Loss: 0.7836458683013916, Accuracy: 0.6591165065765381\n",
      "Iteration 6: Loss: 0.9697378277778625, Accuracy: 0.6668663024902344\n"
     ]
    }
   ],
   "source": [
    "# Print Final Statistics:\n",
    "for i in range(len(iterations)):\n",
    "    print(f\"Iteration {i+1}: Loss: {iterations[i]['Loss']}, Accuracy: {iterations[i]['Accuracy']}\")\n",
    "\n",
    "# Add final statistics to a dataframe and output to a csv\n",
    "stats_df = pd.DataFrame(iterations)\n",
    "stats_df.to_csv(\"summary_stats/categorical_popularity_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
